from IPython.core.interactiveshell import InteractiveShell
InteractiveShell.ast_node_interactivity = "all"
%matplotlib inline
from tqdm.auto import tqdm
import concurrent.futures
from multiprocessing import Pool
import copy,os,sys,psutil
from collections import Counter,deque
import itertools
import os

import tensorflow as tf
import tensorflow_hub as hub
import numpy as np
import matplotlib.pyplot as plt

dir(tf.keras.layers)

model = keras.Sequential([
        layers.Embedding(input_dim=30000, output_dim=32, input_length=maxlen),
        layers.LSTM(32, return_sequences=True),
        layers.LSTM(1, activation='sigmoid', return_sequences=False)
    ])
model.compile(optimizer=keras.optimizers.Adam(),
                 loss=keras.losses.BinaryCrossentropy(),
                 metrics=['accuracy'])



np.random.seed(9)
vocab_size = 10 # 所有词有多少个
seq_length = 3 # 一个句子里词的个数 或者 一个样本的特征个数
emb_dim = 3     # 每个词的词向量维数 或者 一个特征的隐向量维数
batch_size = 2  # 一个输入batch大小 | inp -> emb: (batch_size, seq_length) -> (batch_size, seq_length, emb_dim)

input_arr = np.random.randint(vocab_size, size=(batch_size, seq_length))
input_arr.shape
input_arr

"embl"
embl=tf.keras.layers.Embedding(input_dim=vocab_size, output_dim=emb_dim)#, input_length=seq_length)
embl(input_arr).numpy().shape
embl(input_arr).numpy()

"densel"
densel=tf.keras.layers.Dense(units=4)
densel(embl(input_arr)).numpy().shape
densel(embl(input_arr)).numpy()
"densel kernel bias"
densel.kernel.shape
densel.bias.shape


"flattenl"
flattenl=tf.keras.layers.Flatten()
flattenl(embl(input_arr)).numpy().shape
flattenl(embl(input_arr)).numpy()

"densel(flattenl)"
densel=tf.keras.layers.Dense(units=4)
densel(flattenl(embl(input_arr))).numpy().shape
densel(flattenl(embl(input_arr))).numpy()
"densel(flattenl) kernel bias"
densel.kernel.shape
densel.bias.shape

"lstml0"
lstml0=tf.keras.layers.LSTM(4, return_sequences=True)
lstml0(embl(input_arr)).numpy().shape
lstml0(embl(input_arr)).numpy()

"lstml1"
lstml1=tf.keras.layers.LSTM(4, return_sequences=False)
lstml1(embl(input_arr)).numpy().shape
lstml1(embl(input_arr)).numpy()
# lstml1=tf.keras.layers.LSTM(1, activation='sigmoid', return_sequences=False)
# lstml0.build((batch_size))
# lstml0.count_params()

"lstml2"
lstml2=tf.keras.layers.LSTM(4, return_sequences=True, return_state=True)
output,h,c=[i.numpy() for i in lstml2(embl(input_arr))]
[i.numpy().shape for i in lstml2(embl(input_arr))]

lstml0.get_weights()[0].shape
lstml0.get_weights()[1].shape
lstml0.get_weights()[2].shape
len(lstml0.get_weights())

lstml1.get_weights()[0].shape
lstml1.get_weights()[1].shape
lstml1.get_weights()[2].shape
len(lstml1.get_weights())

[i for i in dir(lstml0) if not i.startswith("_")]

vocab_size = len(vocab)
embedding_dim = 256
rnn_units = 1024
def build_model(vocab_size, embedding_dim, rnn_units, batch_size):
    model = tf.keras.Sequential([
        tf.keras.layers.Embedding(vocab_size, embedding_dim, batch_input_shape=[batch_size, None]),
        tf.keras.layers.GRU(rnn_units,
                            return_sequences=True,
                            stateful=True,
                            recurrent_initializer='glorot_uniform'),
        tf.keras.layers.Dense(vocab_size)
      ])
    return model
