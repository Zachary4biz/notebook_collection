{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-12T08:23:59.773565Z",
     "start_time": "2021-01-12T08:23:59.280051Z"
    }
   },
   "outputs": [],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "%matplotlib inline\n",
    "from tqdm.auto import tqdm\n",
    "import concurrent.futures\n",
    "from multiprocessing import Pool\n",
    "import copy,os,sys\n",
    "from collections import Counter,deque\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "import functools\n",
    "import itertools\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-12T08:24:02.638076Z",
     "start_time": "2021-01-12T08:23:59.775425Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.15.4'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import feature_column\n",
    "tf.__version__\n",
    "tf.get_logger().setLevel(tf.logging.ERROR)\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from glob import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-12T08:24:02.645007Z",
     "start_time": "2021-01-12T08:24:02.642258Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"-1\"  # 禁用GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-09T08:08:33.446946Z",
     "start_time": "2021-01-09T08:08:32.718720Z"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"/Users/didi/Downloads/group_line_data.csv\",sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-09T08:08:37.342929Z",
     "start_time": "2021-01-09T08:08:36.988766Z"
    }
   },
   "outputs": [],
   "source": [
    "df[\"vl_volume\"].describe()\n",
    "df[\"vl_volume\"].apply(lambda x: 100 if x>=100 else x).hist(bins=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-06T08:27:05.317723Z",
     "start_time": "2021-01-06T08:27:04.757794Z"
    }
   },
   "outputs": [],
   "source": [
    "df[\"vl_capacity\"].describe()\n",
    "df[\"vl_capacity\"].apply(lambda x: 100 if x>=100 else x).hist(bins=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-09T08:09:07.956203Z",
     "start_time": "2021-01-09T08:09:07.452544Z"
    }
   },
   "outputs": [],
   "source": [
    "df[\"sku_cnt\"].describe()\n",
    "df[\"sku_cnt\"].apply(lambda x: 100 if x>=100 else x).hist(bins=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-06T08:28:40.411169Z",
     "start_time": "2021-01-06T08:28:39.834057Z"
    }
   },
   "outputs": [],
   "source": [
    "df[\"user_cnt\"].describe()\n",
    "df[\"user_cnt\"].apply(lambda x: 100 if x>=100 else x).hist(bins=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-06T10:09:49.745146Z",
     "start_time": "2021-01-06T10:09:49.413369Z"
    }
   },
   "outputs": [],
   "source": [
    "df[\"num_sum\"].describe()\n",
    "df[\"num_sum\"].apply(lambda x: 100 if x>=100 else x).hist(bins=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-06T10:11:42.283842Z",
     "start_time": "2021-01-06T10:11:41.175146Z"
    }
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "help(datetime)\n",
    "datetime.isoformat(\"2020-10-29\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-06T11:19:18.281873Z",
     "start_time": "2021-01-06T11:19:18.274435Z"
    }
   },
   "outputs": [],
   "source": [
    "time_str=\"2020-12-02 07:18:55\"\n",
    "datetime.datetime.strptime(time_str, '%Y-%m-%d %H:%M:%S').timestamp()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-06T11:16:07.872431Z",
     "start_time": "2021-01-06T11:16:05.127420Z"
    }
   },
   "outputs": [],
   "source": [
    "df[\"tocar_time\"].describe()\n",
    "def time_format(time_str):\n",
    "    try:\n",
    "        datetime.datetime.strptime(time_str, '%Y-%m-%d %H:%M:%S')\n",
    "    except Exception:\n",
    "        \n",
    "df[\"tocar_time\"].apply(lambda x: ).head(3)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## simulation | numeric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-05T13:11:20.707642Z",
     "start_time": "2021-01-05T13:11:20.544425Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(2020)\n",
    "def gen_func(x1,x2,x3):\n",
    "    y = 20*x1**3 + 10*x2*x2 + 5*x3 + 7\n",
    "    return y\n",
    "\n",
    "X = np.random.random((50,3))\n",
    "Y = np.apply_along_axis(lambda x: gen_func(*x), axis=1, arr=X)\n",
    "\n",
    "features = {}\n",
    "features[\"x0\"] = X[:,0]\n",
    "features[\"x1\"] = X[:,1]\n",
    "features[\"x2\"] = X[:,2]\n",
    "features[\"y\"] = np.where(Y>20,1,0)\n",
    "\n",
    "pd.DataFrame({k:v.tolist() for k,v in features.items()}).head(3)\n",
    "\n",
    "tf.reset_default_graph()  # required\n",
    "\n",
    "fc_numeric = list()\n",
    "fc_numeric.append(feature_column.numeric_column(\"x0\", default_value=0))\n",
    "fc_numeric.append(feature_column.numeric_column(\"x1\", default_value=0))\n",
    "fc_numeric.append(feature_column.numeric_column(\"x2\", default_value=0))\n",
    "\n",
    "# tf.estimator.LinearRegressor(feature_columns=fc_numeric)\n",
    "labels = np.expand_dims(features[\"y\"], -1)\n",
    "inp = feature_column.input_layer(features, fc_numeric)\n",
    "lr_logits = tf.layers.dense(inp, 1, activation=tf.sigmoid)\n",
    "loss = tf.losses.sigmoid_cross_entropy(labels, lr_logits)\n",
    "opt = tf.train.AdagradOptimizer(1e-3)\n",
    "opt.minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.estimator.LinearRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## simulation | trd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-05T12:05:34.928478Z",
     "start_time": "2021-01-05T12:05:34.883222Z"
    },
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "np.random.seed(2020)\n",
    "num_buckets_deep=7\n",
    "dims_embs_deep=4\n",
    "_batch_size=2\n",
    "\n",
    "# features\n",
    "features = {}\n",
    "features[\"featA\"] = np.random.randint(num_buckets_deep, size=(_batch_size, 1))\n",
    "features[\"featB\"] = np.random.randint(num_buckets_deep, size=(_batch_size, 1))\n",
    "features[\"featC\"] = np.random.randint(num_buckets_deep, size=(_batch_size, 1))\n",
    "features[\"featD\"] = np.random.randint(num_buckets_deep, size=(_batch_size, 1))\n",
    "features[\"featE\"] = np.random.randint(num_buckets_deep, size=(_batch_size, 1))\n",
    "features[\"numA\"] = np.random.random((_batch_size,1))\n",
    "features[\"numB\"] = np.random.random((_batch_size,1))\n",
    "features[\"wideFeatures\"] = np.random.randint(100, size=(_batch_size, 4))\n",
    "# features\n",
    "pd.DataFrame({k:v.tolist() for k,v in features.items()})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## simulation | titanic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "####################\n",
    "# for massive data | not-yet\n",
    "# trainIterDF = pd.read_csv(\"/Users/zac/Downloads/titanic_train.csv\", \n",
    "#                           chunksize=16, dtype=str, converters={'age':float, 'fare':float})\n",
    "# evalIterDF = pd.read_csv(\"/Users/zac/Downloads/titanic_eval.csv\", \n",
    "#                          chunksize=16, dtype=str, converters={'age':float, 'fare':float})\n",
    "\n",
    "# def genIter(n=10):\n",
    "#     inp = trainIterDF.read(n)\n",
    "#     feats,lbls = (inp, inp.pop(\"survived\"))\n",
    "#     feats = feats.to_dict('list')\n",
    "#     lbls = np.expand_dims(np.array(lbls),-1)\n",
    "#     yield (feats,lbls)\n",
    "######################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-05T16:44:12.953397Z",
     "start_time": "2021-01-05T16:44:11.688913Z"
    },
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "pd.read_csv('https://storage.googleapis.com/tf-datasets/titanic/train.csv').to_csv(\"/Users/zac/Downloads/titanic_train.csv\")\n",
    "pd.read_csv('https://storage.googleapis.com/tf-datasets/titanic/eval.csv').to_csv(\"/Users/zac/Downloads/titanic_eval.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-05T18:34:02.699289Z",
     "start_time": "2021-01-05T18:34:02.693518Z"
    }
   },
   "outputs": [],
   "source": [
    "CATEGORICAL_COLUMNS = ['sex', 'n_siblings_spouses', 'parch', 'class', 'deck', 'embark_town', 'alone']\n",
    "EMB_COLUMNS = ['sex', 'n_siblings_spouses']\n",
    "CATEGORICAL_COLUMNS = ['parch', 'class', 'deck', 'embark_town', 'alone']\n",
    "NUMERIC_COLUMNS = ['age', 'fare']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-05T18:39:57.320257Z",
     "start_time": "2021-01-05T18:39:57.189319Z"
    },
    "code_folding": [],
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "trainDF = pd.read_csv(\"/Users/zac/Downloads/titanic_train.csv\", dtype=str, converters={'age':float, 'fare':float, 'survived':int})\n",
    "evalDF = pd.read_csv(\"/Users/zac/Downloads/titanic_eval.csv\", dtype=str, converters={'age':float, 'fare':float, 'survived':int})\n",
    "\n",
    "\n",
    "# for custom | not-yet as GBDT?\n",
    "def getIter(data_df, label_col=\"survived\", mode=None):\n",
    "    ds = tf.data.Dataset.from_tensor_slices((data_df.to_dict('list'), data_df[label_col]))\n",
    "    ds = ds.batch(16).repeat(1)\n",
    "    \n",
    "    if mode is not None and mode.upper() == \"EXP\":\n",
    "        iterator = ds.make_initializable_iterator()\n",
    "        feats,lbls = iterator.get_next()\n",
    "        lbls = tf.expand_dims(lbls,-1)\n",
    "        return (feats,lbls), iterator.initializer\n",
    "    \n",
    "    iterator = ds.make_one_shot_iterator()\n",
    "    feats,lbls = iterator.get_next()\n",
    "    lbls = tf.expand_dims(lbls,-1)\n",
    "    return (feats,lbls)\n",
    "\n",
    "# trainData = getIter(trainDF)\n",
    "# evData = getIter(evalDF)\n",
    "# evData,evInit = getIter(evalDF,mode=\"EXP\")\n",
    "# train_input_fn = functools.partial(getIter, trainDF)\n",
    "# eval_input_fn = functools.partial(getIter, evalDF)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-07T08:52:12.420576Z",
     "start_time": "2021-01-07T08:52:12.340385Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "df = pd.concat([pd.read_csv(i) for i in glob(\"/Users/zac/Downloads/vl_regression.csv/part-*\")], ignore_index=True)\n",
    "df['vl_volume'] = df['vl_volume'].astype(np.float32)\n",
    "def bucketize(ser):\n",
    "    holds = ser.quantile([.25, .35, .5, .75, .95]).values\n",
    "    holds\n",
    "    def _bucketize(x):\n",
    "        for idx, i in enumerate(holds):\n",
    "            if x <= i:\n",
    "                return idx\n",
    "        return len(holds)\n",
    "    return ser.apply(_bucketize)\n",
    "\n",
    "df.head(3)\n",
    "# df['num_sum1'] = bucketize(df['num_sum1'])\n",
    "# df['sku_cnt'] = bucketize(df['sku_cnt'])\n",
    "df.head(3)\n",
    "\n",
    "trainDF = df.query(\"dt<='2020-12-28'\")\n",
    "evalDF = df.query(\"dt>='2020-12-28'\")\n",
    "\n",
    "trainDF.shape\n",
    "evalDF.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-07T09:32:04.122204Z",
     "start_time": "2021-01-07T09:32:03.350334Z"
    }
   },
   "outputs": [],
   "source": [
    "def percentile(n):\n",
    "    def percentile_(x):\n",
    "        return np.percentile(x, n)\n",
    "    percentile_.__name__ = 'percentile_%s' % n\n",
    "    return percentile_\n",
    "\n",
    "evalDF.groupby(\"vl_type\").agg({\"vl_volume\":[\"count\",min,\"mean\",max]})\n",
    "evalDF.groupby(\"vl_type\").agg({\"num_sum1\":[min,percentile(10),percentile(90),max]})\n",
    "\n",
    "# evalDF.plot(subplots=True,kind='hist',y='vl_volume')\n",
    "for idx,(k,g) in enumerate(evalDF.groupby(\"vl_type\")):\n",
    "    g.plot(title=k,kind='hist',y='vl_volume')\n",
    "#     g['vl_volume'].plot.hist(tag=k)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# parse_fn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## schem verification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-12T08:24:04.880111Z",
     "start_time": "2021-01-12T08:24:04.788935Z"
    },
    "code_folding": [
     5
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[featName]:categoryIds_indices [type]:bytes_list [value_example]:[b'1066', b'1068', b'1070', b'1072' ...] | len=12\n",
      "[featName]:categoryIds_values [type]:float_list [value_example]:[128.0, 288.0, 5.0, 74.0 ...] | len=12\n",
      "[featName]:specInfo_indices [type]:bytes_list [value_example]:[b'4098_100g/\\xe8\\xa2\\x8b', b'21612_400g/\\xe7\\x9b\\x92', b'22430_52g/\\xe8\\xa2\\x8b', b'22826_(500g\\xc2\\xb125g)/\\xe4\\xbb\\xbd' ...] | len=378\n",
      "[featName]:specInfo_values [type]:float_list [value_example]:[2.0, 3.0, 1.0, 1.0 ...] | len=378\n",
      "[featName]:vl_capacity [type]:bytes_list [value_example]:[b'59' ...] | len=1\n",
      "[featName]:vl_type [type]:bytes_list [value_example]:[b'VAN_TYPE' ...] | len=1\n",
      "[featName]:vl_volume [type]:bytes_list [value_example]:[b'179' ...] | len=1\n"
     ]
    }
   ],
   "source": [
    "trd_fp = glob(\"/Users/zac/Downloads/cleanInfo_trd_train/*.tfrecord.gz\")\n",
    "trd=tf.data.TFRecordDataset(trd_fp,compression_type='GZIP')\n",
    "iterator=trd.make_one_shot_iterator()\n",
    "one=iterator.get_next()\n",
    "res = []\n",
    "with tf.Session() as sess:\n",
    "    one_ = sess.run(one)\n",
    "    featInfo = tf.train.Example.FromString(one_).features.feature # not dict\n",
    "    for key in featInfo:\n",
    "        info_list=[(key, attr, featInfo[key].__getattribute__(attr).value) for attr in [\"bytes_list\",\"float_list\",\"int64_list\"]]\n",
    "        info = [i for i in info_list if i[-1] != []][0] # 只应该有一个\n",
    "        info = list(info)\n",
    "        info[-1] = \"[%s ...] | len=%s\" % (\", \".join(map(str,info[-1][:4])), len(info[-1]))\n",
    "        res.append(\"[featName]:{} [type]:{} [value_example]:{}\".format(*info))\n",
    "\n",
    "print(\"\\n\".join(sorted(res)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load_ds_fn | iterator load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-12T09:04:46.800673Z",
     "start_time": "2021-01-12T09:04:46.139701Z"
    },
    "code_folding": [
     7,
     16,
     30,
     54
    ],
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> all keys: categoryIds_indices,categoryIds_values,specInfo_indices,specInfo_values,vl_type,vl_volume,weights\n",
      "\n",
      ">>> labels: [[3], [3], [0], [1]...]\n",
      ">>> labels distrib: (array([0, 1, 2, 3]), array([101, 686,  23,  24]))\n",
      "\n",
      ">>> label-shape: (834, 1)\n",
      "\n",
      ">>> weights:\n",
      "[1. 1. 2. 3.]\n",
      ">>> vl_type:\n",
      "[b'VAN_TYPE' b'VAN_TYPE' b'SMALL_CAR' b'MIDSIZE_CAR']\n",
      ">>> specInfo_values:\n",
      "(array([[  0,   0],\n",
      "       [  0,   1],\n",
      "       [  0,   2],\n",
      "       ...,\n",
      "       [833, 374],\n",
      "       [833, 375],\n",
      "       [833, 376]]), array([3., 1., 4., ..., 4., 1., 4.], dtype=float32), array([834, 772]))\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8cb8f7089954318b14532b3171fe705",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='batch', max=1.0, style=ProgressStyle(de…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "trd_train_fp = glob(\"/Users/zac/Downloads/cleanInfo_trd_train/*.tfrecord.gz\")  # 6478\n",
    "trd_test_fp = glob(\"/Users/zac/Downloads/cleanInfo_trd_test/*.tfrecord.gz\")    # 1291\n",
    "\n",
    "VL_TYPES = [\"SMALL_CAR\",\"MIDSIZE_CAR\",\"LARGE_CAR\",\"VAN_TYPE\"]\n",
    "LBL_WEIGHTS = [2.0,3.0,1.0,1.0]\n",
    "cls_labeler = lambda x: tf.where(tf.equal(x, VL_TYPES))\n",
    "\n",
    "features_schema={\n",
    "    \"vl_type\":tf.io.FixedLenFeature([], tf.string),\n",
    "    \"vl_volume\":tf.io.FixedLenFeature([], tf.string),\n",
    "    \"categoryIds_indices\":tf.io.VarLenFeature(tf.string),\n",
    "    \"categoryIds_values\":tf.io.VarLenFeature(tf.float32),\n",
    "    \"specInfo_indices\":tf.io.VarLenFeature(tf.string),\n",
    "    \"specInfo_values\":tf.io.VarLenFeature(tf.float32),\n",
    "}\n",
    "\n",
    "def _parse_func(model_type, inp):\n",
    "    parsed_features = tf.io.parse_single_example(inp,features_schema)\n",
    "    if model_type.lower() == \"reg\":\n",
    "        # regression\n",
    "        label = tf.strings.to_number(parsed_features[\"vl_volume\"],tf.float32)\n",
    "        labels = tf.expand_dims(label, -1)\n",
    "    elif model_type.lower() == \"cls\":\n",
    "        # classify\n",
    "        label = parsed_features[\"vl_type\"]\n",
    "        label = tf.reduce_min(tf.where(tf.equal(label, VL_TYPES)))\n",
    "        labels = tf.expand_dims(label, -1)\n",
    "        parsed_features[\"weights\"] = tf.squeeze(tf.gather(LBL_WEIGHTS,label))\n",
    "    return (parsed_features,labels)\n",
    "\n",
    "def load_ds(trd_fp, mode, batch_size=1024, epoch_num=10, model_type=\"reg\"):\n",
    "    _partial_parse_func = functools.partial(_parse_func,model_type)\n",
    "    \n",
    "    ds = tf.data.TFRecordDataset(trd_fp,compression_type='GZIP')\n",
    "    ds = ds.map(_partial_parse_func).shuffle(True)\n",
    "    if mode.lower() == \"train\":\n",
    "        ds = ds.batch(batch_size,drop_remainder=False).repeat(epoch_num)\n",
    "        iterator = ds.make_one_shot_iterator()\n",
    "        return iterator.get_next()\n",
    "    elif mode.lower() == \"test\":\n",
    "        ds = ds.batch(batch_size,drop_remainder=False).repeat(1)\n",
    "        iterator = ds.make_one_shot_iterator()\n",
    "        return iterator.get_next()\n",
    "    elif mode.lower() == \"exp\":\n",
    "        ds = ds.batch(batch_size,drop_remainder=False).repeat(1)\n",
    "        iterator = ds.make_initializable_iterator()\n",
    "        return iterator.get_next(), iterator.initializer\n",
    "    else:\n",
    "        raise\n",
    "\n",
    "\n",
    "\n",
    "trainData=load_ds(trd_test_fp,\"train\",model_type=\"cls\",epoch_num=2)\n",
    "to_print = [\"weights\",\"vl_type\",\"specInfo_values\"]\n",
    "with tf.Session() as sess:\n",
    "    i = sess.run(trainData)\n",
    "    feat = i[0]\n",
    "    label = i[1]\n",
    "    print(\">>> all keys: {}\\n\".format(\",\".join(feat.keys())))\n",
    "    print(\">>> labels: [{}...]\".format(\", \".join(map(str,label[:4]))))\n",
    "    print(\">>> labels distrib: {}\\n\".format(np.unique(label,return_counts=True)))\n",
    "    print(\">>> label-shape: {}\\n\".format(label.shape))\n",
    "    for key in to_print:\n",
    "        print(\">>> {}:\".format(key))\n",
    "        print(feat[key][:4])\n",
    "    \n",
    "    try:\n",
    "        with tqdm(desc=\"batch\") as pbar:\n",
    "            while True:\n",
    "                _ = sess.run(trainData)\n",
    "                pbar.update()\n",
    "    except tf.errors.OutOfRangeError:\n",
    "        pass\n",
    "                \n",
    "    \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## wrapper_fc_list | feature_column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-12T09:04:51.459672Z",
     "start_time": "2021-01-12T09:04:51.452873Z"
    },
    "code_folding": [
     1
    ]
   },
   "outputs": [],
   "source": [
    "# 为了配合reset graph ，它必须在reset之后执行，但是不想写成一坨\n",
    "def wrapper_fc_list():\n",
    "    fc_list = list()\n",
    "#     categoryID_weighted = feature_column.weighted_categorical_column(categorical_column=feature_column.categorical_column_with_hash_bucket(\"categoryIds_indices\",20),\n",
    "#                                                                      weight_feature_key='categoryIds_values')\n",
    "#     fc_list.append(feature_column.indicator_column(categoryID_weighted))\n",
    "\n",
    "    specInfo_weighted = feature_column.weighted_categorical_column(categorical_column=feature_column.categorical_column_with_hash_bucket(\"specInfo_indices\",100), \n",
    "                                                                   weight_feature_key='specInfo_values')\n",
    "    fc_list.append(feature_column.indicator_column(specInfo_weighted))\n",
    "\n",
    "    return fc_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LR/NN | Custom"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  model_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-11T12:00:59.623332Z",
     "start_time": "2021-01-11T12:00:59.620483Z"
    },
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "MODEL_TYPE=\"cls\"\n",
    "BATCH_SIZE=1024\n",
    "LOSS_TYPE=\"CE_W\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-11T12:01:00.508555Z",
     "start_time": "2021-01-11T12:01:00.052704Z"
    },
    "code_folding": [
     2,
     24
    ],
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "def my_loss_fn(kind,labels,preds,**kwargs):\n",
    "    if kind.upper() == \"RMSE\":\n",
    "        loss = tf.losses.mean_squared_error(labels,preds,reduction=tf.losses.Reduction.MEAN)\n",
    "        loss = tf.sqrt(loss)\n",
    "    elif kind.upper() == \"CE_W\":\n",
    "        weights = tf.squeeze(tf.gather(LBL_WEIGHTS,labels))\n",
    "        loss = tf.losses.sparse_softmax_cross_entropy(labels,preds,weights)\n",
    "    elif kind.upper() == \"CE\":\n",
    "        loss = tf.losses.sparse_softmax_cross_entropy(labels,preds)\n",
    "    elif kind.upper() == \"MAPE\":\n",
    "        _loss_each = tf.sqrt(tf.math.pow(labels - preds,2))/labels\n",
    "        loss = tf.reduce_mean(_loss_each)\n",
    "    else:\n",
    "        raise\n",
    "    return loss\n",
    "\n",
    "output_shape = len(VL_TYPES) if MODEL_TYPE.lower() == \"cls\" else 1\n",
    "loss_fn = functools.partial(my_loss_fn,LOSS_TYPE)\n",
    "logits_activation = None if MODEL_TYPE.lower()==\"cls\" else tf.nn.relu\n",
    "fc_list = wrapper_fc_list()\n",
    "\n",
    "\n",
    "def model_fn(features,lbls,fc_list,mode):\n",
    "    net_inp = feature_column.input_layer(features,fc_list)\n",
    "    net = net_inp\n",
    "#     net = tf.layers.dense(net_inp,128,activation=tf.nn.sigmoid,reuse=tf.AUTO_REUSE,name=\"l1\")\n",
    "#     net = tf.layers.dense(net_inp,32,activation=tf.nn.sigmoid,reuse=tf.AUTO_REUSE,name=\"l2\")\n",
    "    logits = tf.layers.dense(net,output_shape,activation=logits_activation,reuse=tf.AUTO_REUSE,name=\"logits\")\n",
    "#     initA = tf.constant_initializer(np.random.random(size=[net.shape[1],1])*0.1, verify_shape=True)\n",
    "#     logits = tf.layers.dense(net,1,activation=tf.nn.relu,\n",
    "#                              reuse=tf.AUTO_REUSE,\n",
    "#                              kernel_initializer=initA,\n",
    "#                              kernel_constraint=tf.keras.constraints.NonNeg(),\n",
    "#                              name=\"logits\") \n",
    "\n",
    "    if mode.lower() == \"predict\":\n",
    "        with tf.variable_scope(\"predict\", reuse=tf.AUTO_REUSE):\n",
    "            predictions = {\"logits\":logits, \"labels\":lbls}\n",
    "            return tf.estimator.EstimatorSpec(mode=mode,\n",
    "                                              predictions=predictions)\n",
    "    elif mode.lower() == \"eval\":\n",
    "        with tf.variable_scope(\"eval\", reuse=tf.AUTO_REUSE):\n",
    "            loss = loss_fn(labels=lbls,preds=logits)\n",
    "            predictions = {\"loss\":loss, \"logits\":logits, \"labels\":lbls}\n",
    "            return tf.estimator.EstimatorSpec(mode=mode,\n",
    "                                              loss=loss,\n",
    "                                              predictions=predictions)\n",
    "    elif mode.lower() == \"train\":\n",
    "        with tf.variable_scope(\"train\", reuse=tf.AUTO_REUSE):\n",
    "            loss = loss_fn(labels=lbls,preds=logits)\n",
    "            optimizer = tf.train.AdamOptimizer(1e-1)\n",
    "            opt = optimizer.minimize(loss)\n",
    "            return tf.estimator.EstimatorSpec(mode=mode,\n",
    "                                              loss=loss,\n",
    "                                              train_op=opt)\n",
    "    elif mode.lower() == \"testonly\":\n",
    "         return my_loss_fn(labels=lbls,preds=logits,kind=\"CE_W\")\n",
    "    else:\n",
    "        assert False,\"mode is %s\" % mode\n",
    "    \n",
    "\n",
    "trainData,trainDataInit=load_ds(trd_train_fp,\"exp\",batch_size=BATCH_SIZE,model_type=MODEL_TYPE)\n",
    "evalData,evalInit=load_ds(trd_test_fp,\"exp\",batch_size=BATCH_SIZE*10,model_type=MODEL_TYPE)\n",
    "\n",
    "train_spec = model_fn(trainData[0],trainData[1],fc_list,\"train\")\n",
    "_ = tf.get_variable_scope().reuse_variables()\n",
    "eval_spec = model_fn(evalData[0],evalData[1],fc_list,\"eval\")\n",
    "predict_spec = model_fn(evalData[0],evalData[1],fc_list,\"predict\")\n",
    "\n",
    "test_res = model_fn(evalData[0],evalData[1],fc_list,\"testOnly\")\n",
    "\n",
    "tf.trainable_variables()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## exec_debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-11T12:01:03.387592Z",
     "start_time": "2021-01-11T12:01:01.862175Z"
    },
    "code_folding": [
     0
    ],
    "deletable": false,
    "editable": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    sess.run(tf.local_variables_initializer())\n",
    "    \n",
    "    sess.run(evalInit)\n",
    "    print(\">>> test_res: {}\".format(test_res.eval()))\n",
    "    \n",
    "    print(\">>> predictions\")\n",
    "    sess.run(evalInit)\n",
    "    pred = sess.run(eval_spec.predictions)\n",
    "    print(\"loss:  \",pred['loss'])\n",
    "    print(\"logits:\",pred['logits'][0])\n",
    "    print(\"labels:\",pred['labels'][0])\n",
    "    \n",
    "    print(\">>> logits/kernel\")\n",
    "    tf.get_default_graph().get_tensor_by_name(\"logits/kernel:0\").eval()[:10]\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## exec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-11T12:01:30.802394Z",
     "start_time": "2021-01-11T12:01:21.980205Z"
    },
    "code_folding": [
     0
    ],
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    loss_train_list = []\n",
    "    loss_eval_list = []\n",
    "    step = 0\n",
    "    for i in tqdm(range(10)):\n",
    "        sess.run(tf.local_variables_initializer())\n",
    "        sess.run(trainDataInit)\n",
    "        try:\n",
    "            with tqdm(leave=False) as pbar:\n",
    "                while True:\n",
    "                    if step % 2 == 0:\n",
    "#                         head5=tf.get_default_graph().get_tensor_by_name(\"logits/kernel:0\").eval()[:5]\n",
    "#                         kernel_mean = np.mean(tf.get_default_graph().get_tensor_by_name(\"logits/kernel:0\").eval())\n",
    "#                         print(\"logits/kernel: [mean]:{}\".format(kernel_mean))\n",
    "#                         print(\",\".join(map(str,head5)))\n",
    "                        pass\n",
    "                    _,_loss_train = sess.run([train_spec.train_op, train_spec.loss])\n",
    "                    loss_train_list.append({\"step\":step,\"{}_train\".format(LOSS_TYPE):_loss_train})\n",
    "                    if step%10 == 0:\n",
    "                        try:\n",
    "                            sess.run(evalInit)\n",
    "                            _loss_eval,pred = sess.run([eval_spec.loss,eval_spec.predictions])\n",
    "                            loss_eval_list.append({\"step\":step,\"{}_test\".format(LOSS_TYPE):_loss_eval})\n",
    "#                             print(\"\\n[predictions_mean]:{:.4f}\".format(np.mean(pred['logits'])))\n",
    "#                             print(\"[loss]\",pred[\"loss\"])\n",
    "#                             print(\"[logits]\",\",\".join(map(str,pred[\"logits\"][10:15])))\n",
    "#                             print(\"[labels]\",\",\".join(map(str,pred[\"labels\"][10:15])))\n",
    "#                             print(\"[logits]\",\",\".join(map(str,pred[\"logits\"][-5:])))\n",
    "#                             print(\"[labels]\",\",\".join(map(str,pred[\"labels\"][-5:])))\n",
    "                        except tf.errors.OutOfRangeError:\n",
    "                            pass\n",
    "                    pbar.update()\n",
    "                    step = step+1\n",
    "        except tf.errors.OutOfRangeError:\n",
    "            pass\n",
    "#             print(\"done\")\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "## visualize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "### metrics of test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-11T12:16:42.156994Z",
     "start_time": "2021-01-11T12:16:41.589122Z"
    },
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "print(\"logits, labels 分布\")\n",
    "fig,axes=plt.subplots(2,1)\n",
    "cur_axe=axes.flatten()[0]\n",
    "cur_axe.title.set_text(\"labels\")\n",
    "pd.Series(pred[\"labels\"].flatten()).hist(bins=100, ax=cur_axe)\n",
    "\n",
    "cur_axe=axes.flatten()[1]\n",
    "cur_axe.title.set_text(\"logits\")\n",
    "logits_processed=np.argmax(pred[\"logits\"],axis=1) if MODEL_TYPE.lower() == \"cls\" else pred[\"logits\"]\n",
    "pd.Series(logits_processed).hist(bins=100, ax=cur_axe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-11T12:19:32.156398Z",
     "start_time": "2021-01-11T12:19:32.128540Z"
    },
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "infoDF = pd.DataFrame({'labels':pred['labels'].flatten(),\n",
    "                       'logits':np.argmax(pred['logits'],axis=1).flatten()})\n",
    "\n",
    "for k,g in infoDF.groupby(\"labels\"):\n",
    "    head=\"acc_{}\".format(k)\n",
    "    detail=\"{}/{}\".format(g.query(\"logits == labels\").shape[0],g.shape[0])\n",
    "    res=g.query(\"logits == labels\").shape[0]/g.shape[0]\n",
    "    print(\"{}: {}={:.4f}\".format(head,detail,res))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "### loss of test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-11T12:15:26.232090Z",
     "start_time": "2021-01-11T12:15:25.894118Z"
    },
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "resDF = pd.DataFrame(loss_train_list).merge(pd.DataFrame(loss_eval_list),on=\"step\",how=\"left\")\n",
    "# resDF.head(3)\n",
    "resDF.interpolate(method='linear').plot(subplots=True, x=\"step\",y=['{}_train'.format(LOSS_TYPE),'{}_test'.format(LOSS_TYPE)])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LR | Linear Official"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-12T09:05:37.677524Z",
     "start_time": "2021-01-12T09:05:37.673697Z"
    }
   },
   "outputs": [],
   "source": [
    "fc_list = wrapper_fc_list()\n",
    "train_input_fn = lambda : load_ds(trd_train_fp,\"train\",epoch_num=5,model_type=\"cls\")\n",
    "eval_input_fn = lambda : load_ds(trd_test_fp,\"test\",epoch_num=1,model_type=\"cls\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-12T09:05:45.005602Z",
     "start_time": "2021-01-12T09:05:40.690153Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'accuracy': 0.6366907, 'average_loss': 10.433507, 'loss': 8701.545, 'global_step': 15}\n"
     ]
    }
   ],
   "source": [
    "linear_est = tf.estimator.LinearClassifier(feature_columns=fc_list,n_classes=len(VL_TYPES))\n",
    "_ = linear_est.train(train_input_fn)\n",
    "result = linear_est.evaluate(eval_input_fn)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-12T09:10:04.516557Z",
     "start_time": "2021-01-12T09:10:03.655489Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "class_ids\n",
       "1    612\n",
       "0    222\n",
       "dtype: int64"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "labels\n",
       "1    686\n",
       "0    101\n",
       "3     24\n",
       "2     23\n",
       "dtype: int64"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = []\n",
    "for i in linear_est.predict(eval_input_fn,predict_keys=[\"probabilities\",\"class_ids\"],yield_single_examples=True):\n",
    "    i['probabilities'] = np.max(i['probabilities'])\n",
    "    i['class_ids'] = i['class_ids'][0]\n",
    "    preds.append(i)\n",
    "\n",
    "lbls=[]\n",
    "with tf.Session() as sess:\n",
    "    try:\n",
    "        lbls.append(eval_input_fn()[1].eval().flatten())\n",
    "    except tf.errors.OutOfRangeError:\n",
    "        pass\n",
    "\n",
    "lbls = np.hstack(lbls)\n",
    "\n",
    "resDF = pd.DataFrame(preds)\n",
    "resDF['labels'] = lbls\n",
    "resDF.value_counts('class_ids')\n",
    "resDF.value_counts('labels')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GBDT+FM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-12T08:11:04.427923Z",
     "start_time": "2021-01-12T08:11:03.817411Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 3.,  3.,  1., ...,  3.,  3.,  4.],\n",
       "       [ 3.,  8.,  2., ...,  5.,  7.,  3.],\n",
       "       [ 1.,  1.,  0., ...,  0., 19.,  8.],\n",
       "       ...,\n",
       "       [ 5.,  1.,  3., ..., 18., 16.,  2.],\n",
       "       [ 4.,  0.,  2., ...,  2., 11.,  1.],\n",
       "       [11.,  0.,  4., ...,  4.,  4.,  1.]], dtype=float32)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "array([[0],\n",
       "       [1],\n",
       "       [0],\n",
       "       ...,\n",
       "       [0],\n",
       "       [0],\n",
       "       [1]])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fc_list = wrapper_fc_list()\n",
    "\n",
    "train_input_fn = lambda : load_ds(trd_train_fp,\"train\",model_type=\"cls\")\n",
    "    \n",
    "eval_input_fn = lambda : load_ds(trd_test_fp,\"test\",model_type=\"cls\")\n",
    "\n",
    "features,labels = train_input_fn()\n",
    "with tf.Session() as sess:\n",
    "    feature_column.input_layer(features,fc_list).eval()\n",
    "    labels.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-12T07:37:13.737180Z",
     "start_time": "2021-01-12T07:37:13.651117Z"
    }
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "in converted code:\n\n    <ipython-input-5-f278512c193a>:18 _parse_func  *\n        parsed_features = tf.io.parse_single_example(inp,features)\n    /Users/didi/opt/miniconda3/lib/python3.7/site-packages/tensorflow_core/python/ops/parsing_ops.py:1019 parse_single_example\n        serialized, features, example_names, name\n    /Users/didi/opt/miniconda3/lib/python3.7/site-packages/tensorflow_core/python/ops/parsing_ops.py:1063 parse_single_example_v2_unoptimized\n        return parse_single_example_v2(serialized, features, name)\n    /Users/didi/opt/miniconda3/lib/python3.7/site-packages/tensorflow_core/python/ops/parsing_ops.py:2090 parse_single_example_v2\n        [VarLenFeature, SparseFeature, FixedLenFeature, FixedLenSequenceFeature])\n    /Users/didi/opt/miniconda3/lib/python3.7/site-packages/tensorflow_core/python/ops/parsing_ops.py:300 _features_to_raw_params\n        raise ValueError(\"Invalid feature %s:%s.\" % (key, feature))\n\n    ValueError: Invalid feature categoryIds_indices:SparseTensor(indices=Tensor(\"DeserializeSparse_16:0\", shape=(?, 2), dtype=int64), values=Tensor(\"DeserializeSparse_16:1\", shape=(?,), dtype=string), dense_shape=Tensor(\"DeserializeSparse_16:2\", shape=(2,), dtype=int64)).\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-edb64c224b23>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBoostedTreesClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfc_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_batches_per_layer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_input_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Users/didi/opt/miniconda3/lib/python3.7/site-packages/tensorflow_estimator/python/estimator/estimator.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, input_fn, hooks, steps, max_steps, saving_listeners)\u001b[0m\n\u001b[1;32m    368\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    369\u001b[0m       \u001b[0msaving_listeners\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_check_listeners_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msaving_listeners\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 370\u001b[0;31m       \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_train_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msaving_listeners\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    371\u001b[0m       \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Loss for final step: %s.'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    372\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/didi/opt/miniconda3/lib/python3.7/site-packages/tensorflow_estimator/python/estimator/estimator.py\u001b[0m in \u001b[0;36m_train_model\u001b[0;34m(self, input_fn, hooks, saving_listeners)\u001b[0m\n\u001b[1;32m   1159\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_train_model_distributed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msaving_listeners\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1160\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1161\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_train_model_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msaving_listeners\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1162\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1163\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_train_model_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msaving_listeners\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/didi/opt/miniconda3/lib/python3.7/site-packages/tensorflow_estimator/python/estimator/estimator.py\u001b[0m in \u001b[0;36m_train_model_default\u001b[0;34m(self, input_fn, hooks, saving_listeners)\u001b[0m\n\u001b[1;32m   1186\u001b[0m       features, labels, input_hooks = (\n\u001b[1;32m   1187\u001b[0m           self._get_features_and_labels_from_input_fn(\n\u001b[0;32m-> 1188\u001b[0;31m               input_fn, ModeKeys.TRAIN))\n\u001b[0m\u001b[1;32m   1189\u001b[0m       \u001b[0mworker_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_hooks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1190\u001b[0m       estimator_spec = self._call_model_fn(\n",
      "\u001b[0;32m/Users/didi/opt/miniconda3/lib/python3.7/site-packages/tensorflow_estimator/python/estimator/estimator.py\u001b[0m in \u001b[0;36m_get_features_and_labels_from_input_fn\u001b[0;34m(self, input_fn, mode)\u001b[0m\n\u001b[1;32m   1023\u001b[0m     \u001b[0;34m\"\"\"Extracts the `features` and labels from return values of `input_fn`.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1024\u001b[0m     return estimator_util.parse_input_fn_result(\n\u001b[0;32m-> 1025\u001b[0;31m         self._call_input_fn(input_fn, mode))\n\u001b[0m\u001b[1;32m   1026\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1027\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_extract_batch_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreds_evaluated\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/didi/opt/miniconda3/lib/python3.7/site-packages/tensorflow_estimator/python/estimator/estimator.py\u001b[0m in \u001b[0;36m_call_input_fn\u001b[0;34m(self, input_fn, mode, input_context)\u001b[0m\n\u001b[1;32m   1114\u001b[0m       \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'input_context'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1115\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/cpu:0'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1116\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0minput_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1118\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_model_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-15-26d168f56d39>\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mfc_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwrapper_fc_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mtrain_input_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mload_ds\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrd_train_fp\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"train\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmodel_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"cls\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0meval_input_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mload_ds\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrd_test_fp\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"test\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmodel_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"cls\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-14-f278512c193a>\u001b[0m in \u001b[0;36mload_ds\u001b[0;34m(trd_fp, mode, batch_size, epoch_num, model_type)\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0mds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTFRecordDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrd_train_fp\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcompression_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'GZIP'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m     \u001b[0mds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_partial_parse_func\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"train\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0mds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdrop_remainder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrepeat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch_num\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/didi/opt/miniconda3/lib/python3.7/site-packages/tensorflow_core/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36mmap\u001b[0;34m(self, map_func, num_parallel_calls)\u001b[0m\n\u001b[1;32m   1907\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mnum_parallel_calls\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1908\u001b[0m       return DatasetV1Adapter(\n\u001b[0;32m-> 1909\u001b[0;31m           MapDataset(self, map_func, preserve_cardinality=False))\n\u001b[0m\u001b[1;32m   1910\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1911\u001b[0m       return DatasetV1Adapter(\n",
      "\u001b[0;32m/Users/didi/opt/miniconda3/lib/python3.7/site-packages/tensorflow_core/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, input_dataset, map_func, use_inter_op_parallelism, preserve_cardinality, use_legacy_function)\u001b[0m\n\u001b[1;32m   3432\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transformation_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3433\u001b[0m         \u001b[0mdataset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_dataset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3434\u001b[0;31m         use_legacy_function=use_legacy_function)\n\u001b[0m\u001b[1;32m   3435\u001b[0m     variant_tensor = gen_dataset_ops.map_dataset(\n\u001b[1;32m   3436\u001b[0m         \u001b[0minput_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variant_tensor\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/didi/opt/miniconda3/lib/python3.7/site-packages/tensorflow_core/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, func, transformation_name, dataset, input_classes, input_shapes, input_types, input_structure, add_to_graph, use_legacy_function, defun_kwargs)\u001b[0m\n\u001b[1;32m   2711\u001b[0m       \u001b[0mresource_tracker\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtracking\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mResourceTracker\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2712\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mtracking\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresource_tracker_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource_tracker\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2713\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwrapper_fn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_concrete_function_internal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2714\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0madd_to_graph\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2715\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_to_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_default_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/didi/opt/miniconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_get_concrete_function_internal\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1851\u001b[0m     \u001b[0;34m\"\"\"Bypasses error checking when getting a graph function.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1852\u001b[0m     graph_function = self._get_concrete_function_internal_garbage_collected(\n\u001b[0;32m-> 1853\u001b[0;31m         *args, **kwargs)\n\u001b[0m\u001b[1;32m   1854\u001b[0m     \u001b[0;31m# We're returning this concrete function to someone, and they may keep a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1855\u001b[0m     \u001b[0;31m# reference to the FuncGraph without keeping a reference to the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/didi/opt/miniconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_get_concrete_function_internal_garbage_collected\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1845\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_signature\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1846\u001b[0m       \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1847\u001b[0;31m     \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1848\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1849\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/didi/opt/miniconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_maybe_define_function\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   2145\u001b[0m         \u001b[0mgraph_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprimary\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcache_key\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2146\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mgraph_function\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2147\u001b[0;31m           \u001b[0mgraph_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_graph_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2148\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprimary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcache_key\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2149\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/didi/opt/miniconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_create_graph_function\u001b[0;34m(self, args, kwargs, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m   2036\u001b[0m             \u001b[0marg_names\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0marg_names\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2037\u001b[0m             \u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2038\u001b[0;31m             capture_by_value=self._capture_by_value),\n\u001b[0m\u001b[1;32m   2039\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_attributes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2040\u001b[0m         \u001b[0;31m# Tell the ConcreteFunction to clean up its graph once it goes out of\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/didi/opt/miniconda3/lib/python3.7/site-packages/tensorflow_core/python/framework/func_graph.py\u001b[0m in \u001b[0;36mfunc_graph_from_py_func\u001b[0;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m    913\u001b[0m                                           converted_func)\n\u001b[1;32m    914\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 915\u001b[0;31m       \u001b[0mfunc_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpython_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mfunc_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfunc_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    916\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    917\u001b[0m       \u001b[0;31m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/didi/opt/miniconda3/lib/python3.7/site-packages/tensorflow_core/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36mwrapper_fn\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m   2705\u001b[0m           attributes=defun_kwargs)\n\u001b[1;32m   2706\u001b[0m       \u001b[0;32mdef\u001b[0m \u001b[0mwrapper_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=missing-docstring\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2707\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_wrapper_helper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2708\u001b[0m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstructure\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_tensor_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output_structure\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2709\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/didi/opt/miniconda3/lib/python3.7/site-packages/tensorflow_core/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36m_wrapper_helper\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m   2650\u001b[0m         \u001b[0mnested_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnested_args\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2651\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2652\u001b[0;31m       \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mautograph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtf_convert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mag_ctx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mnested_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2653\u001b[0m       \u001b[0;31m# If `func` returns a list of tensors, `nest.flatten()` and\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2654\u001b[0m       \u001b[0;31m# `ops.convert_to_tensor()` would conspire to attempt to stack\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/didi/opt/miniconda3/lib/python3.7/site-packages/tensorflow_core/python/autograph/impl/api.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    235\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint:disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'ag_error_metadata'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 237\u001b[0;31m           \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    238\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    239\u001b[0m           \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: in converted code:\n\n    <ipython-input-5-f278512c193a>:18 _parse_func  *\n        parsed_features = tf.io.parse_single_example(inp,features)\n    /Users/didi/opt/miniconda3/lib/python3.7/site-packages/tensorflow_core/python/ops/parsing_ops.py:1019 parse_single_example\n        serialized, features, example_names, name\n    /Users/didi/opt/miniconda3/lib/python3.7/site-packages/tensorflow_core/python/ops/parsing_ops.py:1063 parse_single_example_v2_unoptimized\n        return parse_single_example_v2(serialized, features, name)\n    /Users/didi/opt/miniconda3/lib/python3.7/site-packages/tensorflow_core/python/ops/parsing_ops.py:2090 parse_single_example_v2\n        [VarLenFeature, SparseFeature, FixedLenFeature, FixedLenSequenceFeature])\n    /Users/didi/opt/miniconda3/lib/python3.7/site-packages/tensorflow_core/python/ops/parsing_ops.py:300 _features_to_raw_params\n        raise ValueError(\"Invalid feature %s:%s.\" % (key, feature))\n\n    ValueError: Invalid feature categoryIds_indices:SparseTensor(indices=Tensor(\"DeserializeSparse_16:0\", shape=(?, 2), dtype=int64), values=Tensor(\"DeserializeSparse_16:1\", shape=(?,), dtype=string), dense_shape=Tensor(\"DeserializeSparse_16:2\", shape=(2,), dtype=int64)).\n"
     ]
    }
   ],
   "source": [
    "est = tf.estimator.BoostedTreesClassifier(fc_list, n_batches_per_layer=1)\n",
    "est.train(train_input_fn, max_steps=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\">>> result:\\n\",result)\n",
    "\n",
    "pred_dicts = list(linear_est.predict(eval_input_fn))\n",
    "probs = pd.Series([pred['probabilities'][1] for pred in pred_dicts])\n",
    "probs.plot(kind='hist', bins=20, title='predicted probabilities')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WideDeep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fc_emb = list()\n",
    "fc_linear = list()\n",
    "fc_num = list()\n",
    "for fname in CATEGORICAL_COLUMNS:\n",
    "    hash_col = feature_column.categorical_column_with_hash_bucket(fname, hash_bucket_size=10000, dtype=tf.string)\n",
    "    id_col = feature_column.indicator_column(hash_col)\n",
    "    fc_linear.append(id_col)\n",
    "\n",
    "for fname in EMB_COLUMNS:\n",
    "    hash_col = feature_column.categorical_column_with_hash_bucket(fname, hash_bucket_size=10000, dtype=tf.string)\n",
    "    emb_col = feature_column.embedding_column(hash_col, dimension=6)\n",
    "    fc_emb.append(emb_col)\n",
    "\n",
    "for fname in NUMERIC_COLUMNS:\n",
    "    fc_num.append(feature_column.numeric_column(fname, dtype=tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-05T19:22:37.567539Z",
     "start_time": "2021-01-05T19:22:31.843551Z"
    }
   },
   "outputs": [],
   "source": [
    "train_input_fn = functools.partial(getIter, trainDF)\n",
    "eval_input_fn = functools.partial(getIter, evalDF)\n",
    "\n",
    "wd_cls = tf.estimator.DNNLinearCombinedClassifier(linear_feature_columns=fc_linear,\n",
    "                                                  dnn_feature_columns=fc_emb+fc_num,\n",
    "                                                  dnn_hidden_units=[128,32],\n",
    "                                                  n_classes=2)\n",
    "\n",
    "_ = wd_cls.train(train_input_fn)\n",
    "result = wd_cls.evaluate(eval_input_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-05T19:22:54.730227Z",
     "start_time": "2021-01-05T19:22:53.908712Z"
    }
   },
   "outputs": [],
   "source": [
    "print(\">>> result:\\n\",result)\n",
    "\n",
    "pred_dicts = list(linear_est.predict(eval_input_fn))\n",
    "probs = pd.Series([pred['probabilities'][1] for pred in pred_dicts])\n",
    "probs.plot(kind='hist', bins=20, title='predicted probabilities')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WD custom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fc_list = list()\n",
    "for fname in CATEGORICAL_COLUMNS:\n",
    "    hash_col = feature_column.categorical_column_with_hash_bucket(fname, hash_bucket_size=10000, dtype=tf.string)\n",
    "    id_col = feature_column.indicator_column(hash_col)\n",
    "    fc_list.append(id_col)\n",
    "\n",
    "for fname in EMB_COLUMNS:\n",
    "    hash_col = feature_column.categorical_column_with_hash_bucket(fname, hash_bucket_size=10000, dtype=tf.string)\n",
    "    emb_col = feature_column.embedding_column(hash_col, dimension=6)\n",
    "    fc_list.append(emb_col)\n",
    "\n",
    "for fname in NUMERIC_COLUMNS:\n",
    "    fc_list.append(feature_column.numeric_column(fname, dtype=tf.float32))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-05T19:17:16.372272Z",
     "start_time": "2021-01-05T19:17:15.157482Z"
    },
    "code_folding": [
     26,
     40,
     46
    ]
   },
   "outputs": [],
   "source": [
    "from tensorflow.python.ops import init_ops\n",
    "\n",
    "\n",
    "trainData = getIter(trainDF)\n",
    "evData = getIter(evalDF)\n",
    "\n",
    "#########\n",
    "# Model\n",
    "#########\n",
    "\n",
    "\n",
    "def dnn_logits(features,n_classes,fc_emb,fc_num,mode):\n",
    "    net = feature_column.input_layer(features, fc_emb+fc_num)\n",
    "    for layer_id, num_hidden_units in enumerate([128,32]):\n",
    "        with tf.variable_scope('hiddenlayer_%d' % layer_id\n",
    "                               ) as hidden_layer_scope:\n",
    "            net = tf.layers.dense(net, num_hidden_units, activation=tf.nn.relu,\n",
    "                                  kernel_initializer=init_ops.glorot_uniform_initializer(),\n",
    "                                  name=hidden_layer_scope)\n",
    "\n",
    "    with tf.variable_scope('logits') as logits_scope:\n",
    "        dnn_logits = tf.layers.dense(net, n_classes, kernel_initializer=init_ops.glorot_uniform_initializer(),\n",
    "                                     activation=None,name=logits_scope)\n",
    "    return dnn_logits\n",
    "\n",
    "def linear_logits(features,n_classes,fc_linear,mode):\n",
    "    net = feature_column.input_layer(features, fc_linear)\n",
    "    linear_logits = tf.layers.dense(net, n_classes, activation=tf.nn.relu, name=\"linear_layer\")\n",
    "    return linear_logits\n",
    "\n",
    "\n",
    "def model_fn(features,n_classes,fc_emb,fc_num,fc_linear,mode):\n",
    "    with tf.variable_scope('dnn') as dnn_scope:\n",
    "        dnn_scope_name = dnn_scope.name()\n",
    "        dnn_logit = dnn_logits(features,n_classes,fc_emb,fc_num,mode)\n",
    "\n",
    "    with tf.variable_scope('linear') as linear_scope:\n",
    "        linear_scope_name = linear_scope.name\n",
    "        linear_logits = linear_logits(features,n_classes,fc_linear,mode)\n",
    "\n",
    "    logits = dnn_logits + linear_logits\n",
    "    \n",
    "    if mode.upper() ==\"EVAL\":\n",
    "        probabilities = tf.nn.softmax(logits)\n",
    "        class_ids = tf.argmax(probabilities, 1)\n",
    "        export_predictions = {\n",
    "            \"class_id\": class_ids,\n",
    "            \"probabilities\": probabilities,\n",
    "        }\n",
    "        export_outputs = {\n",
    "            \"predict\": tf.estimator.export.PredictOutput(export_predictions)\n",
    "        }\n",
    "        return tf.estimator.EstimatorSpec(mode,\n",
    "                                          predictions=export_predictions,\n",
    "                                          export_outputs=export_outputs)\n",
    "    \n",
    "    if mode.upper() == \"TRAIN\":\n",
    "        loss = tf.losses.sparse_softmax_cross_entropy(logits=logits, labels=lbls, reduction=tf.losses.Reduction.SUM)\n",
    "\n",
    "        dnn_optimizer = tf.train.AdagradOptimizer(learning_rate=1e-3)\n",
    "        _dnn_optimizer = tf.contrib.estimator.clip_gradients_by_norm(dnn_optimizer, clip_norm=50)\n",
    "        linear_optimizer = tf.train.FtrlOptimizer(learning_rate=1e-4)\n",
    "        \n",
    "        train_ops=[\n",
    "            _dnn_optimizer.minize(loss=loss, \n",
    "                                  var_list=tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES,scope=dnn_scope_name)),\n",
    "            linear_optimizer.minize(loss=loss, \n",
    "                                    var_list=tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES,scope=linear_scope_name))\n",
    "        ]\n",
    "        train_op = tf.group(*train_ops)\n",
    "        train_metric_ops = {\"LOSS\": tf.metrics.mean(gross_loss, name=\"mean_loss\")}\n",
    "        # Provide an estimator spec for `ModeKeys.TRAIN` modes.\n",
    "        return tf.estimator.EstimatorSpec(mode=mode,\n",
    "                                          loss=loss,\n",
    "                                          train_op=train_op,\n",
    "                                          eval_metric_ops=train_metric_ops)\n",
    "\n",
    "\n",
    "# model_fn(...)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    net = feature_column.input_layer(trainData[0], fc_list)\n",
    "    net.shape\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    lbls = trainData[1]\n",
    "    net.eval()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
