{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-08T12:19:55.324858Z",
     "start_time": "2020-12-08T12:19:55.295634Z"
    }
   },
   "outputs": [],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "%matplotlib inline\n",
    "from tqdm.auto import tqdm\n",
    "import concurrent.futures\n",
    "from multiprocessing import Pool\n",
    "import copy,os,sys\n",
    "from collections import Counter,deque\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "import functools, itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-08T12:19:55.651701Z",
     "start_time": "2020-12-08T12:19:55.644581Z"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.feature_column as fc\n",
    "import numpy as np\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pyhocon import ConfigFactory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-08T12:19:56.389999Z",
     "start_time": "2020-12-08T12:19:56.379108Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"-1\"  # 禁用GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-08T12:19:59.615088Z",
     "start_time": "2020-12-08T12:19:59.604827Z"
    }
   },
   "outputs": [],
   "source": [
    "DATA_PATH=\"/tmp-data/zhoutongzt/DD/Data\"\n",
    "NFS_DATA_PATH=\"/nfs/map-tmp-vol0/zhoutong/Data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-08T12:20:04.653347Z",
     "start_time": "2020-12-08T12:20:04.609945Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 1, 2])"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "array([3, 3, 2])"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "array([[1, 0, 0],\n",
       "       [0, 1, 1]])"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "array([0.25, 0.25, 0.5 ])"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = [[1,0,0],\n",
    "     [0,0,0],\n",
    "     [0,0,1],\n",
    "     [0,1,1]]\n",
    "m = np.array(m)\n",
    "np.count_nonzero(m, axis=1)\n",
    "np.count_nonzero(m == 0 , axis=0)\n",
    "m[[0,3],:]\n",
    "np.mean(m, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 处理TFRecord"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 检查schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-09T07:42:15.733518Z",
     "start_time": "2020-12-09T07:42:15.485845Z"
    },
    "run_control": {
     "marked": true
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[featName]:dnnCrsCurRoad [type]:int64_list [value_example]:[8593260L]\n",
      "[featName]:dnnCrsDragRoad [type]:int64_list [value_example]:[-1L]\n",
      "[featName]:dnnCrsPoiRoad [type]:int64_list [value_example]:[8593262L]\n",
      "[featName]:dnnDGlobalTime [type]:int64_list [value_example]:[8655481L]\n",
      "[featName]:dnnNpoiDis [type]:float_list [value_example]:[0.9300000071525574]\n",
      "[featName]:dnnPGlobalTime [type]:int64_list [value_example]:[8655390L]\n",
      "[featName]:dnncCurrentDis [type]:float_list [value_example]:[27.799999237060547]\n",
      "[featName]:dnncPoiDis [type]:float_list [value_example]:[23.520000457763672]\n",
      "[featName]:dnncRgeoDis [type]:float_list [value_example]:[0.0]\n",
      "[featName]:dnnpidDropoffHeat [type]:float_list [value_example]:[4.0]\n",
      "[featName]:dnnpidDropoffRatio [type]:float_list [value_example]:[6.659999847412109]\n",
      "[featName]:dnnpidLBChargeTime [type]:float_list [value_example]:[0.0]\n",
      "[featName]:dnnpidLFChargeTime [type]:float_list [value_example]:[0.0]\n",
      "[featName]:dnnpidPickupHeat [type]:float_list [value_example]:[8.0]\n",
      "[featName]:dnnpidRatio [type]:float_list [value_example]:[13.789999961853027]\n",
      "[featName]:dnnpidTrajNum [type]:float_list [value_example]:[0.0]\n",
      "[featName]:dnnpidTrajRatio [type]:float_list [value_example]:[0.0]\n",
      "[featName]:dnnpoiDropoffRatio [type]:float_list [value_example]:[0.0]\n",
      "[featName]:dnnpoiRatio [type]:float_list [value_example]:[19.709999084472656]\n",
      "[featName]:dnnpsw [type]:float_list [value_example]:[16.0]\n",
      "[featName]:hourIdx [type]:int64_list [value_example]:[8593274L]\n",
      "[featName]:label [type]:int64_list [value_example]:[3L, 2L]\n",
      "[featName]:link [type]:int64_list [value_example]:[9297929L]\n",
      "[featName]:locHash [type]:int64_list [value_example]:[-1L]\n",
      "[featName]:pickupHash [type]:int64_list [value_example]:[5063647L]\n",
      "[featName]:pid [type]:int64_list [value_example]:[-1L]\n",
      "[featName]:poi [type]:int64_list [value_example]:[-1L]\n",
      "[featName]:rgeoHash [type]:int64_list [value_example]:[-1L]\n",
      "[featName]:timeIdx [type]:int64_list [value_example]:[12273629L]\n",
      "[featName]:weekDay [type]:int64_list [value_example]:[8593266L]\n",
      "[featName]:wideFeatures [type]:int64_list [value_example]:[10858061L, -1L, -1L, -1L, 19158838L, 19158842L, 19158848L, 19158855L, 19158863L, 19158873L, -1L, 19158888L, -1L, 19158901L, 19158908L, -1L, -1L, -1L, -1L, 19158945L, 19391765L, 19391866L, 19158951L, 19158953L, -1L, 19158957L, 19158965L, 26750457L, 19391973L, 19396777L, 19396811L, 19396839L, 82629406L, 65402163L, -1L, -1L, -1L, -1L, -1L, -1L, -1L, -1L]\n",
      "[featName]:wifi [type]:int64_list [value_example]:[-1L, -1L, -1L, -1L, -1L, -1L, -1L, -1L]\n"
     ]
    }
   ],
   "source": [
    "# trd_dir=\"/tmp-data/zhoutongzt/DD/Data/BR_useReqTime_addCross_test\"\n",
    "trd_dir = \"/nfs/map-tmp-vol0/zhoutong/Data\"+\"/BR_useReqTime_expTryMMOE_train\"\n",
    "trd_fp = [os.path.join(trd_dir,i) for i in os.listdir(trd_dir)]\n",
    "trd=tf.data.TFRecordDataset(trd_fp,compression_type='GZIP')\n",
    "iterator=trd.make_one_shot_iterator()\n",
    "one=iterator.get_next()\n",
    "res = []\n",
    "with tf.Session() as sess:\n",
    "    one_ = sess.run(one)\n",
    "    featInfo = tf.train.Example.FromString(one_).features.feature # not dict\n",
    "    for key in featInfo:\n",
    "        info_list=[(key, attr, featInfo[key].__getattribute__(attr).value) for attr in [\"bytes_list\",\"float_list\",\"int64_list\"]]\n",
    "        info = [i for i in info_list if i[-1] != []][0] # 只应该有一个\n",
    "        res.append(\"[featName]:{} [type]:{} [value_example]:{}\".format(*info))\n",
    "\n",
    "print(\"\\n\".join(sorted(res)))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-10T06:46:21.862573Z",
     "start_time": "2020-12-10T06:46:21.852736Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fc_config={\"a\":0}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 解析"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-08T05:07:42.424116Z",
     "start_time": "2020-12-08T05:07:41.480455Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[features.keys]: dnnDGlobalTime, dnnNpoiDis, dnnPGlobalTime, dnncCurrentDis, dnncPoiDis, dnncRgeoDis, dnnpidDropoffHeat, dnnpidDropoffRatio, dnnpidLBChargeTime, dnnpidLFChargeTime, dnnpidPickupHeat, dnnpidRatio, dnnpidTrajNum, dnnpidTrajRatio, dnnpoiDropoffRatio, dnnpoiRatio, dnnpsw, link, locHash, pickupHash, pid, poi, rgeoHash, wideFeatures, wifi\n",
      ">>> all keys: rgeoHash,pid,dnncRgeoDis,poi,link,dnncPoiDis,dnnpidDropoffRatio,wideFeatures,dnncCurrentDis,dnnpidDropoffHeat,locHash,dnnPGlobalTime,dnnDGlobalTime,dnnpoiDropoffRatio,pickupHash,dnnpidLFChargeTime,dnnpidPickupHeat,dnnNpoiDis,dnnpidTrajRatio,dnnpidTrajNum,dnnpidRatio,wifi,dnnpsw,dnnpidLBChargeTime,dnnpoiRatio\n",
      "\n",
      ">>> dnnCrsCurRoad:\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'dnnCrsCurRoad'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-82-30ecbb4355b9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mto_print\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\">>> {}:\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m         \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m: 'dnnCrsCurRoad'"
     ]
    }
   ],
   "source": [
    "trd_dir = \"/nfs/map-tmp-vol0/zhoutong/Data/BR_MMOE_ori_test\"\n",
    "trd_fp = [os.path.join(trd_dir,i) for i in os.listdir(trd_dir)]\n",
    "wide_size = 35\n",
    "config_path = open('/tmp-data/zhoutongzt/DD/tensorflow_lr/src/main/resources/features_oriFeat.json','r')\n",
    "conf = json.load(config_path)\n",
    "features={}\n",
    "features['wideFeatures'] = tf.io.FixedLenFeature([wide_size], tf.int64)\n",
    "# features['label'] = tf.io.FixedLenFeature([1], tf.int64)\n",
    "for deepName in conf[\"deepFeat\"]:\n",
    "    fshape = conf[\"varlensFeat_length\"].get(deepName, None)\n",
    "    fshape = [] if fshape is None else [fshape]\n",
    "    fdtype = tf.float32 if deepName in conf[\"floatFeat\"] else tf.int64\n",
    "    features[deepName] = tf.io.FixedLenFeature(fshape, fdtype)\n",
    "\n",
    "print(\"[features.keys]: \"+\", \".join(sorted(features.keys())))\n",
    "def _parse_func(inp):\n",
    "    parsed_features = tf.io.parse_single_example(inp,features)\n",
    "    return parsed_features\n",
    "\n",
    "trd=tf.data.TFRecordDataset(trd_fp,compression_type='GZIP').map(_parse_func)\n",
    "\n",
    "####################\n",
    "# tf1.x specific\n",
    "####################\n",
    "dataset = trd.batch(64,drop_remainder=True)\n",
    "line = dataset.make_one_shot_iterator().get_next()\n",
    "to_print = [\"dnnCrsCurRoad\",]\n",
    "with tf.Session() as sess:\n",
    "    i = sess.run(line)\n",
    "    print(\">>> all keys: {}\\n\".format(\",\".join(i.keys())))\n",
    "    for key in to_print:\n",
    "        print(\">>> {}:\".format(key))\n",
    "        print(i[key])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 处理TFRecord | tf2.x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 检查schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trd_dir = \"/nfs/map-tmp-vol0/zhoutong/Data/BR_useReqTime_addCross_addTimeByFreq_train\"\n",
    "trd_fp = [os.path.join(trd_dir,i) for i in os.listdir(trd_dir)]\n",
    "trd_unparsed=tf.data.TFRecordDataset(trd_fp,compression_type='GZIP')\n",
    "\n",
    "for raw_record in trd_unparsed.take(1):\n",
    "    example = tf.train.Example()\n",
    "    _ = example.ParseFromString(raw_record.numpy())\n",
    "    \n",
    "def format_exampleInfo(exampleInfo):\n",
    "    res = []\n",
    "    for name,feat in exampleInfo.features.feature.items():\n",
    "        info_list=[(name, attr, feat.__getattribute__(attr).value) for attr in [\"bytes_list\",\"float_list\",\"int64_list\"]]\n",
    "        info = [(name,attr,value) for name,attr,value in info_list if value != []][0]\n",
    "        res.append(\"[featName]:{} [type]:{} [value_example]:{}\".format(*info))\n",
    "    print(\"\\n\".join(sorted(map(str,res))))\n",
    "    \n",
    "format_exampleInfo(example)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 解析"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trd_dir = \"/nfs/map-tmp-vol0/zhoutong/Data/BR_useReqTime_addCross_addTimeByFreq_addTimeHeatCross_test\"\n",
    "trd_fp = [os.path.join(trd_dir,i) for i in os.listdir(trd_dir)]\n",
    "wide_size = 48\n",
    "config_path = open('/tmp-data/zhoutongzt/DD/tensorflow_lr/src/main/resources/features_useReqTime_addCross.json','r')\n",
    "# config_path = open('/tmp-data/zhoutongzt/DD/tensorflow_lr/src/main/resources/features_oriFeat.json','r')\n",
    "conf = json.load(config_path)\n",
    "features={}\n",
    "features['wideFeatures'] = tf.io.FixedLenFeature([wide_size], tf.int64)\n",
    "features['label'] = tf.io.FixedLenFeature([1], tf.int64)\n",
    "for deepName in conf[\"deepFeat\"]:\n",
    "    fshape = conf[\"varlensFeat_length\"].get(deepName, None)\n",
    "    fshape = [] if fshape is None else [fshape]\n",
    "    fdtype = tf.float32 if deepName in conf[\"floatFeat\"] else tf.int64\n",
    "    features[deepName] = tf.io.FixedLenFeature(fshape, fdtype)\n",
    "\n",
    "print(\"[features.keys]: \"+\", \".join(sorted(features.keys())))\n",
    "def _parse_func(inp):\n",
    "    parsed_features = tf.io.parse_single_example(inp,features)\n",
    "    return parsed_features\n",
    "\n",
    "trd=tf.data.TFRecordDataset(trd_fp,compression_type='GZIP').map(_parse_func)\n",
    "\n",
    "####################\n",
    "# tf2.x specific\n",
    "####################\n",
    "for i in trd.take(1):\n",
    "    print(i)\n",
    "    print(\"\\n[keys]:\" + str(i.keys()))\n",
    "    print(\"\\n[heat_time_idx_poi_id]: \" + str(i['heat_time_idx_poi_id']))\n",
    "    print(\"[heat_time_idx_pickup_geoHash]: \" + str(i['heat_time_idx_pickup_geoHash']))\n",
    "    print(\"[heat_time_idx_bind_line_id_single]: \" + str(i['heat_time_idx_bind_line_id_single']))\n",
    "#     print(\"[weights]:\" + str(i['weights']))\n",
    "#     print(\"[order_info]:\" + str(i['order_info']))\n",
    "    print(i['wideFeatures'].shape)\n",
    "    \n",
    "# 跑不出来377w数太多了？\n",
    "# set([i['wideFeatures'].shape for i in trd.as_numpy_iterator()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# feature_column TEST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## categorical_column_with_hash_bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-02T06:56:06.895537Z",
     "start_time": "2020-12-02T06:56:06.859909Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'featA': array([[2],\n",
       "        [6]]), 'featB': array([[4],\n",
       "        [2]]), 'featC': array([[5],\n",
       "        [6]]), 'featD': array([[0],\n",
       "        [4]]), 'featE': array([[4],\n",
       "        [6]]), 'pickupHash': array([[3],\n",
       "        [5]])}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_buckets_deep=7\n",
    "dims_embs_deep=4\n",
    "_batch_size=2\n",
    "\n",
    "# features\n",
    "features = {}\n",
    "features[\"featA\"] = np.random.randint(num_buckets_deep, size=(_batch_size, 1))\n",
    "features[\"featB\"] = np.random.randint(num_buckets_deep, size=(_batch_size, 1))\n",
    "features[\"featC\"] = np.random.randint(num_buckets_deep, size=(_batch_size, 1))\n",
    "features[\"featD\"] = np.random.randint(num_buckets_deep, size=(_batch_size, 1))\n",
    "features[\"featE\"] = np.random.randint(num_buckets_deep, size=(_batch_size, 1))\n",
    "features[\"pickupHash\"] = np.random.randint(num_buckets_deep, size=(_batch_size, 1))\n",
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-02T07:03:39.745875Z",
     "start_time": "2020-12-02T07:03:39.403303Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'input_layer/concat:0' shape=(2, 7) dtype=float32>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'input_layer_1/concat:0' shape=(2, 10) dtype=float32>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 1., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 1.]], dtype=float32)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 1., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "# codec column\n",
    "\n",
    "featA_cat_id_column = fc.categorical_column_with_identity(\"featA\", num_buckets_deep, 0)\n",
    "featA_cat_hash_column = fc.categorical_column_with_hash_bucket(\"featA\", hash_bucket_size=10, dtype=tf.int64)\n",
    "\n",
    "with tf.Session():\n",
    "    fc.input_layer(features, fc.indicator_column(featA_cat_id_column))\n",
    "    fc.input_layer(features, fc.indicator_column(featA_cat_hash_column))\n",
    "    fc.input_layer(features, fc.indicator_column(featA_cat_id_column)).eval()\n",
    "    fc.input_layer(features, fc.indicator_column(featA_cat_hash_column)).eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## cross_column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### basic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-09T07:16:28.052382Z",
     "start_time": "2020-12-09T07:16:27.996807Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'featA': array([[3],\n",
       "        [2]]), 'featB': array([[4],\n",
       "        [3]]), 'featC': array([[6],\n",
       "        [2]]), 'featD': array([[6],\n",
       "        [0]]), 'featE': array([[5],\n",
       "        [5]]), 'wideFeatures': array([[11, 26, 44, 53],\n",
       "        [61, 87, 14, 99]])}"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>featA</th>\n",
       "      <th>featB</th>\n",
       "      <th>featC</th>\n",
       "      <th>featD</th>\n",
       "      <th>featE</th>\n",
       "      <th>wideFeatures</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[3]</td>\n",
       "      <td>[4]</td>\n",
       "      <td>[6]</td>\n",
       "      <td>[6]</td>\n",
       "      <td>[5]</td>\n",
       "      <td>[11, 26, 44, 53]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[2]</td>\n",
       "      <td>[3]</td>\n",
       "      <td>[2]</td>\n",
       "      <td>[0]</td>\n",
       "      <td>[5]</td>\n",
       "      <td>[61, 87, 14, 99]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  featA featB featC featD featE      wideFeatures\n",
       "0   [3]   [4]   [6]   [6]   [5]  [11, 26, 44, 53]\n",
       "1   [2]   [3]   [2]   [0]   [5]  [61, 87, 14, 99]"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_buckets_deep=7\n",
    "dims_embs_deep=4\n",
    "_batch_size=2\n",
    "\n",
    "# features\n",
    "features = {}\n",
    "features[\"featA\"] = np.random.randint(num_buckets_deep, size=(_batch_size, 1))\n",
    "features[\"featB\"] = np.random.randint(num_buckets_deep, size=(_batch_size, 1))\n",
    "features[\"featC\"] = np.random.randint(num_buckets_deep, size=(_batch_size, 1))\n",
    "features[\"featD\"] = np.random.randint(num_buckets_deep, size=(_batch_size, 1))\n",
    "features[\"featE\"] = np.random.randint(num_buckets_deep, size=(_batch_size, 1))\n",
    "features[\"wideFeatures\"] = np.random.randint(100, size=(_batch_size, 4))\n",
    "features\n",
    "pd.DataFrame({k:v.tolist() for k,v in features.items()})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-09T07:23:46.023062Z",
     "start_time": "2020-12-09T07:23:44.892891Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> featA_indicator:\n",
      "[[0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0.]]\n",
      ">>> featB_indicator:\n",
      "[[0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0.]]\n",
      ">>> featC_indicator:\n",
      "[[0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 1. 0. 0. 0. 0.]]\n",
      ">>> featA_X_featB_indicator:\n",
      "[[0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]]\n",
      ">>> featA_X_featB_embedding:\n",
      "[[ 0.8866521  -0.9491471  -0.28430918  0.72624975]\n",
      " [ 0.5315663   0.4613311   0.8415003  -0.01883175]]\n",
      ">>> featA_X_featB_X_featC_indicator:\n",
      "[[0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]]\n",
      ">>> featA_X_featB_X_featC_embedding:\n",
      "[[ 0.5352437   0.12370125  0.41470584  0.4559309 ]\n",
      " [-0.55044717 -0.02150568  0.01373834 -0.32568774]]\n",
      ">>> wideFeatures_indicator:\n",
      "[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 1.]]\n",
      ">>> featA_X_featB_shared_embedding:\n",
      "[[ 0.51776284  0.27670813  0.2168164   0.5317676 ]\n",
      " [ 0.13072243 -0.20055397 -0.38618922  0.502116  ]]\n",
      ">>> featA_X_featB_X_featC_shared_embedding:\n",
      "[[-0.03766352  0.9150562  -0.5249452  -0.2893404 ]\n",
      " [ 0.13072243 -0.20055397 -0.38618922  0.502116  ]]\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "# codec column\n",
    "fc_wide = fc.categorical_column_with_identity(\"wideFeatures\", 100)\n",
    "fc_idx_wide = fc.indicator_column(fc_wide)\n",
    "\n",
    "featA_cat_column = fc.categorical_column_with_identity(\"featA\", num_buckets_deep, 0)\n",
    "featA_idx_column = fc.indicator_column(featA_cat_column)\n",
    "\n",
    "featB_cat_column = fc.categorical_column_with_identity(\"featB\", num_buckets_deep, 0)\n",
    "featB_idx_column = fc.indicator_column(featB_cat_column)\n",
    "\n",
    "featC_cat_column = fc.categorical_column_with_identity(\"featC\", num_buckets_deep, 0)\n",
    "featC_idx_column = fc.indicator_column(featC_cat_column)\n",
    "\n",
    "featAxB_cat_column = fc.crossed_column([\"featA\",\"featB\"], hash_bucket_size=10)\n",
    "featAxB_idx_column = fc.indicator_column(featAxB_cat_column)\n",
    "featAxB_emb_column = fc.embedding_column(featAxB_cat_column, dimension=dims_embs_deep)\n",
    "\n",
    "featAxBxC_cat_column = fc.crossed_column([\"featA\",\"featB\",\"featC\"], hash_bucket_size=10)\n",
    "featAxBxC_idx_column = fc.indicator_column(featAxBxC_cat_column)\n",
    "featAxBxC_emb_column = fc.embedding_column(featAxBxC_cat_column, dimension=dims_embs_deep)\n",
    "\n",
    "shaerd_emb_columns = fc.shared_embedding_columns([featAxB_cat_column, featAxBxC_cat_column], dimension=dims_embs_deep)\n",
    "\n",
    "to_be_input = [featA_idx_column,\n",
    "               featB_idx_column,\n",
    "               featC_idx_column,\n",
    "               featAxB_idx_column,\n",
    "               featAxB_emb_column,\n",
    "               featAxBxC_idx_column,\n",
    "               featAxBxC_emb_column,\n",
    "               fc_idx_wide]+shaerd_emb_columns\n",
    "inp_layer_list = [fc.input_layer(features,i) for i in to_be_input]\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer()) \n",
    "    inp_list = sess.run(inp_layer_list)\n",
    "    for k,v in zip([i.name for i in to_be_input], inp_list):\n",
    "        print(\">>> {}:\\n{}\".format(k,v))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 如何直接把cross加到wideFeatures里(list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-09T07:56:07.879683Z",
     "start_time": "2020-12-09T07:56:07.755798Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_IdentityCategoricalColumn(key='wideFeatures', num_buckets=100, default_value=None)"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "_CrossedColumn(keys=('featA', 'featB'), hash_bucket_size=10, hash_key=None)"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "[_IdentityCategoricalColumn(key='wideFeatures', num_buckets=100, default_value=None),\n",
       " _CrossedColumn(keys=('featA', 'featB'), hash_bucket_size=10, hash_key=None)]"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "_IndicatorColumn(categorical_column=[_IdentityCategoricalColumn(key='wideFeatures', num_buckets=100, default_value=None), _CrossedColumn(keys=('featA', 'featB'), hash_bucket_size=10, hash_key=None)])"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'input_layer_15/concat:0' shape=(2, 110) dtype=float32>"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fc_wide\n",
    "featAxB_cat_column\n",
    "fc_inp = [fc_wide, featAxB_cat_column]\n",
    "fc_inp\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    # 注意在LinearModel里传给它的featureColumn用fc_inp就行了，它内部自行做了indicator_column\n",
    "    fc.input_layer(features, [fc.indicator_column(i) for i in fc_inp])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interact emb by Add | fake Positional-Emb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### basic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-18T07:27:44.217697Z",
     "start_time": "2020-11-18T07:27:44.151838Z"
    },
    "run_control": {
     "marked": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'featA': array([[19],\n",
      "       [19]]), 'pickupHash': array([[15],\n",
      "       [ 8]]), 'time_idx': array([[9],\n",
      "       [1]]), 'numericB': [[0.5], [0.2]], 'numericA': [[0.5], [1.2]], 'link': [[-1], [-1]], 'hour_idx': array([[5],\n",
      "       [2]]), 'whateverB': array([[15],\n",
      "       [ 0]]), 'whateverA': array([[17],\n",
      "       [16]]), 'featB': array([[1],\n",
      "       [2]])}\n"
     ]
    }
   ],
   "source": [
    "##############\n",
    "# h-params\n",
    "##############\n",
    "num_buckets_deep=20\n",
    "dims_embs_deep=4\n",
    "_batch_size=2\n",
    "\n",
    "##############\n",
    "# features\n",
    "##############\n",
    "features = {}\n",
    "features[\"featA\"] = np.random.randint(num_buckets_deep, size=(_batch_size, 1))\n",
    "features[\"featB\"] = np.random.randint(num_buckets_deep, size=(_batch_size, 1))\n",
    "features[\"link\"] = [[-1],[-1]]\n",
    "features[\"pickupHash\"] = np.random.randint(num_buckets_deep, size=(_batch_size, 1))\n",
    "features[\"hour_idx\"] = np.random.randint(num_buckets_deep, size=(_batch_size, 1))\n",
    "features[\"time_idx\"] = np.random.randint(num_buckets_deep, size=(_batch_size, 1))\n",
    "features[\"whateverA\"] = np.random.randint(num_buckets_deep, size=(_batch_size, 1))\n",
    "features[\"whateverB\"] = np.random.randint(num_buckets_deep, size=(_batch_size, 1))\n",
    "features[\"numericA\"] = [[0.5],[1.2]]\n",
    "features[\"numericB\"] = [[0.5],[0.2]]\n",
    "print(features)\n",
    "\n",
    "################\n",
    "# build columns\n",
    "################\n",
    "tf.reset_default_graph()\n",
    "# numeric column\n",
    "fc_numeric = list()\n",
    "fc_numeric.append(fc.numeric_column(\"numericA\", default_value=0))\n",
    "fc_numeric.append(fc.numeric_column(\"numericB\", default_value=0))\n",
    "\n",
    "# codec column\n",
    "emb_feat_list = list()\n",
    "for i in [\"featA\",\"featB\",\"link\",\"pickupHash\",\"hour_idx\",\"time_idx\"]:\n",
    "    emb_feat_list.append(fc.categorical_column_with_identity(i, num_buckets_deep, default_value=0))\n",
    "\n",
    "# emb column\n",
    "fc_shared_emb = fc.shared_embedding_columns(\n",
    "                    emb_feat_list,\n",
    "                    dimension=dims_embs_deep, max_norm=np.sqrt(dims_embs_deep),\n",
    "                    trainable=True)\n",
    "\n",
    "# shared_emb 转成KV结构方便后面操作\n",
    "fc_emb_dict = {seCol.categorical_column.key: seCol for seCol in fc_shared_emb}\n",
    "\n",
    "# 指出将要相加的emb\n",
    "emb_to_add=[['link','hour_idx'],\n",
    "            ['pickupHash','time_idx']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### input_layer搞事情"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-18T07:27:51.079465Z",
     "start_time": "2020-11-18T07:27:48.288221Z"
    }
   },
   "outputs": [],
   "source": [
    "emb_added = []\n",
    "for f1,f2 in emb_to_add:\n",
    "    net1 = fc.input_layer(features, fc_emb_dict[f1])\n",
    "    net2 = fc.input_layer(features, fc_emb_dict[f2])\n",
    "    emb_added.append(net1+net2)\n",
    "\n",
    "emb_to_add_flatten = [i for sub in emb_to_add for i in sub]\n",
    "net_ori = fc.input_layer(features, [v for k,v in fc_emb_dict.items() if k not in emb_to_add_flatten])\n",
    "\n",
    "net = tf.concat([net_ori]+emb_added,axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### run起来看看结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-18T07:28:29.698347Z",
     "start_time": "2020-11-18T07:28:27.741264Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('net_ori',\n",
       " array([[-0.03376518, -0.75637287,  0.01980731,  0.57804877, -0.41253054,\n",
       "          0.76133245,  0.21623057,  0.34377784],\n",
       "        [-0.03376518, -0.75637287,  0.01980731,  0.57804877,  0.5149282 ,\n",
       "         -0.06459129, -0.2458921 , -0.7198298 ]], dtype=float32))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "('emb_added', [array([[-0.2679743 , -0.51504064,  0.3350453 ,  0.47310346],\n",
       "         [ 0.5149282 , -0.06459129, -0.2458921 , -0.7198298 ]],\n",
       "        dtype=float32),\n",
       "  array([[-0.02274224,  1.3028573 , -0.6851692 ,  0.33623815],\n",
       "         [ 0.28992552,  1.1738838 ,  0.27457008,  0.5552162 ]],\n",
       "        dtype=float32)])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "('net',\n",
       " array([[-0.03376518, -0.75637287,  0.01980731,  0.57804877, -0.41253054,\n",
       "          0.76133245,  0.21623057,  0.34377784, -0.2679743 , -0.51504064,\n",
       "          0.3350453 ,  0.47310346, -0.02274224,  1.3028573 , -0.6851692 ,\n",
       "          0.33623815],\n",
       "        [-0.03376518, -0.75637287,  0.01980731,  0.57804877,  0.5149282 ,\n",
       "         -0.06459129, -0.2458921 , -0.7198298 ,  0.5149282 , -0.06459129,\n",
       "         -0.2458921 , -0.7198298 ,  0.28992552,  1.1738838 ,  0.27457008,\n",
       "          0.5552162 ]], dtype=float32))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> details of emb_added\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('link', array([[0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.]], dtype=float32))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "('hour_idx', array([[-0.2679743 , -0.51504064,  0.3350453 ,  0.47310346],\n",
       "        [ 0.5149282 , -0.06459129, -0.2458921 , -0.7198298 ]],\n",
       "       dtype=float32))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "('pickupHash', array([[-0.5041411 ,  0.9702361 , -0.7937973 , -0.33923268],\n",
       "        [ 0.70245606,  0.41255134,  0.05833951,  0.21143836]],\n",
       "       dtype=float32))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "('time_idx', array([[ 0.48139885,  0.33262116,  0.10862812,  0.6754708 ],\n",
       "        [-0.41253054,  0.76133245,  0.21623057,  0.34377784]],\n",
       "       dtype=float32))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \"net_ori\",sess.run(net_ori)\n",
    "    \"emb_added\",sess.run(emb_added)\n",
    "    \"net\",sess.run(net)\n",
    "    print(\">>> details of emb_added\")\n",
    "    for f1,f2 in emb_to_add:\n",
    "        net1 = fc.input_layer(features, fc_emb_dict[f1])\n",
    "        net2 = fc.input_layer(features, fc_emb_dict[f2])\n",
    "        f1,sess.run(net1)\n",
    "        f2,sess.run(net2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interact emb by Inner-product"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### basic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-07T13:18:24.328638Z",
     "start_time": "2020-12-07T13:18:24.298252Z"
    },
    "run_control": {
     "marked": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "featA\n",
      "[[3]\n",
      " [4]]\n",
      "pickupHash\n",
      "[[6]\n",
      " [2]]\n",
      "featB\n",
      "[[6]\n",
      " [1]]\n",
      "link\n",
      "[[-1], [2]]\n",
      "featC\n",
      "[[5]\n",
      " [2]]\n",
      "hour_idx\n",
      "[[5]\n",
      " [3]]\n",
      "time_idx\n",
      "[[0]\n",
      " [1]]\n"
     ]
    }
   ],
   "source": [
    "num_buckets_deep=7\n",
    "dims_embs_deep=3\n",
    "_batch_size=2\n",
    "\n",
    "# features\n",
    "features = {}\n",
    "features[\"featA\"] = np.random.randint(num_buckets_deep, size=(_batch_size, 1))\n",
    "features[\"featB\"] = np.random.randint(num_buckets_deep, size=(_batch_size, 1))\n",
    "features[\"featC\"] = np.random.randint(num_buckets_deep, size=(_batch_size, 1))\n",
    "features[\"link\"] = [[-1],[2]]\n",
    "assert(len(features[\"link\"]) == _batch_size)\n",
    "features[\"pickupHash\"] = np.random.randint(num_buckets_deep, size=(_batch_size, 1))\n",
    "features[\"hour_idx\"] = np.random.randint(num_buckets_deep, size=(_batch_size, 1))\n",
    "features[\"time_idx\"] = np.random.randint(num_buckets_deep, size=(_batch_size, 1))\n",
    "\n",
    "for k,v in features.items():\n",
    "    print(k)\n",
    "    print(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-07T16:01:46.213898Z",
     "start_time": "2020-12-07T16:01:46.192106Z"
    },
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "# numeric column\n",
    "fc_numeric = list()\n",
    "fc_numeric.append(fc.numeric_column(\"numericA\", default_value=0))\n",
    "fc_numeric.append(fc.numeric_column(\"numericB\", default_value=0))\n",
    "\n",
    "# codec column\n",
    "emb_feat_list = list()\n",
    "for i in [\"featA\",\"featB\",\"featC\",\"link\",\"pickupHash\",\"hour_idx\",\"time_idx\"]:\n",
    "    emb_feat_list.append(fc.categorical_column_with_identity(i, num_buckets_deep, default_value=0))\n",
    "\n",
    "# emb column\n",
    "fc_shared_emb = fc.shared_embedding_columns(\n",
    "                    emb_feat_list,\n",
    "                    dimension=dims_embs_deep, max_norm=np.sqrt(dims_embs_deep),\n",
    "                    trainable=True)\n",
    "\n",
    "# shared_emb 转成KV结构方便后面操作\n",
    "fc_emb_dict = {seCol.categorical_column.key: seCol for seCol in fc_shared_emb}\n",
    "\n",
    "# 指出将要相加的emb\n",
    "emb_to_add=[['link','hour_idx'],\n",
    "            ['pickupHash','time_idx'],\n",
    "            ['featA','featB']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-07T16:07:24.659568Z",
     "start_time": "2020-12-07T16:07:22.703016Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(u'link_shared_embedding_hour_idx_shared_embedding_3:0',\n",
       " array([ 0.        , -0.05191137], dtype=float32))"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(u'pickupHash_shared_embedding_time_idx_shared_embedding_3:0',\n",
       " array([0.05215313, 0.04003584], dtype=float32))"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(u'featA_shared_embedding_featB_shared_embedding_3:0',\n",
       " array([-0.04963669, -0.01114402], dtype=float32))"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "array([[ 0.        ,  0.05215313, -0.04963669],\n",
       "       [-0.05191137,  0.04003584, -0.01114402]], dtype=float32)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "array([[ 0.3013491 ,  0.99670464,  0.9063871 ],\n",
       "       [-0.3013491 , -0.99670464, -0.9063875 ]], dtype=float32)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fc_cross_2Dlist=list()\n",
    "for emb_list in emb_to_add:\n",
    "    fc_cross_2Dlist.append([fc_emb_dict[i] for i in emb_list])\n",
    "\n",
    "cross_logits = list()\n",
    "for fc_cross_list in fc_cross_2Dlist:\n",
    "    assert len(fc_cross_list) == 2\n",
    "    fc0, fc1 = fc_cross_list\n",
    "    net0 = fc.input_layer(features, fc0)\n",
    "    net1 = fc.input_layer(features, fc1)\n",
    "    cross_logits.append(tf.reduce_sum(net0*net1, axis=1, name=\"%s_%s\" % (fc0.name,fc1.name)))\n",
    "    \n",
    "# 直接把点积加起来作为interaction_logits | 不行，logits是四分类，四维的\n",
    "all_emb_dot = tf.stack(cross_logits, axis=1)\n",
    "all_emb_dot_bn = tf.contrib.layers.batch_norm(all_emb_dot, is_training=True, updates_collections=None)\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for i in cross_logits:\n",
    "        i.name,i.eval()\n",
    "    all_emb_dot_bn.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-07T15:49:39.153660Z",
     "start_time": "2020-12-07T15:49:38.201997Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([Dimension(2), Dimension(3)])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "array([[ 0.6932142 ,  0.49308798,  0.02976007],\n",
       "       [-0.5361837 , -0.19622478,  0.5870663 ]], dtype=float32)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "array([[-0.5361837 , -0.19622478,  0.5870663 ],\n",
       "       [-0.51281106,  0.42062   , -0.9718037 ]], dtype=float32)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "-0.37169015"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "-0.09675608"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "array([[-0.37169015, -0.09675608,  0.01747113],\n",
       "       [ 0.27496094, -0.08253606, -0.5705132 ]], dtype=float32)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "array([-0.4509751 , -0.37808833], dtype=float32)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fc.input_layer(features,fc_emb_dict[\"hour_idx\"]).shape\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    fc0=fc.input_layer(features,fc_emb_dict[\"hour_idx\"])\n",
    "    fc1=fc.input_layer(features,fc_emb_dict[\"featA\"])\n",
    "    fc0.eval()\n",
    "    fc1.eval()\n",
    "    fc0.eval()[0][0] * fc1.eval()[0][0]\n",
    "    fc0.eval()[0][1] * fc1.eval()[0][1]\n",
    "    (fc0*fc1).eval()\n",
    "    tf.reduce_sum(fc0*fc1, axis=1).eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### input_layer操作起来\n",
    "\n",
    "format by: [[\"hour_idx\",\"featA\"], [\"hour_idx\",\"featB\"], [\"hour_idx\", \"featC\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-07T13:18:27.298142Z",
     "start_time": "2020-12-07T13:18:26.022675Z"
    },
    "run_control": {
     "marked": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> last-one as demo\n",
      "pickupHash\n",
      "[[-0.4284786  -0.26747006 -0.17833763]\n",
      " [ 0.16898142  0.38039657 -0.26833487]]\n",
      "hour_idx\n",
      "[[-0.7861729   0.2811521  -0.30317506]\n",
      " [ 0.6207778   0.12892464  0.10991306]]\n",
      "_res | f1 f2做内积得到\n",
      "[[ 0.33685827 -0.07519977  0.05406752]\n",
      " [ 0.10489991  0.04904249 -0.02949351]]\n",
      "res | _res做reduce_sum得到\n",
      "[0.315726  0.1244489]\n",
      ">>> emb_dot\n",
      "[array([0.       , 0.1244489], dtype=float32), array([0.315726 , 0.1244489], dtype=float32)]\n",
      ">>> fm_2nd_res\n",
      "[0.315726  0.2488978]\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "emb_to_innerProduct = [\"link\",\"pickupHash\"]\n",
    "emb_to_innerProduct = [[i,\"hour_idx\"] for i in emb_to_innerProduct]\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    emb_dot = []\n",
    "    for f1,f2 in emb_to_innerProduct:\n",
    "        net1 = fc.input_layer(features, fc_emb_dict[f1])\n",
    "        net2 = fc.input_layer(features, fc_emb_dict[f2])\n",
    "        _res = net1 * net2\n",
    "        res = tf.reduce_sum(net1*net2, axis=1)\n",
    "        emb_dot.append(res)\n",
    "\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    print(\">>> last-one as demo\")\n",
    "    print(f1)\n",
    "    print(sess.run(net1))\n",
    "    print(f2)\n",
    "    print(sess.run(net2))\n",
    "    print(\"_res | f1 f2做内积得到\")\n",
    "    print(sess.run(_res))\n",
    "    print(\"res | _res做reduce_sum得到\")\n",
    "    print(sess.run(res))\n",
    "    \n",
    "    \n",
    "    print(\">>> emb_dot\")\n",
    "    print(sess.run(emb_dot))\n",
    "    fm_2nd_res = tf.reduce_sum(emb_dot, axis=0)\n",
    "    print(\">>> fm_2nd_res\")\n",
    "    print(sess.run(fm_2nd_res))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-07T13:19:29.896803Z",
     "start_time": "2020-12-07T13:19:29.871174Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Variable 'abc/Variable:0' shape=(1,) dtype=float32_ref>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<tf.Variable 'bbc/Variable:0' shape=(1,) dtype=float32_ref>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with tf.variable_scope(\"abc\"):\n",
    "    tf.Variable([1.0],\"m\")\n",
    "    \n",
    "with tf.variable_scope(\"bbc\"):\n",
    "    tf.Variable([1.0],\"k\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-07T13:21:22.634414Z",
     "start_time": "2020-12-07T13:21:22.267800Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Variable 'abc/Variable:0' shape=(1,) dtype=float32_ref>,\n",
       " <tf.Variable 'bbc/Variable:0' shape=(1,) dtype=float32_ref>]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "ename": "TypeError",
     "evalue": "unhashable type: 'list'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-c853cde073ae>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_collection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"trainable_variables\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscope\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"abc\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_collection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"trainable_variables\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscope\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"bbc\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_collection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"trainable_variables\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscope\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"abc\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"bbc\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp-data/luban/anaconda3/envs/tf1.12/lib/python2.7/site-packages/tensorflow/python/framework/ops.pyc\u001b[0m in \u001b[0;36mget_collection\u001b[0;34m(key, scope)\u001b[0m\n\u001b[1;32m   5925\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mend_compatibility\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5926\u001b[0m   \"\"\"\n\u001b[0;32m-> 5927\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mget_default_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_collection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5928\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5929\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp-data/luban/anaconda3/envs/tf1.12/lib/python2.7/site-packages/tensorflow/python/framework/ops.pyc\u001b[0m in \u001b[0;36mget_collection\u001b[0;34m(self, name, scope)\u001b[0m\n\u001b[1;32m   3845\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3846\u001b[0m         \u001b[0mc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3847\u001b[0;31m         \u001b[0mregex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3848\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcollection\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3849\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"name\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mregex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp-data/luban/anaconda3/envs/tf1.12/lib/python2.7/re.pyc\u001b[0m in \u001b[0;36mcompile\u001b[0;34m(pattern, flags)\u001b[0m\n\u001b[1;32m    192\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpattern\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    193\u001b[0m     \u001b[0;34m\"Compile a regular expression pattern, returning a pattern object.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 194\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_compile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpattern\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    195\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mpurge\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp-data/luban/anaconda3/envs/tf1.12/lib/python2.7/re.pyc\u001b[0m in \u001b[0;36m_compile\u001b[0;34m(*key)\u001b[0m\n\u001b[1;32m    235\u001b[0m         \u001b[0mcachekey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 237\u001b[0;31m             \u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_cache\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcachekey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    238\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mloc\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mloc\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_locale\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetlocale\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_locale\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLC_CTYPE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    239\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: unhashable type: 'list'"
     ]
    }
   ],
   "source": [
    "tf.get_collection(\"trainable_variables\", scope=\"abc\") + tf.get_collection(\"trainable_variables\", scope=\"bbc\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### input_layer操作起来 | 字典形式\n",
    "\n",
    "format by: {\"hour_idx\": [\"featA\", \"featB\", \"featC\"], \"time_idx\": [\"featA\", \"featB\", \"featC\"]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-19T08:28:52.679488Z",
     "start_time": "2020-11-19T08:28:52.668988Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function batch_norm in module tensorflow.contrib.layers.python.layers.layers:\n",
      "\n",
      "batch_norm(*args, **kwargs)\n",
      "    Adds a Batch Normalization layer from http://arxiv.org/abs/1502.03167.\n",
      "    \n",
      "      \"Batch Normalization: Accelerating Deep Network Training by Reducing\n",
      "      Internal Covariate Shift\"\n",
      "    \n",
      "      Sergey Ioffe, Christian Szegedy\n",
      "    \n",
      "    Can be used as a normalizer function for conv2d and fully_connected. The\n",
      "    normalization is over all but the last dimension if `data_format` is `NHWC`\n",
      "    and all but the second dimension if `data_format` is `NCHW`.  In case of a 2D\n",
      "    tensor this corresponds to the batch dimension, while in case of a 4D tensor\n",
      "    this\n",
      "    corresponds to the batch and space dimensions.\n",
      "    \n",
      "    Note: when training, the moving_mean and moving_variance need to be updated.\n",
      "    By default the update ops are placed in `tf.GraphKeys.UPDATE_OPS`, so they\n",
      "    need to be added as a dependency to the `train_op`. For example:\n",
      "    \n",
      "    ```python\n",
      "      update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
      "      with tf.control_dependencies(update_ops):\n",
      "        train_op = optimizer.minimize(loss)\n",
      "    ```\n",
      "    \n",
      "    One can set updates_collections=None to force the updates in place, but that\n",
      "    can have a speed penalty, especially in distributed settings.\n",
      "    \n",
      "    Args:\n",
      "      inputs: A tensor with 2 or more dimensions, where the first dimension has\n",
      "        `batch_size`. The normalization is over all but the last dimension if\n",
      "        `data_format` is `NHWC` and the second dimension if `data_format` is\n",
      "        `NCHW`.\n",
      "      decay: Decay for the moving average. Reasonable values for `decay` are close\n",
      "        to 1.0, typically in the multiple-nines range: 0.999, 0.99, 0.9, etc.\n",
      "        Lower `decay` value (recommend trying `decay`=0.9) if model experiences\n",
      "        reasonably good training performance but poor validation and/or test\n",
      "        performance. Try zero_debias_moving_mean=True for improved stability.\n",
      "      center: If True, add offset of `beta` to normalized tensor. If False, `beta`\n",
      "        is ignored.\n",
      "      scale: If True, multiply by `gamma`. If False, `gamma` is\n",
      "        not used. When the next layer is linear (also e.g. `nn.relu`), this can be\n",
      "        disabled since the scaling can be done by the next layer.\n",
      "      epsilon: Small float added to variance to avoid dividing by zero.\n",
      "      activation_fn: Activation function, default set to None to skip it and\n",
      "        maintain a linear activation.\n",
      "      param_initializers: Optional initializers for beta, gamma, moving mean and\n",
      "        moving variance.\n",
      "      param_regularizers: Optional regularizer for beta and gamma.\n",
      "      updates_collections: Collections to collect the update ops for computation.\n",
      "        The updates_ops need to be executed with the train_op.\n",
      "        If None, a control dependency would be added to make sure the updates are\n",
      "        computed in place.\n",
      "      is_training: Whether or not the layer is in training mode. In training mode\n",
      "        it would accumulate the statistics of the moments into `moving_mean` and\n",
      "        `moving_variance` using an exponential moving average with the given\n",
      "        `decay`. When it is not in training mode then it would use the values of\n",
      "        the `moving_mean` and the `moving_variance`.\n",
      "      reuse: Whether or not the layer and its variables should be reused. To be\n",
      "        able to reuse the layer scope must be given.\n",
      "      variables_collections: Optional collections for the variables.\n",
      "      outputs_collections: Collections to add the outputs.\n",
      "      trainable: If `True` also add variables to the graph collection\n",
      "        `GraphKeys.TRAINABLE_VARIABLES` (see `tf.Variable`).\n",
      "      batch_weights: An optional tensor of shape `[batch_size]`,\n",
      "        containing a frequency weight for each batch item. If present,\n",
      "        then the batch normalization uses weighted mean and\n",
      "        variance. (This can be used to correct for bias in training\n",
      "        example selection.)\n",
      "      fused: if `None` or `True`, use a faster, fused implementation if possible.\n",
      "        If `False`, use the system recommended implementation.\n",
      "      data_format: A string. `NHWC` (default) and `NCHW` are supported.\n",
      "      zero_debias_moving_mean: Use zero_debias for moving_mean. It creates a new\n",
      "        pair of variables 'moving_mean/biased' and 'moving_mean/local_step'.\n",
      "      scope: Optional scope for `variable_scope`.\n",
      "      renorm: Whether to use Batch Renormalization\n",
      "        (https://arxiv.org/abs/1702.03275). This adds extra variables during\n",
      "        training. The inference is the same for either value of this parameter.\n",
      "      renorm_clipping: A dictionary that may map keys 'rmax', 'rmin', 'dmax' to\n",
      "        scalar `Tensors` used to clip the renorm correction. The correction\n",
      "        `(r, d)` is used as `corrected_value = normalized_value * r + d`, with\n",
      "        `r` clipped to [rmin, rmax], and `d` to [-dmax, dmax]. Missing rmax, rmin,\n",
      "        dmax are set to inf, 0, inf, respectively.\n",
      "      renorm_decay: Momentum used to update the moving means and standard\n",
      "        deviations with renorm. Unlike `momentum`, this affects training\n",
      "        and should be neither too small (which would add noise) nor too large\n",
      "        (which would give stale estimates). Note that `decay` is still applied\n",
      "        to get the means and variances for inference.\n",
      "      adjustment: A function taking the `Tensor` containing the (dynamic) shape of\n",
      "        the input tensor and returning a pair (scale, bias) to apply to the\n",
      "        normalized values (before gamma and beta), only during training. For\n",
      "        example,\n",
      "          `adjustment = lambda shape: (\n",
      "            tf.random_uniform(shape[-1:], 0.93, 1.07),\n",
      "            tf.random_uniform(shape[-1:], -0.1, 0.1))`\n",
      "        will scale the normalized value by up to 7% up or down, then shift the\n",
      "        result by up to 0.1 (with independent scaling and bias for each feature\n",
      "        but shared across all examples), and finally apply gamma and/or beta. If\n",
      "        `None`, no adjustment is applied.\n",
      "    \n",
      "    Returns:\n",
      "      A `Tensor` representing the output of the operation.\n",
      "    \n",
      "    Raises:\n",
      "      ValueError: If `data_format` is neither `NHWC` nor `NCHW`.\n",
      "      ValueError: If the rank of `inputs` is undefined.\n",
      "      ValueError: If rank or channels dimension of `inputs` is undefined.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(tf.contrib.layers.batch_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-20T04:46:43.914529Z",
     "start_time": "2020-11-20T04:46:42.347242Z"
    }
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "from tensorflow.python.keras import regularizers\n",
    "from tensorflow.python.ops import init_ops\n",
    "emb_to_innerProduct = {\"hour_idx\":[\"featA\",\"featB\",\"featC\",\"link\",\"pickupHash\"],\n",
    "                       \"time_idx\":[\"featA\",\"featB\",\"featC\",\"link\",\"pickupHash\"],}\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    emb_dot = []\n",
    "    for k, v in emb_to_innerProduct.items():\n",
    "        # k: f1  v: [featA,featB,featC]\n",
    "        # cross: f1*featA + f1*featB + f1*featC \n",
    "        emb1 = fc.input_layer(features, fc_emb_dict[k])\n",
    "        emb2 = fc.input_layer(features, [fc_emb_dict[i] for i in v])\n",
    "        emb1_r = tf.expand_dims(emb1, -1)\n",
    "        emb2_r = tf.reshape(emb2,[-1, len(v), emb1.shape[-1]])\n",
    "        res = tf.squeeze(tf.matmul(emb2_r, emb1_r))\n",
    "        emb_dot.append(res)\n",
    "    all_emb_dot = tf.concat(emb_dot, axis=1)\n",
    "#     fm_2nd_res = tf.reduce_sum(tf.concat(emb_dot, axis=1), axis=1)\n",
    "#     fm_2nd_res = tf.reshape(fm_2nd_res, [-1,1])\n",
    "    \n",
    "    logits = tf.layers.dense(all_emb_dot, 4, kernel_initializer=init_ops.glorot_uniform_initializer(),\n",
    "                        activation=None, kernel_regularizer=regularizers.l2(0.01),\n",
    "                        name=\"logits\")\n",
    "\n",
    "    all_emb_dot_bn = tf.contrib.layers.batch_norm(all_emb_dot,is_training=False,updates_collections=None)\n",
    "    logits_bn = tf.layers.dense(all_emb_dot_bn, 4, kernel_initializer=init_ops.glorot_uniform_initializer(),\n",
    "                        activation=None, kernel_regularizer=regularizers.l2(0.01),\n",
    "                        name=\"logits_bn\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-20T04:48:00.380741Z",
     "start_time": "2020-11-20T04:47:59.599829Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'MatMul_5:0' shape=(2, 5, 1) dtype=float32>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'Squeeze_1:0' shape=(2, 5) dtype=float32>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "array([[[-0.4614984 ],\n",
       "        [-0.13459314],\n",
       "        [ 0.8183997 ],\n",
       "        [ 0.        ],\n",
       "        [-0.1535201 ]],\n",
       "\n",
       "       [[-0.03328903],\n",
       "        [ 0.82868665],\n",
       "        [ 0.55066764],\n",
       "        [ 0.55066764],\n",
       "        [ 0.55066764]]], dtype=float32)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "array([[-0.4614984 , -0.13459314,  0.8183997 ,  0.        , -0.1535201 ],\n",
       "       [-0.03328903,  0.82868665,  0.55066764,  0.55066764,  0.55066764]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    tf.matmul(emb2_r, emb1_r)\n",
    "    res\n",
    "    sess.run(tf.matmul(emb2_r, emb1_r))\n",
    "    sess.run(tf.reshape(tf.matmul(emb2_r, emb1_r),[-1, tf.matmul(emb2_r, emb1_r).shape[1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-19T08:34:04.898297Z",
     "start_time": "2020-11-19T08:34:00.537240Z"
    },
    "run_control": {
     "marked": true
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time_idx\n",
      "['featA', 'featB', 'featC', 'link', 'pickupHash']\n",
      "emb1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 0.41871086,  0.52999526,  0.4945093 ],\n",
       "       [-0.629783  , -0.06730537, -0.3083893 ]], dtype=float32)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "emb2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 0.00486276,  0.46456635,  0.63068444, -0.629783  , -0.06730537,\n",
       "        -0.3083893 ,  0.79900736, -0.19154112, -0.7691954 ,  0.        ,\n",
       "         0.        ,  0.        ,  0.26811442, -0.47049198, -0.89255047],\n",
       "       [ 0.26811442, -0.47049198, -0.89255047, -0.629783  , -0.06730537,\n",
       "        -0.3083893 , -0.27455962,  0.29733664, -0.7572018 , -0.27455962,\n",
       "         0.29733664, -0.7572018 , -0.27455962,  0.29733664, -0.7572018 ]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "emb1_r\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[[ 0.41871086],\n",
       "        [ 0.52999526],\n",
       "        [ 0.4945093 ]],\n",
       "\n",
       "       [[-0.629783  ],\n",
       "        [-0.06730537],\n",
       "        [-0.3083893 ]]], dtype=float32)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "emb2_r\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[[ 0.00486276,  0.46456635,  0.63068444],\n",
       "        [-0.629783  , -0.06730537, -0.3083893 ],\n",
       "        [ 0.79900736, -0.19154112, -0.7691954 ],\n",
       "        [ 0.        ,  0.        ,  0.        ],\n",
       "        [ 0.26811442, -0.47049198, -0.89255047]],\n",
       "\n",
       "       [[ 0.26811442, -0.47049198, -0.89255047],\n",
       "        [-0.629783  , -0.06730537, -0.3083893 ],\n",
       "        [-0.27455962,  0.29733664, -0.7572018 ],\n",
       "        [-0.27455962,  0.29733664, -0.7572018 ],\n",
       "        [-0.27455962,  0.29733664, -0.7572018 ]]], dtype=float32)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "res\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 0.5601334 , -0.45186988, -0.14733711,  0.        , -0.57847065],\n",
       "       [ 0.13806576,  0.49626055,  0.38641354,  0.38641354,  0.38641354]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> 只看batch里的第一个\n",
      "emb1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.41871086, 0.52999526, 0.4945093 ], dtype=float32)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "emb2_r\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 0.00486276,  0.46456635,  0.63068444],\n",
       "       [-0.629783  , -0.06730537, -0.3083893 ],\n",
       "       [ 0.79900736, -0.19154112, -0.7691954 ],\n",
       "       [ 0.        ,  0.        ,  0.        ],\n",
       "       [ 0.26811442, -0.47049198, -0.89255047]], dtype=float32)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "res\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 0.5601334 , -0.45186988, -0.14733711,  0.        , -0.57847065],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "emb_dot\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[array([[ 0.12446526, -0.48983523,  0.30339354,  0.        , -0.03028464],\n",
       "        [ 0.13806576,  0.49626055,  0.38641354,  0.38641354,  0.38641354]],\n",
       "       dtype=float32),\n",
       " array([[ 0.5601334 , -0.45186988, -0.14733711,  0.        , -0.57847065],\n",
       "        [ 0.13806576,  0.49626055,  0.38641354,  0.38641354,  0.38641354]],\n",
       "       dtype=float32)]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all_emb_dot | concat emb_dot\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 0.12446526, -0.48983523,  0.30339354,  0.        , -0.03028464,\n",
       "         0.5601334 , -0.45186988, -0.14733711,  0.        , -0.57847065],\n",
       "       [ 0.13806576,  0.49626055,  0.38641354,  0.38641354,  0.38641354,\n",
       "         0.13806576,  0.49626055,  0.38641354,  0.38641354,  0.38641354]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logits | tf.concat(all_emb_dot, axis=1) & 4分类\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[-0.03423132,  0.16598332,  0.17956713, -0.2545311 ],\n",
       "       [ 0.16397662, -0.02424657, -1.1411203 ,  0.46004915]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all_emb_dot_bn | concat emb_dot\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 0.12440307, -0.4895905 ,  0.30324197,  0.        , -0.03026951,\n",
       "         0.55985355, -0.45164412, -0.1472635 ,  0.        , -0.5781816 ],\n",
       "       [ 0.13799678,  0.49601263,  0.38622048,  0.38622048,  0.38622048,\n",
       "         0.13799678,  0.49601263,  0.38622048,  0.38622048,  0.38622048]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logits_bn | tf.concat(all_emb_dot_bn, axis=1) & 4分类\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[-0.04481216,  0.5127205 , -0.04209074, -0.35768437],\n",
       "       [-0.33998445, -0.2264207 , -0.0203252 , -0.3708216 ]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(k)\n",
    "print(v)\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    print(\"emb1\")\n",
    "    sess.run(emb1)\n",
    "    print(\"emb2\")\n",
    "    sess.run(emb2)\n",
    "    print(\"emb1_r\")\n",
    "    sess.run(emb1_r)\n",
    "    print(\"emb2_r\")\n",
    "    sess.run(emb2_r)\n",
    "    print(\"res\")\n",
    "    sess.run(res)\n",
    "    \n",
    "    print(\">>> 只看batch里的第一个\")\n",
    "    print(\"emb1\")\n",
    "    sess.run(emb1)[0]\n",
    "#     print(\"emb2\")\n",
    "#     sess.run(emb2)[0]\n",
    "#     print(\"emb1_r\")\n",
    "#     sess.run(emb1_r)[0]\n",
    "    print(\"emb2_r\")\n",
    "    sess.run(emb2_r)[0]\n",
    "    print(\"res\")\n",
    "    sess.run(res)[0]\n",
    "    \n",
    "    print(\"emb_dot\")\n",
    "    sess.run(emb_dot)\n",
    "    print(\"all_emb_dot | concat emb_dot\")\n",
    "    sess.run(all_emb_dot)\n",
    "#     print(\"tf.concat(emb_dot, axis=0)\")\n",
    "#     sess.run(tf.concat(emb_dot, axis=1))\n",
    "#     print(\"fm_2nd_res | tf.reduce_sum(tf.concat(emb_dot, axis=1), axis=1)\")\n",
    "#     sess.run(fm_2nd_res)\n",
    "    print(\"logits | tf.concat(all_emb_dot, axis=1) & 4分类\")\n",
    "    sess.run(logits)\n",
    "\n",
    "    print(\"all_emb_dot_bn | concat emb_dot\")\n",
    "    sess.run(all_emb_dot_bn)\n",
    "    print(\"logits_bn | tf.concat(all_emb_dot_bn, axis=1) & 4分类\")\n",
    "    sess.run(logits_bn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-19T08:30:45.214216Z",
     "start_time": "2020-11-19T08:30:45.203254Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.2338353800000001"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "3.66484891"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum([ 0.12115489,  0.39662713, -0.45657066,  0.        , -0.11924169,\n",
    "        -0.06230509, -0.43095323,  0.20267147,  0.        ,  0.1147818 ])\n",
    "sum([ 0.98669964, -0.9935233 , -0.96973526,  0.9557099 ,  0.80604684,\n",
    "         0.95365196, -0.9989523 ,  0.9882004 ,  0.9557099 ,  0.98104113])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-19T07:48:16.846625Z",
     "start_time": "2020-11-19T07:48:16.827614Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.5709219853572799, -0.013578032623862401, -0.093756372889564]"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "0.4635875798438535"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# copy 出来手工计算\n",
    "emb1 = [ 0.91038424, -0.03854756, -0.2341526 ]\n",
    "emb2 = [ 0.627122  ,  0.35224104,  0.40040714]\n",
    "\n",
    "[a*b for a,b in zip(emb1,emb2)]\n",
    "sum([a*b for a,b in zip(emb1,emb2)])\n",
    "\n",
    "# sum([ 0.3972234 , -0.308128  ,  0.        , -0.07037204, -0.308128  ,\n",
    "#          1.4594309 ,  0.        , -0.29277474])\n",
    "# sum([ 0.5564581 ,  0.41053557,  0.        ,  0.5564581 ,  0.4649981 , 0.17164305,  0.        ,  0.4649981 ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \"net\",sess.run(net)\n",
    "    \"emb_added\",sess.run(emb_added)\n",
    "    \"net_final\",sess.run(net_final)\n",
    "    print(\">>> details of emb_added\")\n",
    "    for f1,f2 in emb_to_add:\n",
    "        net1 = fc.input_layer(features, fc_emb_dict[f1])\n",
    "        net2 = fc.input_layer(features, fc_emb_dict[f2])\n",
    "        f1,sess.run(net1)\n",
    "        f2,sess.run(net2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def fm(inputs):\n",
    "#     if K.ndim(inputs) != 3:\n",
    "#         raise ValueError(\n",
    "#             \"Unexpected inputs dimensions %d, expect to be 3 dimensions\"\n",
    "#             % (K.ndim(inputs)))\n",
    "\n",
    "#     concated_embeds_value = inputs\n",
    "\n",
    "#     square_of_sum = tf.square(reduce_sum(\n",
    "#         concated_embeds_value, axis=1, keep_dims=True))\n",
    "#     sum_of_square = reduce_sum(\n",
    "#         concated_embeds_value * concated_embeds_value, axis=1, keep_dims=True)\n",
    "#     cross_term = square_of_sum - sum_of_square\n",
    "#     cross_term = 0.5 * reduce_sum(cross_term, axis=2, keep_dims=False)\n",
    "\n",
    "#     return cross_term\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interact emb by Co-Action Unit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### basic | imitation hparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-02T11:06:25.665748Z",
     "start_time": "2020-12-02T11:06:25.650280Z"
    },
    "run_control": {
     "marked": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'batch_size': 2,\n",
       " 'emb_mlp_feature_size': 100,\n",
       " 'emb_mlp_units': [8, 8, 8],\n",
       " 'embedding_feature_size': 20,\n",
       " 'embedding_size': 4,\n",
       " 'n_classes': [4, 4]}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hparams = {}\n",
    "hparams['batch_size'] = 2\n",
    "hparams['n_classes'] = [4,4]  # idx=1是跨路label\n",
    "hparams['embedding_size'] = 4\n",
    "hparams[\"embedding_feature_size\"]=20\n",
    "hparams['emb_mlp_units'] = [8,8,8]\n",
    "hparams['emb_mlp_feature_size'] = 100\n",
    "\n",
    "hparams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### basic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-02T11:06:25.791993Z",
     "start_time": "2020-12-02T11:06:25.733144Z"
    },
    "run_control": {
     "marked": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'featA': array([[1],\n",
      "       [6]]), 'pickupHash': array([[18],\n",
      "       [ 5]]), 'time_idx': array([[16],\n",
      "       [17]]), 'numericB': [[0.5], [0.2]], 'numericA': [[0.5], [1.2]], 'link': [[-1], [-1]], 'hour_idx': array([[15],\n",
      "       [ 3]]), 'whateverB': array([[14],\n",
      "       [14]]), 'whateverA': array([[5],\n",
      "       [6]]), 'featB': array([[ 5],\n",
      "       [17]])}\n"
     ]
    }
   ],
   "source": [
    "##############\n",
    "# h-params\n",
    "##############\n",
    "num_buckets_deep=hparams[\"embedding_feature_size\"]\n",
    "dims_embs_deep=hparams['embedding_size']\n",
    "_batch_size=hparams['batch_size']\n",
    "\n",
    "##############\n",
    "# features\n",
    "##############\n",
    "features = {}\n",
    "features[\"featA\"] = np.random.randint(num_buckets_deep, size=(_batch_size, 1))\n",
    "features[\"featB\"] = np.random.randint(num_buckets_deep, size=(_batch_size, 1))\n",
    "features[\"link\"] = [[-1],[-1]]\n",
    "features[\"pickupHash\"] = np.random.randint(num_buckets_deep, size=(_batch_size, 1))\n",
    "features[\"hour_idx\"] = np.random.randint(num_buckets_deep, size=(_batch_size, 1))\n",
    "features[\"time_idx\"] = np.random.randint(num_buckets_deep, size=(_batch_size, 1))\n",
    "features[\"whateverA\"] = np.random.randint(num_buckets_deep, size=(_batch_size, 1))\n",
    "features[\"whateverB\"] = np.random.randint(num_buckets_deep, size=(_batch_size, 1))\n",
    "features[\"numericA\"] = [[0.5],[1.2]]\n",
    "features[\"numericB\"] = [[0.5],[0.2]]\n",
    "print(features)\n",
    "\n",
    "################\n",
    "# build columns\n",
    "################\n",
    "tf.reset_default_graph()\n",
    "# numeric column\n",
    "fc_numeric = list()\n",
    "fc_numeric.append(fc.numeric_column(\"numericA\", default_value=0))\n",
    "fc_numeric.append(fc.numeric_column(\"numericB\", default_value=0))\n",
    "\n",
    "# codec column\n",
    "emb_feat_list = list()\n",
    "for i in [\"featA\",\"featB\",\"link\",\"pickupHash\",\"hour_idx\",\"time_idx\"]:\n",
    "    emb_feat_list.append(fc.categorical_column_with_identity(i, num_buckets_deep, default_value=0))\n",
    "\n",
    "# emb column\n",
    "fc_shared_emb = fc.shared_embedding_columns(\n",
    "                    emb_feat_list,\n",
    "                    dimension=dims_embs_deep, max_norm=np.sqrt(dims_embs_deep),\n",
    "                    trainable=True)\n",
    "\n",
    "# shared_emb 转成KV结构方便后面操作\n",
    "fc_emb_dict = {seCol.categorical_column.key: seCol for seCol in fc_shared_emb}\n",
    "\n",
    "# 指出将要操作的emb\n",
    "emb_to_add=[['link','hour_idx'],\n",
    "            ['pickupHash','time_idx']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-02T11:06:27.391034Z",
     "start_time": "2020-12-02T11:06:27.356136Z"
    },
    "code_folding": [
     0,
     33
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[_EmbeddingColumn(categorical_column=_HashedCategoricalColumn(key='pickupHash', hash_bucket_size=100, dtype=tf.int64), dimension=32, combiner='mean', layer_creator=<function _creator at 0x7f94cdf64b18>, ckpt_to_load_from=None, tensor_name_in_ckpt=None, max_norm=5.656854249492381, trainable=True),\n",
       " _EmbeddingColumn(categorical_column=_HashedCategoricalColumn(key='pickupHash', hash_bucket_size=100, dtype=tf.int64), dimension=64, combiner='mean', layer_creator=<function _creator at 0x7f94cdf64c08>, ckpt_to_load_from=None, tensor_name_in_ckpt=None, max_norm=8.0, trainable=True),\n",
       " _EmbeddingColumn(categorical_column=_HashedCategoricalColumn(key='pickupHash', hash_bucket_size=100, dtype=tf.int64), dimension=64, combiner='mean', layer_creator=<function _creator at 0x7f94cdf64938>, ckpt_to_load_from=None, tensor_name_in_ckpt=None, max_norm=8.0, trainable=True)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "[_EmbeddingColumn(categorical_column=_HashedCategoricalColumn(key='pickupHash', hash_bucket_size=100, dtype=tf.int64), dimension=8, combiner='mean', layer_creator=<function _creator at 0x7f94cdf64ed8>, ckpt_to_load_from=None, tensor_name_in_ckpt=None, max_norm=2.8284271247461903, trainable=True),\n",
       " _EmbeddingColumn(categorical_column=_HashedCategoricalColumn(key='pickupHash', hash_bucket_size=100, dtype=tf.int64), dimension=8, combiner='mean', layer_creator=<function _creator at 0x7f94cdf64e60>, ckpt_to_load_from=None, tensor_name_in_ckpt=None, max_norm=2.8284271247461903, trainable=True),\n",
       " _EmbeddingColumn(categorical_column=_HashedCategoricalColumn(key='pickupHash', hash_bucket_size=100, dtype=tf.int64), dimension=8, combiner='mean', layer_creator=<function _creator at 0x7f94cdf64d70>, ckpt_to_load_from=None, tensor_name_in_ckpt=None, max_norm=2.8284271247461903, trainable=True)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def co_action_unit_fc(hparams):\n",
    "    # fc_target_item = fc.categorical_column_with_identity(hparams[\"emb_mlp_feat\"], hparams['mlp_embedding_feature_size'], default_value=0)\n",
    "    fc_target_item = fc.categorical_column_with_hash_bucket(\"pickupHash\", hash_bucket_size=hparams['emb_mlp_feature_size'], dtype=tf.int64)\n",
    "\n",
    "    fc_emb_mlp_kernel_list = []\n",
    "    fc_emb_mlp_bias_list = []\n",
    "    inp_size = hparams['embedding_size']\n",
    "    for idx, i in enumerate(hparams['emb_mlp_units']):\n",
    "        kernel_size = inp_size*i\n",
    "        bias_size = i\n",
    "        _fc_emb_kernel = fc.embedding_column(fc_target_item,\n",
    "                                             dimension=kernel_size, max_norm=np.sqrt(kernel_size),\n",
    "                                             trainable=True)\n",
    "        fc_emb_mlp_kernel_list.append(_fc_emb_kernel)\n",
    "        _fc_emb_bias = fc.embedding_column(fc_target_item,\n",
    "                                           dimension=bias_size, max_norm=np.sqrt(bias_size),\n",
    "                                           trainable=True)\n",
    "        fc_emb_mlp_bias_list.append(_fc_emb_bias)\n",
    "\n",
    "        # # >>>deubg\n",
    "        # kernel = np.random.random([inp_size,i])\n",
    "        # bias = np.random.random(bias_size)\n",
    "        # print(\"kernel shape:%s size:%s\" % (kernel.shape, kernel_size))\n",
    "        # print(\"bias shape:%s size:%s\" % (bias.shape, bias_size))\n",
    "        # # <<<deubg\n",
    "\n",
    "        inp_size = i  # IMPORTANT\n",
    "\n",
    "    # [fc_emb.dimension for fc_emb in fc_emb_mlp_kernel_list]\n",
    "    # [fc_emb.dimension for fc_emb in fc_emb_mlp_bias_list]\n",
    "    return fc_emb_mlp_kernel_list, fc_emb_mlp_bias_list\n",
    "fc_emb_mlp_kernel_list, fc_emb_mlp_bias_list = co_action_unit_fc(hparams)\n",
    "fc_emb_mlp_kernel_list\n",
    "fc_emb_mlp_bias_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-02T11:07:36.930994Z",
     "start_time": "2020-12-02T11:07:36.519843Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'Reshape_2:0' shape=(2, 4, 8) dtype=float32>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'Reshape_3:0' shape=(2, 8, 8) dtype=float32>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'Reshape_4:0' shape=(2, 8, 8) dtype=float32>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'ExpandDims_5:0' shape=(2, 1, 8) dtype=float32>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fc_emb_inp_list = [fc_emb_dict[i] for i in ['featA','featB']]\n",
    "\n",
    "emb_inp = fc.input_layer(features, fc_emb_inp)  # (batch_size, D)\n",
    "emb_inp = tf.math.pow(emb_inp, 1) + tf.math.pow(emb_inp, 2) + tf.math.pow(emb_inp, 3)\n",
    "emb_inp = tf.expand_dims(emb_inp, 1)\n",
    "\n",
    "net = emb_inp\n",
    "for idx, fc_emb in enumerate(fc_emb_mlp_kernel_list):\n",
    "    emb_mlp_kernel = fc.input_layer(features, fc_emb)\n",
    "    cur_output = hparams['emb_mlp_units'][idx]\n",
    "    emb_mlp_kernel = tf.reshape(emb_mlp_kernel, [-1, fc_emb.dimension/cur_output, cur_output])\n",
    "    emb_mlp_kernel\n",
    "#     emb_mlp_bias = fc.input_layer(features, fc_emb_mlp_bias_list[idx])\n",
    "#     emb_mlp_bias = tf.expand_dims(emb_mlp_bias, 1)\n",
    "#     # wx+b\n",
    "#     res = tf.einsum(\"bik,bkj->bij\", net, emb_mlp_kernel) + emb_mlp_bias\n",
    "#     # activation\n",
    "#     res = tf.nn.relu(res)\n",
    "#     net = res\n",
    "# net = tf.squeeze(net)\n",
    "net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-02T11:09:21.319787Z",
     "start_time": "2020-12-02T11:09:19.985205Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Squeeze:0\", shape=(2, 8), dtype=float32)\n",
      "Tensor(\"Squeeze_1:0\", shape=(2, 8), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "def co_action_unit(features, fc_emb_inp, fc_emb_mlp_kernel_list, fc_emb_mlp_bias_list, hparams, mode):\n",
    "    \"\"\"\n",
    "\n",
    "    :param features:\n",
    "    :param fc_emb_inp: emb_feature_column\n",
    "    :param fc_emb_mlp_kernel_list: list[emb_feature_column]\n",
    "    :param fc_emb_mlp_bias_list: list[emb_feature_column]\n",
    "    :param hparams: params\n",
    "    :param mode: TRAIN EVALUATION .etc\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    emb_inp = fc.input_layer(features, fc_emb_inp)  # (batch_size, D)\n",
    "    emb_inp = tf.math.pow(emb_inp, 1) + tf.math.pow(emb_inp, 2) + tf.math.pow(emb_inp, 3)\n",
    "    emb_inp = tf.expand_dims(emb_inp, 1)\n",
    "\n",
    "    net = emb_inp\n",
    "    for idx, fc_emb in enumerate(fc_emb_mlp_kernel_list):\n",
    "        emb_mlp_kernel = fc.input_layer(features, fc_emb)\n",
    "        cur_output = hparams['emb_mlp_units'][idx]\n",
    "        emb_mlp_kernel = tf.reshape(emb_mlp_kernel, [-1, fc_emb.dimension/cur_output, cur_output])\n",
    "        emb_mlp_bias = fc.input_layer(features, fc_emb_mlp_bias_list[idx])\n",
    "        emb_mlp_bias = tf.expand_dims(emb_mlp_bias, 1)\n",
    "        # wx+b\n",
    "        res = tf.einsum(\"bik,bkj->bij\", net, emb_mlp_kernel) + emb_mlp_bias\n",
    "        # activation\n",
    "        res = tf.nn.relu(res)\n",
    "        net = res\n",
    "    net = tf.squeeze(net)\n",
    "    return net\n",
    "\n",
    "fc_emb_inp_list = [fc_emb_dict[i] for i in ['featA','featB']]\n",
    "emb_can_list = []\n",
    "\n",
    "for fc_emb_inp in fc_emb_inp_list:\n",
    "        emb_can = co_action_unit(features, fc_emb_inp, fc_emb_mlp_kernel_list, fc_emb_mlp_bias_list, hparams, 'TRAIN')\n",
    "        print(emb_can)\n",
    "        emb_can_list.append(emb_can)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-02T11:10:16.056334Z",
     "start_time": "2020-12-02T11:10:15.543578Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'input_layer_28/concat:0' shape=(2, 24) dtype=float32>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor 'Squeeze:0' shape=(2, 8) dtype=float32>,\n",
       " <tf.Tensor 'Squeeze_1:0' shape=(2, 8) dtype=float32>]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'concat:0' shape=(2, 40) dtype=float32>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = fc.input_layer(features, fc_emb_dict.values())\n",
    "net\n",
    "emb_can_list\n",
    "tf.concat([net] + emb_can_list ,axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### basic | emb_mlp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "kernel一张表，bias一张表"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-02T06:34:58.124973Z",
     "start_time": "2020-12-02T06:34:58.069293Z"
    },
    "code_folding": [
     0
    ],
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true,
     "marked": true
    }
   },
   "outputs": [],
   "source": [
    "# emb_mlp size的计算，输入是普通的emb_size输出是n_class，中间是N个全连接\n",
    "hparams['emb_mlp_units'] = [8]*3  # 先用桶状结构吧，有利于后面reshape然后用矩阵乘法计算更高效（不然要用slice）\n",
    "hparams['emb_mlp_bias'] = [8]*3\n",
    "\n",
    "emb_mlp_kernel_size_1st = 0\n",
    "emb_mlp_kernel_size = 0\n",
    "emb_mlp_bias_size = 0\n",
    "inp_size = hparams['embedding_size']\n",
    "for idx,i in enumerate(hparams['emb_mlp_units']):\n",
    "    kernel_size = inp_size*i\n",
    "    bias_size = i\n",
    "    kernel = np.random.random([inp_size,i])\n",
    "    bias = np.random.random(bias_size)\n",
    "    print(\"shape:%s size:%s\" % (kernel.shape, kernel_size))\n",
    "    print(\"shape:%s size:%s\" % (bias.shape, bias_size))\n",
    "    if idx == 0:\n",
    "        emb_mlp_kernel_size_1st = kernel_size\n",
    "    \n",
    "    total_emb_mlp_kernel_size += kernel_size\n",
    "    total_emb_mlp_bias_size += bias_size\n",
    "    inp_size = i\n",
    "\n",
    "hparams['emb_mlp_kernel_size'] = total_emb_mlp_kernel_size\n",
    "hparams['emb_mlp_bias_size'] = total_emb_mlp_bias_size\n",
    "total_emb_mlp_kernel_size\n",
    "total_emb_mlp_bias_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "每个kernel各自一张表，每个bias各自一张表"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-02T07:05:43.154016Z",
     "start_time": "2020-12-02T07:05:43.146280Z"
    }
   },
   "outputs": [],
   "source": [
    "# emb_mlp size的计算，输入是普通的emb_size输出是n_class，中间是N个全连接\n",
    "hparams['emb_mlp_units'] = [8,16,8]\n",
    "# 应该是独立去重计算，但是这里不像重新建索引，就还是偷个懒用之前的size\n",
    "hparams[\"mlp_embedding_feature_size\"] = hparams[\"embedding_feature_size\"]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-02T07:05:45.178269Z",
     "start_time": "2020-12-02T07:05:43.851577Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kernel shape:(4, 8) size:32\n",
      "bias shape:(8,) size:8\n",
      "kernel shape:(8, 16) size:128\n",
      "bias shape:(16,) size:16\n",
      "kernel shape:(16, 8) size:128\n",
      "bias shape:(8,) size:8\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[32, 128, 128]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "[8, 16, 8]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'ExpandDims:0' shape=(2, 1, 4) dtype=float32>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> at idx: 0 \n",
      ">>> at idx: 1 \n",
      ">>> at idx: 2 \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'Relu_2:0' shape=(2, 1, 8) dtype=float32>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "targetItemCol = fc.categorical_column_with_hash_bucket(\"pickupHash\", hash_bucket_size=hparams['embedding_feature_size'], dtype=tf.int64)\n",
    "fc_emb_mlp_kernel = []\n",
    "fc_emb_mlp_bias = []\n",
    "inp_size = hparams['embedding_size']\n",
    "for idx,i in enumerate(hparams['emb_mlp_units']):\n",
    "    kernel_size = inp_size*i\n",
    "    bias_size = i\n",
    "    _fc_emb_kernel = fc.embedding_column(targetItemCol,\n",
    "                                         dimension=kernel_size, max_norm=np.sqrt(kernel_size),\n",
    "                                         trainable=True)\n",
    "    fc_emb_mlp_kernel.append(_fc_emb_kernel)\n",
    "    _fc_emb_bias = fc.embedding_column(targetItemCol,\n",
    "                                       dimension=bias_size, max_norm=np.sqrt(bias_size),\n",
    "                                       trainable=True)\n",
    "    fc_emb_mlp_bias.append(_fc_emb_bias)\n",
    "    \n",
    "    # >>>deubg\n",
    "    kernel = np.random.random([inp_size,i])\n",
    "    bias = np.random.random(bias_size)\n",
    "    print(\"kernel shape:%s size:%s\" % (kernel.shape, kernel_size))\n",
    "    print(\"bias shape:%s size:%s\" % (bias.shape, bias_size))\n",
    "    # <<<deubg\n",
    "    \n",
    "    inp_size = i  # IMPORTANT\n",
    "\n",
    "[fc_emb.dimension for fc_emb in fc_emb_mlp_kernel]\n",
    "[fc_emb.dimension for fc_emb in fc_emb_mlp_bias]\n",
    "\n",
    "\n",
    "# 输入侧的emb column\n",
    "# fc_emb_inp = [fc_emb_dict[i] for i in ['link','hour_idx']]\n",
    "fc_emb_inp = fc_emb_dict['hour_idx']  # 一个个来吧，考虑到还要做1~3次幂\n",
    "emb_inp = fc.input_layer(features, fc_emb_inp) # (batch_size, D)\n",
    "emb_inp = tf.math.pow(emb_inp,1) + tf.math.pow(emb_inp,2) + tf.math.pow(emb_inp,3)\n",
    "emb_inp = tf.expand_dims(emb_inp,1)\n",
    "emb_inp\n",
    "\n",
    "inp = emb_inp\n",
    "for idx,fc_emb in enumerate(fc_emb_mlp_kernel):\n",
    "    print(\">>> at idx: %s \" % idx)\n",
    "    emb_mlp_kernel = fc.input_layer(features, fc_emb)\n",
    "    cur_output = hparams['emb_mlp_units'][idx]\n",
    "    emb_mlp_kernel = tf.reshape(emb_mlp_kernel,[-1, fc_emb.dimension/cur_output, cur_output])\n",
    "    emb_mlp_bias = fc.input_layer(features, fc_emb_mlp_bias[idx])\n",
    "    emb_mlp_bias = tf.expand_dims(emb_mlp_bias,1)\n",
    "    res = tf.einsum(\"bik,bkj->bij\", inp, emb_mlp_kernel) + emb_mlp_bias\n",
    "    res = tf.nn.relu(res)\n",
    "    inp = res\n",
    "\n",
    "res\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Estimator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 封装一个工具 绕过计算图检查vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-03T09:22:35.548613Z",
     "start_time": "2020-12-03T09:22:31.318612Z"
    },
    "code_folding": [
     4
    ],
    "run_control": {
     "marked": true
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.contrib.framework.python.framework import checkpoint_utils\n",
    "\n",
    "\n",
    "class CheckPointInspector:\n",
    "    def __init__(self, ckpt_dir, check_list=None):\n",
    "        self.ckpt_dir = ckpt_dir\n",
    "        self.name2shape_dict = CheckPointInspector.load_name_shape(self.ckpt_dir)\n",
    "        self.vars = sorted(self.name2shape_dict.keys())\n",
    "        self.check_list = check_list if check_list is not None else []\n",
    "        self.name2value_dict = {}\n",
    "        # workaround: _saver raise ValueError: No variables to save from\n",
    "        # self.__tmpVar = tf.Variable([0],name=\"tmpVarOfInspectUtils\")\n",
    "\n",
    "    def append(self, name):\n",
    "        if name in self.check_list:\n",
    "            print(\"%s exist\" % name)\n",
    "        else:\n",
    "            self.check_list.append(name)\n",
    "\n",
    "    def brief(self):\n",
    "        for i in self.vars:\n",
    "            print(i, self.name2shape_dict[i])\n",
    "\n",
    "    def restore(self):\n",
    "        check_list = self.check_list if len(self.check_list) > 0 else self.vars\n",
    "        print(\"restore with: %s\" % \",\".join(check_list))\n",
    "        # init variable\n",
    "        for name in check_list:\n",
    "            self.name2value_dict[name] = tf.Variable(tf.zeros(self.name2shape_dict[name]), name=name, dtype=tf.float32)\n",
    "        _saver = tf.train.Saver(self.name2value_dict.values(), max_to_keep=1)\n",
    "        with tf.Session() as sess:\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "            # restore\n",
    "            _saver.restore(sess, tf.train.latest_checkpoint(self.ckpt_dir))\n",
    "            # eval variable\n",
    "            for name in check_list:\n",
    "                self.name2value_dict[name] = self.name2value_dict[name].eval()\n",
    "\n",
    "    def show(self, name):\n",
    "        return self.name2value_dict[name]\n",
    "\n",
    "    @staticmethod\n",
    "    def load_name_shape(ckpt_dir):\n",
    "        return dict(checkpoint_utils.list_variables(ckpt_dir))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-04T04:56:02.110326Z",
     "start_time": "2020-12-04T04:56:02.100024Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(64, 71)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "a=\"dnn/moe/expert_2/repr_2/kernel\"\n",
    "b=\"dnn/emb_mlp_kernel_1/input_layer/pickupHash_embedding/embedding_weights\"\n",
    "re.search(\"emb\",a)\n",
    "re.search(\"emb\",b).groups()\n",
    "re.search(\"weights$\",b).span()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-04T03:31:24.433134Z",
     "start_time": "2020-12-04T03:31:24.421398Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"-1\"\n",
    "\n",
    "ckpt_dir=\"/tmp-data/zhoutongzt/DD/Data/BR_useReqTime_MMOE_CAN\"\n",
    "print(\">>> checking ckpt: %s\" % ckpt_dir)\n",
    "ins = CheckPointInspector(ckpt_dir)\n",
    "\n",
    "print(\">>> all var:\")\n",
    "for i in ins.vars:\n",
    "    print(i)\n",
    "\n",
    "# append var to check-value\n",
    "for i in ins.vars:\n",
    "    # if \"embedding_weights/Adagrad\" in i:\n",
    "    #     ins.append(i)\n",
    "    if any(j in i for j in [\"kernel\", \"kernel/Adagrad\", \"bias/Adagrad\"]):\n",
    "        if \"embedding\" not in i:\n",
    "            ins.append(i)\n",
    "ins.restore()\n",
    "\n",
    "print(\">>> inspecting values:\")\n",
    "for name in ins.check_list:\n",
    "    var = ins.show(name)\n",
    "    print(\"\\n===%s %s\" % (name, var.shape))\n",
    "    print(\"min:%s max:%s avg:%s\" % (np.min(var), np.max(var), np.mean(var)))\n",
    "    # leads to err in prefix of 155.1 55.1 5.1\n",
    "    # with np.printoptions(suppress=True, formatter={'float': '{:0.4f}'.format}):\n",
    "    with np.printoptions(suppress=True, precision=4, threshold=50):\n",
    "        print(var)\n",
    "        if len(var.shape) > 1:\n",
    "            max_idx = np.argmax(np.mean(var, axis=1))\n",
    "            min_idx = np.argmin(np.mean(var, axis=1))\n",
    "            print(\"max by mean_axis=1:%s\" % var[max_idx])\n",
    "            print(\"min by mean_axis=1:%s\" % var[min_idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## check model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-24T11:57:02.377916Z",
     "start_time": "2020-11-24T11:57:02.368744Z"
    },
    "run_control": {
     "marked": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/tmp-data/zhoutongzt/DD/Data/BR_addHeat_PE_timIdxByFreqIdx_WDModel_drop_feat'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ckpt_dir = os.path.join(DATA_PATH,\"BR_addHeat_PE_timIdxByFreqIdx_WDModel_drop_feat\")\n",
    "\n",
    "\n",
    "train_spec = dnn_linear_combined_model_fn(\n",
    "                    features=tr_data[0],\n",
    "                    labels=tr_data[1],\n",
    "                    fc_wide=fc_wide,\n",
    "                    fc_numeric=fc_numeric,\n",
    "                    n_classes=hparams['n_classes'],\n",
    "                    hparams=hparams,\n",
    "                    mode=tf.estimator.ModeKeys.TRAIN,\n",
    "                    fc_emb_dict=fc_emb_dict,\n",
    "                    emb_to_add=emb_to_add, emb_to_innerProduct=emb_to_innerProduct)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### restore ckpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-03T02:43:37.143426Z",
     "start_time": "2020-12-03T02:43:37.125924Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.Variable 'input_layer/featA_featB_hour_idx_link_pickupHash_time_idx_shared_embedding/embedding_weights:0' shape=(20, 4) dtype=float32_ref>\n",
      "<tf.Variable 'input_layer_1/pickupHash_embedding/embedding_weights:0' shape=(100, 32) dtype=float32_ref>\n",
      "<tf.Variable 'input_layer_2/pickupHash_embedding/embedding_weights:0' shape=(100, 8) dtype=float32_ref>\n",
      "<tf.Variable 'input_layer_4/pickupHash_embedding/embedding_weights:0' shape=(100, 32) dtype=float32_ref>\n",
      "<tf.Variable 'input_layer_5/pickupHash_embedding/embedding_weights:0' shape=(100, 8) dtype=float32_ref>\n",
      "<tf.Variable 'input_layer_8/pickupHash_embedding/embedding_weights:0' shape=(100, 32) dtype=float32_ref>\n",
      "<tf.Variable 'input_layer_9/pickupHash_embedding/embedding_weights:0' shape=(100, 64) dtype=float32_ref>\n",
      "<tf.Variable 'input_layer_10/pickupHash_embedding/embedding_weights:0' shape=(100, 64) dtype=float32_ref>\n",
      "<tf.Variable 'input_layer_12/pickupHash_embedding/embedding_weights:0' shape=(100, 32) dtype=float32_ref>\n",
      "<tf.Variable 'input_layer_13/pickupHash_embedding/embedding_weights:0' shape=(100, 8) dtype=float32_ref>\n",
      "<tf.Variable 'input_layer_14/pickupHash_embedding/embedding_weights:0' shape=(100, 64) dtype=float32_ref>\n",
      "<tf.Variable 'input_layer_15/pickupHash_embedding/embedding_weights:0' shape=(100, 8) dtype=float32_ref>\n",
      "<tf.Variable 'input_layer_16/pickupHash_embedding/embedding_weights:0' shape=(100, 64) dtype=float32_ref>\n",
      "<tf.Variable 'input_layer_17/pickupHash_embedding/embedding_weights:0' shape=(100, 8) dtype=float32_ref>\n",
      "<tf.Variable 'input_layer_19/pickupHash_embedding/embedding_weights:0' shape=(100, 32) dtype=float32_ref>\n",
      "<tf.Variable 'input_layer_20/pickupHash_embedding/embedding_weights:0' shape=(100, 8) dtype=float32_ref>\n",
      "<tf.Variable 'input_layer_21/pickupHash_embedding/embedding_weights:0' shape=(100, 64) dtype=float32_ref>\n",
      "<tf.Variable 'input_layer_22/pickupHash_embedding/embedding_weights:0' shape=(100, 8) dtype=float32_ref>\n",
      "<tf.Variable 'input_layer_23/pickupHash_embedding/embedding_weights:0' shape=(100, 64) dtype=float32_ref>\n",
      "<tf.Variable 'input_layer_24/pickupHash_embedding/embedding_weights:0' shape=(100, 8) dtype=float32_ref>\n"
     ]
    }
   ],
   "source": [
    "# after restore ckpt\n",
    "for variable_name in tf.global_variables():\n",
    "        print(variable_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 直接检查ckpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-03T08:27:22.624047Z",
     "start_time": "2020-12-03T08:27:05.089652Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.94125503, 0.8952562 , 0.27667555, 0.8631715 ],\n",
       "       [0.58970124, 0.34520158, 0.6283586 , 0.7666799 ],\n",
       "       [0.52678007, 0.38669515, 0.9040632 , 0.8088709 ],\n",
       "       [0.00688401, 0.6805194 , 0.725349  , 0.80912757],\n",
       "       [0.32290053, 0.00987846, 0.66399413, 0.27213082],\n",
       "       [0.77246773, 0.24089827, 0.89595705, 0.9883507 ],\n",
       "       [0.22828436, 0.37639886, 0.9784473 , 0.9773074 ],\n",
       "       [0.49756825, 0.8321498 , 0.7441464 , 0.4640103 ],\n",
       "       [0.89436436, 0.7959515 , 0.5971731 , 0.5020732 ],\n",
       "       [0.9249599 , 0.14479335, 0.32194713, 0.5205036 ]], dtype=float32)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /tmp-data/zhoutongzt/DD/Data/BR_useReqTime_MMOE_CAN/model-100000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 0.09968653,  0.6192992 ,  0.08434715,  0.1492789 ],\n",
       "       [-0.18485333,  0.16393347, -0.12167899,  0.2712814 ],\n",
       "       [-0.04972822,  0.02612558, -0.01568268,  0.28784993],\n",
       "       [-0.7943455 , -0.17553413,  0.7769771 ,  0.3203123 ],\n",
       "       [ 0.7965788 ,  0.30415446, -0.09494041,  0.0148005 ],\n",
       "       [ 0.26068452, -0.14760247,  0.1682965 ,  0.35040897],\n",
       "       [ 0.5200607 ,  0.52716976,  0.38815165, -0.35098234],\n",
       "       [ 0.5012951 , -0.00572503,  0.6568418 ,  0.3084871 ],\n",
       "       [-0.59321654, -0.5363189 , -0.5635058 , -0.41347796],\n",
       "       [-0.03972653,  0.35015455,  0.28889334, -0.24242574]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "m = tf.Variable(np.random.random([12000000, 4]),dtype=tf.float32,name='dnn/emb_mlp_bias_0/input_layer/pickupHash_embedding/embedding_weights')\n",
    "\n",
    "ckpt_dir=\"/tmp-data/zhoutongzt/DD/Data/BR_useReqTime_MMOE_CAN\"\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    saver = tf.train.Saver(max_to_keep=1)\n",
    "    sess.run(m)[:10]\n",
    "    saver.restore(sess, tf.train.latest_checkpoint(ckpt_dir))\n",
    "    sess.run(m)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.contrib.framework.python.framework import checkpoint_utils\n",
    "\n",
    "class checkUtils:\n",
    "    def __init__(self, ckpt_dir, check_list=None):\n",
    "        self.ckpt_dir = ckpt_dir\n",
    "        self.check_list = check_list if check_list is not None else []\n",
    "    \n",
    "    @static\n",
    "    def load_name_shape()\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 梯度测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-15T04:54:37.474459Z",
     "start_time": "2020-12-15T04:54:37.426610Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(None,\n",
       "  <tf.Variable 'input_layer_4/featA_X_featB_embedding/embedding_weights:0' shape=(10, 4) dtype=float32_ref>),\n",
       " (None,\n",
       "  <tf.Variable 'input_layer_6/featA_X_featB_X_featC_embedding/embedding_weights:0' shape=(10, 4) dtype=float32_ref>),\n",
       " (None,\n",
       "  <tf.Variable 'input_layer_8/featA_X_featB_featA_X_featB_X_featC_shared_embedding/embedding_weights:0' shape=(10, 4) dtype=float32_ref>),\n",
       " (None, <tf.Variable 'Variable:0' shape=(4,) dtype=float32_ref>),\n",
       " (None, <tf.Variable 'Variable_1:0' shape=(4,) dtype=float32_ref>),\n",
       " (None, <tf.Variable 'Variable_2:0' shape=(4,) dtype=float32_ref>),\n",
       " (None, <tf.Variable 'Variable_3:0' shape=(3,) dtype=float32_ref>),\n",
       " (<tf.Tensor 'gradients_1/mul_1_grad/tuple/control_dependency:0' shape=(4,) dtype=float32>,\n",
       "  <tf.Variable 'Variable_4:0' shape=(4,) dtype=float32_ref>),\n",
       " (None, <tf.Variable 'Variable_5:0' shape=(3,) dtype=float32_ref>)]"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<bound method GradientDescentOptimizer.apply_gradients of <tensorflow.python.training.gradient_descent.GradientDescentOptimizer object at 0x7fed7372e2d0>>"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = tf.Variable([1,2,3,4], dtype=tf.float32)\n",
    "b = a*2\n",
    "c = tf.Variable([1,2,3], dtype=tf.float32)\n",
    "opt = tf.train.GradientDescentOptimizer(0.1)\n",
    "opt.compute_gradients(b)\n",
    "opt.apply_gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-27T11:32:00.011686Z",
     "start_time": "2020-11-27T11:32:00.001416Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['sigmoid_cross_entropy_with_logits',\n",
       " 'softmax_cross_entropy_with_logits',\n",
       " 'softmax_cross_entropy_with_logits_v2',\n",
       " 'sparse_softmax_cross_entropy_with_logits',\n",
       " 'weighted_cross_entropy_with_logits']"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[i for i in dir(tf.nn) if \"entropy\" in i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-27T12:07:13.292832Z",
     "start_time": "2020-11-27T12:07:12.854315Z"
    },
    "run_control": {
     "marked": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'logits'"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "array([[0.64769123, 0.99691358, 0.51880326, 0.65811273],\n",
       "       [0.59906347, 0.75306733, 0.13624713, 0.00411712],\n",
       "       [0.14950888, 0.698439  , 0.59335256, 0.89991535],\n",
       "       [0.44445739, 0.316785  , 0.92308176, 0.46586186]])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "'labels'"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "[1, 1, 2, 3]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "'softmax(logits)'"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "array([[0.23215253, 0.32918403, 0.20407889, 0.23458456],\n",
       "       [0.29872185, 0.34845766, 0.18804786, 0.16477263],\n",
       "       [0.15605622, 0.27019568, 0.24324277, 0.33050533],\n",
       "       [0.22145306, 0.19491006, 0.35739264, 0.22624424]])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "'target-one of softmax(logits)'"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "[0.32918402769283184,\n",
       " 0.34845765974154597,\n",
       " 0.24324277039282377,\n",
       " 0.22624423686097198]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "[-1.111138329907829,\n",
       " -1.05423854901307,\n",
       " -1.413695279250508,\n",
       " -1.4861401691642224]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "1.2663030624389648"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(1024)\n",
    "logits = np.random.random([4,4])\n",
    "labels=[1,1,2,3]\n",
    "\"logits\"\n",
    "logits\n",
    "\"labels\"\n",
    "labels\n",
    "with tf.Session() as sess:\n",
    "    \"softmax(logits)\"\n",
    "    sess.run(tf.nn.softmax(logits))\n",
    "    \"target-one of softmax(logits)\"\n",
    "    to = [sess.run(tf.nn.softmax(logits))[idx,i] for idx,i in enumerate(labels)]\n",
    "    to\n",
    "    [np.log(i) for i in to]\n",
    "    sess.run(tf.losses.sparse_softmax_cross_entropy(logits=logits,labels=labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python2.7(tf1.12)",
   "language": "python",
   "name": "tf1.12"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "310px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
