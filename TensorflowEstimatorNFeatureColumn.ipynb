{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-08T11:45:33.857519Z",
     "start_time": "2021-06-08T11:45:33.358201Z"
    }
   },
   "outputs": [],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "%matplotlib inline\n",
    "from tqdm.auto import tqdm\n",
    "import concurrent.futures\n",
    "from multiprocessing import Pool\n",
    "import copy,os,sys\n",
    "from collections import Counter,deque\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "import functools, itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-08T11:45:35.691164Z",
     "start_time": "2021-06-08T11:45:33.859453Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.feature_column as fc\n",
    "import numpy as np\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', 70)\n",
    "pd.set_option('display.max_rows', 70)\n",
    "from pyhocon import ConfigFactory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-08T11:45:35.695953Z",
     "start_time": "2021-06-08T11:45:35.693363Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"-1\"  # 禁用GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-08T11:45:35.703673Z",
     "start_time": "2021-06-08T11:45:35.697683Z"
    }
   },
   "outputs": [],
   "source": [
    "DATA_PATH=\"/tmp-data/zhoutongzt/DD/Data\"\n",
    "NFS_DATA_PATH=\"/nfs/map-tmp-vol0/zhoutong/Data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-08T12:20:04.653347Z",
     "start_time": "2020-12-08T12:20:04.609945Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 1, 2])"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "array([3, 3, 2])"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "array([[1, 0, 0],\n",
       "       [0, 1, 1]])"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "array([0.25, 0.25, 0.5 ])"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = [[1,0,0],\n",
    "     [0,0,0],\n",
    "     [0,0,1],\n",
    "     [0,1,1]]\n",
    "m = np.array(m)\n",
    "np.count_nonzero(m, axis=1)\n",
    "np.count_nonzero(m == 0 , axis=0)\n",
    "m[[0,3],:]\n",
    "np.mean(m, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-16T02:55:03.418909Z",
     "start_time": "2021-04-16T02:55:03.331370Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-9ff9e3eaacc5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_default_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mtrd_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"/nfs/project/zhoutongzt/ETA/data_parsed_by_line/10-17-000004_test.trd\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m features={\n\u001b[1;32m      5\u001b[0m     \u001b[0;34m\"uid_idx_list\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFixedLenFeature\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m70\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tf' is not defined"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "trd_dir = \"/nfs/project/zhoutongzt/ETA/data_parsed_by_line/10-17-000004_test.trd\"\n",
    "features={\n",
    "    \"uid_idx_list\": tf.FixedLenFeature([70], tf.int64),\n",
    "    \"driver_phone_ori\": tf.FixedLenFeature([], tf.string),\n",
    "    \"first_length_bucket\": tf.FixedLenFeature([], tf.int64),\n",
    "    \"age_mean\": tf.FixedLenFeature([], tf.float32)\n",
    "}\n",
    "\n",
    "\n",
    "trd_fp = [os.path.join(trd_dir,i) for i in os.listdir(trd_dir)] if os.path.isdir(trd_dir) else trd_dir\n",
    "trd=tf.data.TFRecordDataset(trd_fp).map(lambda inp: tf.io.parse_single_example(inp,features)).batch(1024)\n",
    "iterator=trd.make_one_shot_iterator()\n",
    "one=iterator.get_next()\n",
    "\n",
    "\n",
    "\n",
    "# codec column\n",
    "fc_list=[]\n",
    "fc_wide = fc.categorical_column_with_identity(\"uid_idx_list\", 5*10000)\n",
    "\n",
    "fc_list.append(fc.indicator_column(fc_wide))\n",
    "\n",
    "net = fc.input_layer(one, fc_list)\n",
    "gpu_options = tf.GPUOptions(allow_growth=True)\n",
    "session_config = tf.ConfigProto(allow_soft_placement=True, gpu_options=gpu_options)\n",
    "with tf.Session(config=session_config) as sess:\n",
    "    net.shape\n",
    "    sess.run(net).shape\n",
    "    sess.run(net)\n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 处理TFRecord | tf1.x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 检查schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-15T07:33:28.152301Z",
     "start_time": "2021-04-15T07:33:28.080947Z"
    },
    "run_control": {
     "marked": true
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[featName]:age_25_q [type]:float_list [value_example]:[29.0]\n",
      "[featName]:age_30 [type]:int64_list [value_example]:[2]\n",
      "[featName]:age_40 [type]:int64_list [value_example]:[2]\n",
      "[featName]:age_50 [type]:int64_list [value_example]:[0]\n",
      "[featName]:age_50_q [type]:float_list [value_example]:[32.5]\n",
      "[featName]:age_75_q [type]:float_list [value_example]:[35.25]\n",
      "[featName]:age_mean [type]:float_list [value_example]:[31.75]\n",
      "[featName]:all_due_time [type]:float_list [value_example]:[2410.0]\n",
      "[featName]:assemble_eta [type]:int64_list [value_example]:[2160]\n",
      "[featName]:category_id_count [type]:int64_list [value_example]:[43]\n",
      "[featName]:date [type]:int64_list [value_example]:[20210306]\n",
      "[featName]:delivery_flag [type]:int64_list [value_example]:[1]\n",
      "[featName]:delivery_id_1 [type]:int64_list [value_example]:[4]\n",
      "[featName]:delivery_id_2 [type]:int64_list [value_example]:[0]\n",
      "[featName]:delivery_id_3 [type]:int64_list [value_example]:[0]\n",
      "[featName]:delivery_id_mean [type]:float_list [value_example]:[1.0]\n",
      "[featName]:driver_phone [type]:int64_list [value_example]:[637312124016409538]\n",
      "[featName]:driver_phone_ori [type]:bytes_list [value_example]:[b'18980633107_3']\n",
      "[featName]:female_count [type]:int64_list [value_example]:[3]\n",
      "[featName]:first_eta [type]:float_list [value_example]:[4569.0]\n",
      "[featName]:first_length [type]:float_list [value_example]:[48736.0]\n",
      "[featName]:first_length_bucket [type]:int64_list [value_example]:[9]\n",
      "[featName]:first_order_num [type]:int64_list [value_example]:[33]\n",
      "[featName]:fresh_count [type]:int64_list [value_example]:[32]\n",
      "[featName]:fruits_count [type]:int64_list [value_example]:[4]\n",
      "[featName]:gender_50 [type]:float_list [value_example]:[0.0]\n",
      "[featName]:gmv_25 [type]:float_list [value_example]:[20.360000610351562]\n",
      "[featName]:gmv_50 [type]:float_list [value_example]:[64.19499969482422]\n",
      "[featName]:gmv_75 [type]:float_list [value_example]:[131.77000427246094]\n",
      "[featName]:gmv_mean [type]:float_list [value_example]:[87.93499755859375]\n",
      "[featName]:gmv_mean_bucket [type]:int64_list [value_example]:[4]\n",
      "[featName]:has_store_id_-1 [type]:int64_list [value_example]:[0]\n",
      "[featName]:has_store_id_0 [type]:int64_list [value_example]:[2]\n",
      "[featName]:has_store_id_1 [type]:int64_list [value_example]:[2]\n",
      "[featName]:has_store_id_2 [type]:int64_list [value_example]:[0]\n",
      "[featName]:has_store_id_3 [type]:int64_list [value_example]:[0]\n",
      "[featName]:has_store_id_mean [type]:float_list [value_example]:[0.5]\n",
      "[featName]:is_month_new_leader [type]:int64_list [value_example]:[4]\n",
      "[featName]:is_month_new_leader_mean [type]:float_list [value_example]:[0.0]\n",
      "[featName]:is_td_new_count [type]:int64_list [value_example]:[0]\n",
      "[featName]:is_td_new_count_mean [type]:float_list [value_example]:[0.0]\n",
      "[featName]:isnot_month_new_leader [type]:int64_list [value_example]:[0]\n",
      "[featName]:isnot_td_new_count [type]:int64_list [value_example]:[4]\n",
      "[featName]:label [type]:int64_list [value_example]:[6979]\n",
      "[featName]:last_action_time [type]:bytes_list [value_example]:[b'2021-03-06 09:17:30']\n",
      "[featName]:last_order_num [type]:int64_list [value_example]:[6]\n",
      "[featName]:last_uid [type]:int64_list [value_example]:[639247268642556088]\n",
      "[featName]:leader_eta [type]:float_list [value_example]:[445.0]\n",
      "[featName]:leader_length [type]:float_list [value_example]:[1742.0]\n",
      "[featName]:leader_uid_cnt [type]:int64_list [value_example]:[4]\n",
      "[featName]:leader_uid_cnt_bucket [type]:int64_list [value_example]:[0]\n",
      "[featName]:line_gmv [type]:float_list [value_example]:[351.739990234375]\n",
      "[featName]:line_num_sum [type]:int64_list [value_example]:[88]\n",
      "[featName]:line_num_sum_bucket [type]:int64_list [value_example]:[4]\n",
      "[featName]:line_sku_cnt [type]:float_list [value_example]:[55.0]\n",
      "[featName]:male_count [type]:int64_list [value_example]:[1]\n",
      "[featName]:middle_order_num [type]:int64_list [value_example]:[14]\n",
      "[featName]:num_sum_25 [type]:float_list [value_example]:[12.0]\n",
      "[featName]:num_sum_50 [type]:float_list [value_example]:[23.5]\n",
      "[featName]:num_sum_75 [type]:float_list [value_example]:[33.5]\n",
      "[featName]:num_sum_mean [type]:float_list [value_example]:[22.0]\n",
      "[featName]:refrigeration_type_0 [type]:int64_list [value_example]:[6]\n",
      "[featName]:refrigeration_type_1 [type]:int64_list [value_example]:[8]\n",
      "[featName]:refrigeration_type_2 [type]:int64_list [value_example]:[0]\n",
      "[featName]:refrigeration_type_3 [type]:int64_list [value_example]:[20]\n",
      "[featName]:sku_cnt_25 [type]:float_list [value_example]:[5.75]\n",
      "[featName]:sku_cnt_50 [type]:float_list [value_example]:[12.0]\n",
      "[featName]:sku_cnt_75 [type]:float_list [value_example]:[20.0]\n",
      "[featName]:sku_cnt_mean [type]:float_list [value_example]:[13.75]\n",
      "[featName]:store_type_25 [type]:float_list [value_example]:[0.0]\n",
      "[featName]:store_type_50 [type]:float_list [value_example]:[25.0]\n",
      "[featName]:store_type_75 [type]:float_list [value_example]:[50.0]\n",
      "[featName]:store_type_mean [type]:float_list [value_example]:[25.0]\n",
      "[featName]:tocar_time [type]:bytes_list [value_example]:[b'2021-03-06 07:21:11']\n",
      "[featName]:uid_idx_list [type]:int64_list [value_example]:[2704, 2705, 13371, 13589, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1]\n",
      "[featName]:uid_str [type]:bytes_list [value_example]:[b'637312124016402439,637312124016409538,639247268632470368,639247268642556088']\n",
      "[featName]:uid_str_list [type]:bytes_list [value_example]:[b'637312124016402439', b'637312124016409538', b'639247268632470368', b'639247268642556088']\n",
      "[featName]:user_cnt_25 [type]:float_list [value_example]:[1.0]\n",
      "[featName]:user_cnt_50 [type]:float_list [value_example]:[2.5]\n",
      "[featName]:user_cnt_75 [type]:float_list [value_example]:[4.75]\n",
      "[featName]:user_cnt_mean [type]:float_list [value_example]:[3.25]\n",
      "[featName]:vgtble_count [type]:int64_list [value_example]:[19]\n"
     ]
    }
   ],
   "source": [
    "# trd_dir=\"/tmp-data/zhoutongzt/DD/Data/BR_useReqTime_addCross_test\"\n",
    "# trd_dir = \"/tmp-data/zhoutongzt/DD/Data/BR_useReqTime_expTryMMOE_train\"\n",
    "trd_dir = \"/nfs/project/zhoutongzt/ETA/data_parsed_by_line/10-17-000004_test.trd\"\n",
    "trd_fp = [os.path.join(trd_dir,i) for i in os.listdir(trd_dir)] if os.path.isdir(trd_dir) else trd_dir\n",
    "\n",
    "# trd=tf.data.TFRecordDataset(trd_fp,compression_type='GZIP')\n",
    "trd=tf.data.TFRecordDataset(trd_fp)\n",
    "iterator=trd.make_one_shot_iterator()\n",
    "one=iterator.get_next()\n",
    "res = []\n",
    "with tf.Session() as sess:\n",
    "    one_ = sess.run(one)\n",
    "    featInfo = tf.train.Example.FromString(one_).features.feature # not dict\n",
    "    for key in featInfo:\n",
    "        info_list=[(key, attr, featInfo[key].__getattribute__(attr).value) for attr in [\"bytes_list\",\"float_list\",\"int64_list\"]]\n",
    "        info = [i for i in info_list if i[-1] != []][0] # 只应该有一个\n",
    "        res.append(\"[featName]:{} [type]:{} [value_example]:{}\".format(*info))\n",
    "\n",
    "print(\"\\n\".join(sorted(res)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 解析"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-15T07:32:44.456511Z",
     "start_time": "2021-04-15T07:32:44.414519Z"
    }
   },
   "outputs": [],
   "source": [
    "trd_dir = \"/nfs/project/zhoutongzt/ETA/data_parsed_by_line/10-17-000004_test.trd\"\n",
    "trd_fp = [os.path.join(trd_dir,i) for i in os.listdir(trd_dir)] if os.path.isdir(trd_dir) else trd_dir\n",
    "wide_size = 35\n",
    "config_path = open('/tmp-data/zhoutongzt/DD/tensorflow_lr/src/main/resources/features_oriFeat.json','r')\n",
    "conf = json.load(config_path)\n",
    "features={}\n",
    "features['wideFeatures'] = tf.io.FixedLenFeature([wide_size], tf.int64)\n",
    "# features['label'] = tf.io.FixedLenFeature([1], tf.int64)\n",
    "for deepName in conf[\"deepFeat\"]:\n",
    "    fshape = conf[\"varlensFeat_length\"].get(deepName, None)\n",
    "    fshape = [] if fshape is None else [fshape]\n",
    "    fdtype = tf.float32 if deepName in conf[\"floatFeat\"] else tf.int64\n",
    "    features[deepName] = tf.io.FixedLenFeature(fshape, fdtype)\n",
    "\n",
    "print(\"[features.keys]: \"+\", \".join(sorted(features.keys())))\n",
    "def _parse_func(inp):\n",
    "    parsed_features = tf.io.parse_single_example(inp,features)\n",
    "    return parsed_features\n",
    "\n",
    "trd=tf.data.TFRecordDataset(trd_fp,compression_type='GZIP').map(_parse_func)\n",
    "\n",
    "####################\n",
    "# tf1.x specific\n",
    "####################\n",
    "dataset = trd.batch(64,drop_remainder=True)\n",
    "line = dataset.make_one_shot_iterator().get_next()\n",
    "to_print = [\"dnnCrsCurRoad\",]\n",
    "with tf.Session() as sess:\n",
    "    i = sess.run(line)\n",
    "    print(\">>> all keys: {}\\n\".format(\",\".join(i.keys())))\n",
    "    for key in to_print:\n",
    "        print(\">>> {}:\".format(key))\n",
    "        print(i[key])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 处理TFRecord | tf2.x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 检查schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-29T09:48:22.757067Z",
     "start_time": "2020-12-29T09:48:22.649407Z"
    }
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "dataset.__iter__() is only supported when eager execution is enabled.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-198-aa6f5e496829>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mtrd_unparsed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTFRecordDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrd_fp\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcompression_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'GZIP'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mraw_record\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrd_unparsed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0mexample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexample\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mParseFromString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_record\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp-data/luban/anaconda3/envs/tf1.12/lib/python2.7/site-packages/tensorflow/python/data/ops/dataset_ops.pyc\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    165\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0miterator_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEagerIterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 167\u001b[0;31m       raise RuntimeError(\"dataset.__iter__() is only supported when eager \"\n\u001b[0m\u001b[1;32m    168\u001b[0m                          \"execution is enabled.\")\n\u001b[1;32m    169\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: dataset.__iter__() is only supported when eager execution is enabled."
     ]
    }
   ],
   "source": [
    "trd_dir = \"/nfs/map-tmp-vol0/zhoutong/Data/BR_useReqTime_addCross_addTimeByFreq_train\"\n",
    "trd_dir = \"/tmp-data/zhoutongzt/DD/Data/BR_useReqTime_expTryMMOE_train\"\n",
    "trd_fp = [os.path.join(trd_dir,i) for i in os.listdir(trd_dir)]\n",
    "trd_unparsed=tf.data.TFRecordDataset(trd_fp,compression_type='GZIP')\n",
    "\n",
    "for raw_record in trd_unparsed.take(1):\n",
    "    example = tf.train.Example()\n",
    "    _ = example.ParseFromString(raw_record.numpy())\n",
    "    \n",
    "def format_exampleInfo(exampleInfo):\n",
    "    res = []\n",
    "    for name,feat in exampleInfo.features.feature.items():\n",
    "        info_list=[(name, attr, feat.__getattribute__(attr).value) for attr in [\"bytes_list\",\"float_list\",\"int64_list\"]]\n",
    "        info = [(name,attr,value) for name,attr,value in info_list if value != []][0]\n",
    "        res.append(\"[featName]:{} [type]:{} [value_example]:{}\".format(*info))\n",
    "    print(\"\\n\".join(sorted(map(str,res))))\n",
    "    \n",
    "format_exampleInfo(example)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 解析"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trd_dir = \"/nfs/map-tmp-vol0/zhoutong/Data/BR_useReqTime_addCross_addTimeByFreq_addTimeHeatCross_test\"\n",
    "trd_fp = [os.path.join(trd_dir,i) for i in os.listdir(trd_dir)]\n",
    "wide_size = 48\n",
    "config_path = open('/tmp-data/zhoutongzt/DD/tensorflow_lr/src/main/resources/features_useReqTime_addCross.json','r')\n",
    "# config_path = open('/tmp-data/zhoutongzt/DD/tensorflow_lr/src/main/resources/features_oriFeat.json','r')\n",
    "conf = json.load(config_path)\n",
    "features={}\n",
    "features['wideFeatures'] = tf.io.FixedLenFeature([wide_size], tf.int64)\n",
    "features['label'] = tf.io.FixedLenFeature([1], tf.int64)\n",
    "for deepName in conf[\"deepFeat\"]:\n",
    "    fshape = conf[\"varlensFeat_length\"].get(deepName, None)\n",
    "    fshape = [] if fshape is None else [fshape]\n",
    "    fdtype = tf.float32 if deepName in conf[\"floatFeat\"] else tf.int64\n",
    "    features[deepName] = tf.io.FixedLenFeature(fshape, fdtype)\n",
    "\n",
    "print(\"[features.keys]: \"+\", \".join(sorted(features.keys())))\n",
    "def _parse_func(inp):\n",
    "    parsed_features = tf.io.parse_single_example(inp,features)\n",
    "    return parsed_features\n",
    "\n",
    "trd=tf.data.TFRecordDataset(trd_fp,compression_type='GZIP').map(_parse_func)\n",
    "\n",
    "####################\n",
    "# tf2.x specific\n",
    "####################\n",
    "for i in trd.take(1):\n",
    "    print(i)\n",
    "    print(\"\\n[keys]:\" + str(i.keys()))\n",
    "    print(\"\\n[heat_time_idx_poi_id]: \" + str(i['heat_time_idx_poi_id']))\n",
    "    print(\"[heat_time_idx_pickup_geoHash]: \" + str(i['heat_time_idx_pickup_geoHash']))\n",
    "    print(\"[heat_time_idx_bind_line_id_single]: \" + str(i['heat_time_idx_bind_line_id_single']))\n",
    "#     print(\"[weights]:\" + str(i['weights']))\n",
    "#     print(\"[order_info]:\" + str(i['order_info']))\n",
    "    print(i['wideFeatures'].shape)\n",
    "    \n",
    "# 跑不出来377w数太多了？\n",
    "# set([i['wideFeatures'].shape for i in trd.as_numpy_iterator()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# feature_column TEST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## categorical_column_with_hash_bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-01T07:16:36.445037Z",
     "start_time": "2021-04-01T07:16:36.420279Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'featA': array([[0],\n",
       "        [3]]),\n",
       " 'featB': array([[2],\n",
       "        [6]]),\n",
       " 'featC': array([[3],\n",
       "        [6]]),\n",
       " 'featD': array([[0],\n",
       "        [0]]),\n",
       " 'featE': array([[0],\n",
       "        [6]]),\n",
       " 'pickupHash': array([[4],\n",
       "        [1]])}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>featA</th>\n",
       "      <th>featB</th>\n",
       "      <th>featC</th>\n",
       "      <th>featD</th>\n",
       "      <th>featE</th>\n",
       "      <th>pickupHash</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[0]</td>\n",
       "      <td>[2]</td>\n",
       "      <td>[3]</td>\n",
       "      <td>[0]</td>\n",
       "      <td>[0]</td>\n",
       "      <td>[4]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[3]</td>\n",
       "      <td>[6]</td>\n",
       "      <td>[6]</td>\n",
       "      <td>[0]</td>\n",
       "      <td>[6]</td>\n",
       "      <td>[1]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  featA featB featC featD featE pickupHash\n",
       "0   [0]   [2]   [3]   [0]   [0]        [4]\n",
       "1   [3]   [6]   [6]   [0]   [6]        [1]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_buckets_deep=7\n",
    "dims_embs_deep=4\n",
    "_batch_size=2\n",
    "\n",
    "# features\n",
    "features = {}\n",
    "features[\"featA\"] = np.random.randint(num_buckets_deep, size=(_batch_size, 1))\n",
    "features[\"featB\"] = np.random.randint(num_buckets_deep, size=(_batch_size, 1))\n",
    "features[\"featC\"] = np.random.randint(num_buckets_deep, size=(_batch_size, 1))\n",
    "features[\"featD\"] = np.random.randint(num_buckets_deep, size=(_batch_size, 1))\n",
    "features[\"featE\"] = np.random.randint(num_buckets_deep, size=(_batch_size, 1))\n",
    "features[\"pickupHash\"] = np.random.randint(num_buckets_deep, size=(_batch_size, 1))\n",
    "features\n",
    "pd.DataFrame({k:v.tolist() for k,v in features.items()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-01T07:19:11.182566Z",
     "start_time": "2021-04-01T07:19:10.841472Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 7)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 1., 0., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(2, 10)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(2, 15)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(2, 25)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "# codec column\n",
    "\n",
    "featA_cat_id_column = fc.categorical_column_with_identity(\"featA\", num_buckets_deep, 0)\n",
    "featA_cat_hash_column = fc.categorical_column_with_hash_bucket(\"featA\", hash_bucket_size=10, dtype=tf.int64)\n",
    "featA_indicator = fc.indicator_column(featA_cat_hash_column)\n",
    "\n",
    "featB_cat_hash_column = fc.categorical_column_with_hash_bucket(\"featB\", hash_bucket_size=15, dtype=tf.int64)\n",
    "featB_indicator = fc.indicator_column(featB_cat_hash_column)\n",
    "\n",
    "with tf.Session():\n",
    "    fc.input_layer(features, fc.indicator_column(featA_cat_id_column)).eval().shape\n",
    "    fc.input_layer(features, fc.indicator_column(featA_cat_id_column)).eval()\n",
    "    fc.input_layer(features, featA_indicator).eval().shape\n",
    "    fc.input_layer(features, featA_indicator).eval()\n",
    "    fc.input_layer(features, featB_indicator).eval().shape\n",
    "    fc.input_layer(features, featB_indicator).eval()\n",
    "    fc.input_layer(features, [featA_indicator,featB_indicator]).eval().shape\n",
    "    fc.input_layer(features, [featA_indicator,featB_indicator]).eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 带权column \n",
    "- 例如 特征是 [[id1,cnt1],[id2,cnt],[id3,cnt3]] 这样的\n",
    "    - 以LR为例我们希望把cnt乘到对应id的w上\n",
    "    - 以emb为例我们希望把cnt乘到对应id的emb上"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### data parse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-09T06:13:39.572281Z",
     "start_time": "2021-01-09T06:13:39.346714Z"
    },
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> all keys: categoryIds_indices,categoryIds_values,categoryIdSize,vl_volume\n",
      "\n",
      ">>> categoryIdSize:\n",
      "[6073 6073 6073 6073 6073 6073 6073 6073 6073 6073 6073 6073 6073 6073\n",
      " 6073 6073 6073 6073 6073 6073 6073 6073 6073 6073 6073 6073 6073 6073\n",
      " 6073 6073 6073 6073 6073 6073 6073 6073 6073 6073 6073 6073 6073 6073\n",
      " 6073 6073 6073 6073 6073 6073 6073 6073 6073 6073 6073 6073 6073 6073\n",
      " 6073 6073 6073 6073 6073 6073 6073 6073]\n",
      ">>> categoryIds_indices:\n",
      "SparseTensorValue(indices=array([[ 0,  0],\n",
      "       [ 0,  1],\n",
      "       [ 0,  2],\n",
      "       ...,\n",
      "       [63, 10],\n",
      "       [63, 11],\n",
      "       [63, 12]]), values=array([1066, 1068, 1070, 1072, 1074, 1080, 1082, 1084, 1092, 1112, 1116,\n",
      "       1175, 6072, 1066, 1068, 1070, 1072, 1074, 1080, 1082, 1084, 1092,\n",
      "       1112, 1116, 1175, 6072, 1066, 1068, 1070, 1072, 1074, 1080, 1082,\n",
      "       1084, 1092, 1112, 1116, 1175, 6072, 1066, 1068, 1070, 1072, 1074,\n",
      "       1080, 1082, 1084, 1092, 1112, 1116, 1175, 6072, 1066, 1068, 1070,\n",
      "       1072, 1074, 1080, 1082, 1084, 1092, 1112, 1116, 1175, 6072, 1066,\n",
      "       1068, 1070, 1072, 1074, 1080, 1082, 1084, 1092, 1112, 1116, 1175,\n",
      "       6072, 1066, 1074, 1080, 1082, 1084, 1066, 1080, 1082, 1084, 1066,\n",
      "       1068, 1070, 1072, 1074, 1080, 1082, 1084, 1175, 1066, 1068, 1070,\n",
      "       1072, 1074, 1080, 1082, 1084, 1092, 1112, 1116, 1175, 6072, 1066,\n",
      "       1068, 1070, 1072, 1074, 1080, 1082, 1084, 1092, 1112, 1116, 1175,\n",
      "       6072, 1066, 1068, 1070, 1072, 1074, 1080, 1082, 1084, 1092, 1112,\n",
      "       1116, 1175, 6072, 1066, 1068, 1070, 1072, 1074, 1080, 1082, 1084,\n",
      "       1092, 1112, 1116, 1175, 6072, 1066, 1068, 1070, 1072, 1074, 1080,\n",
      "       1082, 1084, 1092, 1112, 1116, 1175, 6072, 1066, 1068, 1070, 1072,\n",
      "       1074, 1080, 1082, 1084, 1092, 1112, 1116, 1175, 6072, 1066, 1068,\n",
      "       1070, 1072, 1074, 1080, 1082, 1084, 1092, 1112, 1116, 1175, 6072,\n",
      "       1066, 1068, 1070, 1072, 1074, 1080, 1082, 1084, 1092, 1112, 1116,\n",
      "       1175, 6072, 1066, 1068, 1072, 1074, 1080, 1082, 1084, 1116, 1175,\n",
      "       1066, 1068, 1070, 1072, 1074, 1080, 1082, 1084, 1092, 1112, 1116,\n",
      "       1175, 6072, 1066, 1068, 1070, 1072, 1074, 1080, 1082, 1084, 1092,\n",
      "       1112, 1116, 1175, 6072, 1066, 1068, 1070, 1072, 1074, 1080, 1082,\n",
      "       1084, 1092, 1112, 1116, 1175, 6072, 1066, 1068, 1070, 1072, 1074,\n",
      "       1080, 1082, 1084, 1092, 1112, 1116, 1175, 6072, 1066, 1068, 1070,\n",
      "       1072, 1074, 1080, 1082, 1084, 1092, 1112, 1116, 1175, 6072, 1066,\n",
      "       1068, 1070, 1072, 1074, 1080, 1082, 1084, 1092, 1112, 1116, 1175,\n",
      "       6072,    0, 1066, 1068, 1070, 1072, 1074, 1080, 1082, 1084, 1092,\n",
      "       1112, 1116, 1175, 6072, 1066, 1068, 1070, 1072, 1074, 1080, 1082,\n",
      "       1084, 1092, 1116, 1175, 6072, 1066, 1068, 1070, 1072, 1074, 1080,\n",
      "       1082, 1084, 1092, 1112, 1116, 1175, 6072, 1066, 1068, 1070, 1072,\n",
      "       1074, 1080, 1082, 1084, 1092, 1112, 1116, 1175, 6072, 1066, 1068,\n",
      "       1070, 1072, 1074, 1080, 1082, 1084, 1092, 1112, 1116, 1175, 6072,\n",
      "       1066, 1068, 1070, 1072, 1074, 1080, 1082, 1084, 1092, 1112, 1116,\n",
      "       1175, 6072, 1066, 1068, 1070, 1072, 1074, 1080, 1082, 1084, 1092,\n",
      "       1112, 1116, 1175, 6072, 1066, 1068, 1072, 1074, 1080, 1082, 1084,\n",
      "       1116, 1066, 1068, 1070, 1072, 1074, 1080, 1082, 1084, 1092, 1112,\n",
      "       1116, 1175, 6072,    0, 1066, 1068, 1070, 1072, 1074, 1080, 1082,\n",
      "       1084, 1092, 1112, 1116, 1175, 6072,    0, 1066, 1068, 1070, 1072,\n",
      "       1074, 1080, 1082, 1084, 1092, 1112, 1116, 1175, 6072, 1066, 1068,\n",
      "       1070, 1072, 1074, 1080, 1082, 1084, 1092, 1112, 1116, 1175, 6072,\n",
      "       1066, 1068, 1070, 1072, 1074, 1080, 1082, 1084, 1092, 1112, 1116,\n",
      "       1175, 6072, 1066, 1068, 1070, 1072, 1074, 1080, 1082, 1084, 1092,\n",
      "       1112, 1116, 1175, 6072, 1066, 1068, 1070, 1072, 1074, 1080, 1082,\n",
      "       1084, 1116, 1175, 6072, 1066, 1068, 1070, 1072, 1074, 1080, 1082,\n",
      "       1084, 1092, 1112, 1116, 1175, 6072, 1066, 1068, 1070, 1072, 1074,\n",
      "       1080, 1082, 1084, 1092, 1112, 1116, 1175, 6072, 1066, 1068, 1070,\n",
      "       1072, 1074, 1080, 1082, 1084, 1092, 1112, 1116, 1175, 6072, 1066,\n",
      "       1068, 1070, 1072, 1074, 1080, 1082, 1084, 1092, 1112, 1116, 1175,\n",
      "       6072, 1066, 1068, 1070, 1072, 1074, 1080, 1082, 1084, 1092, 1112,\n",
      "       1116, 1175, 6072, 1066, 1068, 1070, 1072, 1074, 1080, 1082, 1084,\n",
      "       1112, 1116, 1175, 6072, 1066, 1068, 1070, 1072, 1074, 1080, 1082,\n",
      "       1084, 1112, 1116, 1175, 6072, 1066, 1068, 1070, 1072, 1074, 1080,\n",
      "       1082, 1084, 1092, 1112, 1116, 1175, 6072, 1066, 1068, 1070, 1072,\n",
      "       1074, 1080, 1082, 1084, 1092, 1112, 1116, 1175, 6072, 1066, 1068,\n",
      "       1070, 1072, 1074, 1080, 1082, 1084, 1092, 1112, 1116, 1175, 6072,\n",
      "       1066, 1068, 1070, 1072, 1074, 1080, 1082, 1084, 1092, 1112, 1116,\n",
      "       1175, 6072, 1066, 1068, 1070, 1072, 1074, 1080, 1082, 1084, 1092,\n",
      "       1112, 1116, 1175, 6072, 1066, 1068, 1070, 1072, 1074, 1080, 1082,\n",
      "       1084, 1092, 1112, 1116, 1175, 6072, 1066, 1068, 1070, 1072, 1074,\n",
      "       1080, 1082, 1084, 1112, 1116, 1175, 6072, 1066, 1068, 1070, 1072,\n",
      "       1074, 1080, 1082, 1084, 1092, 1112, 1116, 1175, 6072, 1066, 1068,\n",
      "       1070, 1072, 1074, 1080, 1082, 1084, 1092, 1112, 1116, 1175, 6072,\n",
      "       1066, 1068, 1070, 1072, 1074, 1080, 1082, 1084, 1092, 1112, 1116,\n",
      "       1175, 6072, 1066, 1068, 1070, 1072, 1074, 1080, 1082, 1084, 1092,\n",
      "       1112, 1116, 1175, 6072, 1066, 1068, 1070, 1072, 1074, 1080, 1082,\n",
      "       1084, 1092, 1112, 1116, 1175, 6072, 1066, 1068, 1070, 1072, 1074,\n",
      "       1080, 1082, 1084, 1092, 1112, 1116, 1175, 6072, 1066, 1068, 1070,\n",
      "       1072, 1074, 1080, 1082, 1084, 1092, 1112, 1116, 1175, 6072, 1066,\n",
      "       1068, 1070, 1072, 1074, 1080, 1082, 1084, 1092, 1116, 1175, 6072,\n",
      "       1066, 1072, 1074, 1084, 1116, 1066, 1068, 1070, 1072, 1074, 1080,\n",
      "       1082, 1084, 1092, 1112, 1116, 1175, 6072, 1066, 1068, 1070, 1072,\n",
      "       1074, 1080, 1082, 1084, 1092, 1112, 1116, 1175, 6072]), dense_shape=array([64, 14]))\n",
      ">>> categoryIds_values:\n",
      "SparseTensorValue(indices=array([[ 0,  0],\n",
      "       [ 0,  1],\n",
      "       [ 0,  2],\n",
      "       ...,\n",
      "       [63, 10],\n",
      "       [63, 11],\n",
      "       [63, 12]]), values=array([200., 296.,  11., 113.,  24.,  86.,  25.,  64.,  12.,   3.,  49.,\n",
      "        56.,  17., 226., 316.,  20., 130.,  39.,  93.,  24.,  60.,   8.,\n",
      "         3.,  52.,  36.,  24., 173., 355.,  26., 116.,  35.,  92.,  30.,\n",
      "        75.,  10.,   1.,  53.,  50.,  38., 174., 344.,  24., 122.,  36.,\n",
      "        84.,  39.,  63.,   5.,   4.,  38.,  51.,  29., 156., 283.,  27.,\n",
      "       103.,  27.,  47.,  12.,  40.,   3.,   2.,  39.,  46.,  24., 133.,\n",
      "       207.,  26.,  72.,  18.,  37.,  23.,  42.,   7.,   2.,  32.,  36.,\n",
      "        16.,   1.,  52.,   1., 110.,  58.,   1.,   2., 181.,  52.,   2.,\n",
      "        10.,   2.,   4.,  10.,  19.,  59.,  49.,   6., 181., 221.,  14.,\n",
      "       130.,  28., 127.,  75., 139.,   4.,   6., 148.,  65.,  75., 211.,\n",
      "       243.,  11., 162.,  64., 146., 142., 168.,   4.,   6., 124.,  54.,\n",
      "        52., 139., 151.,  10.,  63.,  22.,  89.,  53.,  93.,   7.,   2.,\n",
      "        40.,  35.,  22.,  86., 173.,   8.,  79.,  14.,  67.,  52., 105.,\n",
      "         2.,   5.,  47.,  37.,  26., 114.,  92.,   8.,  51.,  15.,  46.,\n",
      "        12.,  67.,   4.,   1.,  63.,  23.,  14., 275., 430.,  18., 200.,\n",
      "        41., 157.,  80., 130.,   6.,   3., 104.,  63.,  61., 230., 481.,\n",
      "        31., 184.,  56., 263., 110., 239.,  10.,   7.,  88.,  96.,  87.,\n",
      "       132., 229.,  22., 104.,  44., 135.,  37.,  92.,   9.,   8.,  38.,\n",
      "        55.,  21.,  22.,  32.,  12.,   3.,  12.,   8.,   9.,  13.,   3.,\n",
      "       124., 255.,  13.,  84.,  33.,  90.,  63., 172.,   5.,   2.,  89.,\n",
      "        54.,  58., 123., 273.,  13.,  77.,  21., 135.,  71., 155.,   4.,\n",
      "         2.,  63.,  73.,  75., 108., 227.,   2., 122.,  35.,  98.,  81.,\n",
      "        98.,   6.,   4.,  73.,  59.,  42., 110., 258.,   8., 115.,  38.,\n",
      "       145.,  82., 216.,   7.,   3.,  89.,  85.,  87., 110., 186.,  15.,\n",
      "        71.,  28., 110., 151.,  73.,   4.,   2.,  85.,  79.,  33.,  51.,\n",
      "       109.,   5.,  57.,   9.,  77., 131.,  31.,   1.,   1.,  78.,  44.,\n",
      "        19.,   1., 125., 194.,   8.,  75.,  15.,  90.,  42.,  73.,   5.,\n",
      "         6.,  58.,  50.,  23., 172., 252.,  12.,  95.,  35., 127.,  92.,\n",
      "        97.,   8.,  89.,  52.,  44., 198., 231.,  14., 135.,  52., 102.,\n",
      "        50., 107.,   4.,   4.,  67.,  62.,  23., 237., 264.,  25., 167.,\n",
      "        61., 151.,  73.,  96.,   7.,   6.,  81.,  69.,  54., 151., 205.,\n",
      "        25.,  99.,  54., 127.,  64.,  85.,  10.,   1.,  69.,  82.,  48.,\n",
      "        98., 119.,   7.,  28.,  22.,  69.,  30.,  86.,   2.,   3.,  38.,\n",
      "        29.,  22.,  76., 101.,  10.,  57.,  36.,  68.,  87.,  49.,   2.,\n",
      "         3.,  61.,  38.,  22.,  12.,   2.,   1.,   4.,  11.,  37.,  31.,\n",
      "        23.,  91., 166.,   4.,  50.,  25.,  31.,  42.,  22.,   3.,   2.,\n",
      "        26.,  26.,  19.,   1., 109., 178.,   9.,  60.,  13., 118.,  37.,\n",
      "       117.,  11.,   8.,  53.,  50.,  28.,   1., 182., 214.,   8.,  92.,\n",
      "        55., 170.,  54.,  94.,  11.,   2.,  77.,  61.,  47., 182., 160.,\n",
      "        16.,  92.,  42., 163.,  51.,  87.,  19.,   4.,  63.,  46.,  30.,\n",
      "       187., 237.,  24.,  90.,  38., 177.,  70., 129.,  11.,   5.,  63.,\n",
      "        72.,  35., 138., 204.,  19., 105.,  35., 122.,  57., 130.,  11.,\n",
      "         7.,  57.,  85.,  54., 106., 117.,   9.,  64.,  28.,  87.,  30.,\n",
      "        36.,  43.,  43.,  27., 150., 195.,   9., 120.,  31., 115.,  72.,\n",
      "        85.,   9.,   6.,  37.,  49.,  31., 142., 255.,  18., 132.,  23.,\n",
      "       136., 106., 151.,   8.,   6.,  52.,  60.,  18.,  90., 143.,  14.,\n",
      "        64.,  40.,  58.,  46.,  54.,   4.,   8.,  32.,  34.,  23.,  53.,\n",
      "        74.,   6.,  30.,  13.,  42.,  20.,  34.,   1.,   1.,  32.,  12.,\n",
      "        16.,  55.,  97.,  12.,  21.,  35.,  55.,  37.,  26.,   2.,   1.,\n",
      "        37.,  27.,  17., 129., 140.,   5.,  56.,  13.,  75.,  81.,  64.,\n",
      "         4.,  80.,  40.,  28., 130.,  97.,  10., 111.,  14.,  85.,  21.,\n",
      "        47.,   2.,  36.,  24.,  14., 100., 124.,  12.,  75.,  40., 109.,\n",
      "        79.,  81.,   1.,   3.,  33.,  34.,  29.,  91., 137.,   6.,  49.,\n",
      "         9.,  71.,  11.,  41.,   3.,   1.,  26.,  22.,  22., 113.,  82.,\n",
      "         5."
     ]
    },
    {
     "data": {
      "text/html": [
       "<b>limit_output extension: Maximum message size of 10000 exceeded with 11343 characters</b>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trd_dir = \"/Users/zac/Downloads/cleanInfo_trd_1/\"\n",
    "trd_fp = [os.path.join(trd_dir,i) for i in os.listdir(trd_dir)]\n",
    "\n",
    "features={\n",
    "    \"vl_volume\":tf.io.FixedLenFeature([], tf.string),\n",
    "    \"categoryIdSize\":tf.io.FixedLenFeature([], tf.int64),\n",
    "    \"categoryIds_indices\":tf.io.VarLenFeature(tf.int64),\n",
    "    \"categoryIds_values\":tf.io.VarLenFeature(tf.float32),\n",
    "}\n",
    "\n",
    "\n",
    "def _parse_func(inp):\n",
    "    parsed_features = tf.io.parse_single_example(inp,features)\n",
    "    return parsed_features\n",
    "\n",
    "trd=tf.data.TFRecordDataset(trd_fp).map(_parse_func)\n",
    "\n",
    "\n",
    "dataset = trd.batch(64,drop_remainder=True)\n",
    "line = dataset.make_one_shot_iterator().get_next()\n",
    "to_print = [\"categoryIdSize\",\"categoryIds_indices\",\"categoryIds_values\"]\n",
    "with tf.Session() as sess:\n",
    "    i = sess.run(line)\n",
    "    print(\">>> all keys: {}\\n\".format(\",\".join(i.keys())))\n",
    "    for key in to_print:\n",
    "        print(\">>> {}:\".format(key))\n",
    "        print(i[key])\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### column parse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-09T06:13:49.285994Z",
     "start_time": "2021-01-09T06:13:48.568790Z"
    },
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    },
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([Dimension(None), Dimension(6074)])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "TensorShape([Dimension(None), Dimension(6074)])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 1., 0.],\n",
       "       [0., 0., 0., ..., 0., 1., 0.],\n",
       "       [0., 0., 0., ..., 0., 1., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 1., 0.],\n",
       "       [0., 0., 0., ..., 0., 1., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "-2.5443935"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "array([[ 0.,  0.,  0., ...,  0., 13.,  0.],\n",
       "       [ 0.,  0.,  0., ...,  0., 37.,  0.],\n",
       "       [ 0.,  0.,  0., ...,  0., 10.,  0.],\n",
       "       ...,\n",
       "       [ 0.,  0.,  0., ...,  0., 23.,  0.],\n",
       "       [ 0.,  0.,  0., ...,  0., 18.,  0.],\n",
       "       [ 0.,  0.,  0., ...,  0., 29.,  0.]], dtype=float32)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "-48.176178"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oriFC = fc.categorical_column_with_identity(\"categoryIds_indices\", 6073+1)\n",
    "oriFC_idx = fc.indicator_column(oriFC)\n",
    "net0 = fc.input_layer(line,oriFC_idx)\n",
    "logits0 = tf.layers.dense(net0,1)\n",
    "net0.shape\n",
    "\n",
    "weightFC = fc.weighted_categorical_column(categorical_column=fc.categorical_column_with_identity(\"categoryIds_indices\", 6073+1),\n",
    "                                          weight_feature_key=\"categoryIds_values\")\n",
    "weightFC_idx = fc.indicator_column(weightFC)\n",
    "net1 = fc.input_layer(line,weightFC_idx)\n",
    "logits1 = tf.layers.dense(net1,1)\n",
    "net1.shape\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    net0.eval()\n",
    "    np.sum(logits0.eval())\n",
    "    net1.eval()\n",
    "    np.sum(logits1.eval())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## parse sparse-vector\n",
    "trd里存 featA_size, featA_indices, featA_values 解析出来再拼接成spvector？\n",
    "- 实际不如用带权Column\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## cross_column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### basic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-01T07:11:42.350223Z",
     "start_time": "2021-04-01T07:11:42.316392Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>featA</th>\n",
       "      <th>featB</th>\n",
       "      <th>featC</th>\n",
       "      <th>featD</th>\n",
       "      <th>featE</th>\n",
       "      <th>wideFeatures</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[2]</td>\n",
       "      <td>[0]</td>\n",
       "      <td>[1]</td>\n",
       "      <td>[0]</td>\n",
       "      <td>[6]</td>\n",
       "      <td>[58, 91, 88, 8]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[3]</td>\n",
       "      <td>[0]</td>\n",
       "      <td>[5]</td>\n",
       "      <td>[3]</td>\n",
       "      <td>[2]</td>\n",
       "      <td>[46, 61, 3, 7]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  featA featB featC featD featE     wideFeatures\n",
       "0   [2]   [0]   [1]   [0]   [6]  [58, 91, 88, 8]\n",
       "1   [3]   [0]   [5]   [3]   [2]   [46, 61, 3, 7]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_buckets_deep=7\n",
    "dims_embs_deep=4\n",
    "_batch_size=2\n",
    "\n",
    "# features\n",
    "features = {}\n",
    "features[\"featA\"] = np.random.randint(num_buckets_deep, size=(_batch_size, 1))\n",
    "features[\"featB\"] = np.random.randint(num_buckets_deep, size=(_batch_size, 1))\n",
    "features[\"featC\"] = np.random.randint(num_buckets_deep, size=(_batch_size, 1))\n",
    "features[\"featD\"] = np.random.randint(num_buckets_deep, size=(_batch_size, 1))\n",
    "features[\"featE\"] = np.random.randint(num_buckets_deep, size=(_batch_size, 1))\n",
    "features[\"wideFeatures\"] = np.random.randint(100, size=(_batch_size, 4))\n",
    "features\n",
    "pd.DataFrame({k:v.tolist() for k,v in features.items()})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-01T07:12:53.589542Z",
     "start_time": "2021-04-01T07:12:52.866160Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/luban/miniconda3/envs/zt/lib/python3.7/site-packages/tensorflow_core/python/feature_column/feature_column.py:206: IndicatorColumn._get_dense_tensor (from tensorflow.python.feature_column.feature_column_v2) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "The old _FeatureColumn APIs are being deprecated. Please use the new FeatureColumn APIs instead.\n",
      "WARNING:tensorflow:From /home/luban/miniconda3/envs/zt/lib/python3.7/site-packages/tensorflow_core/python/feature_column/feature_column.py:2158: IndicatorColumn._transform_feature (from tensorflow.python.feature_column.feature_column_v2) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "The old _FeatureColumn APIs are being deprecated. Please use the new FeatureColumn APIs instead.\n",
      "WARNING:tensorflow:From /home/luban/miniconda3/envs/zt/lib/python3.7/site-packages/tensorflow_core/python/feature_column/feature_column_v2.py:4300: IdentityCategoricalColumn._get_sparse_tensors (from tensorflow.python.feature_column.feature_column_v2) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "The old _FeatureColumn APIs are being deprecated. Please use the new FeatureColumn APIs instead.\n",
      "WARNING:tensorflow:From /home/luban/miniconda3/envs/zt/lib/python3.7/site-packages/tensorflow_core/python/feature_column/feature_column.py:2158: IdentityCategoricalColumn._transform_feature (from tensorflow.python.feature_column.feature_column_v2) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "The old _FeatureColumn APIs are being deprecated. Please use the new FeatureColumn APIs instead.\n",
      "WARNING:tensorflow:From /home/luban/miniconda3/envs/zt/lib/python3.7/site-packages/tensorflow_core/python/feature_column/feature_column_v2.py:4271: IndicatorColumn._variable_shape (from tensorflow.python.feature_column.feature_column_v2) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "The old _FeatureColumn APIs are being deprecated. Please use the new FeatureColumn APIs instead.\n",
      "WARNING:tensorflow:From /home/luban/miniconda3/envs/zt/lib/python3.7/site-packages/tensorflow_core/python/feature_column/feature_column_v2.py:4326: IdentityCategoricalColumn._num_buckets (from tensorflow.python.feature_column.feature_column_v2) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "The old _FeatureColumn APIs are being deprecated. Please use the new FeatureColumn APIs instead.\n",
      "WARNING:tensorflow:From /home/luban/miniconda3/envs/zt/lib/python3.7/site-packages/tensorflow_core/python/feature_column/feature_column_v2.py:4300: CrossedColumn._get_sparse_tensors (from tensorflow.python.feature_column.feature_column_v2) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "The old _FeatureColumn APIs are being deprecated. Please use the new FeatureColumn APIs instead.\n",
      "WARNING:tensorflow:From /home/luban/miniconda3/envs/zt/lib/python3.7/site-packages/tensorflow_core/python/feature_column/feature_column.py:2158: CrossedColumn._transform_feature (from tensorflow.python.feature_column.feature_column_v2) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "The old _FeatureColumn APIs are being deprecated. Please use the new FeatureColumn APIs instead.\n",
      "WARNING:tensorflow:From /home/luban/miniconda3/envs/zt/lib/python3.7/site-packages/tensorflow_core/python/feature_column/feature_column.py:206: EmbeddingColumn._get_dense_tensor (from tensorflow.python.feature_column.feature_column_v2) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "The old _FeatureColumn APIs are being deprecated. Please use the new FeatureColumn APIs instead.\n",
      "WARNING:tensorflow:From /home/luban/miniconda3/envs/zt/lib/python3.7/site-packages/tensorflow_core/python/ops/embedding_ops.py:802: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:From /home/luban/miniconda3/envs/zt/lib/python3.7/site-packages/tensorflow_core/python/feature_column/feature_column.py:207: EmbeddingColumn._variable_shape (from tensorflow.python.feature_column.feature_column_v2) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "The old _FeatureColumn APIs are being deprecated. Please use the new FeatureColumn APIs instead.\n",
      ">>> featA_indicator:\n",
      "[[0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0.]]\n",
      ">>> featB_indicator:\n",
      "[[1. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0.]]\n",
      ">>> featC_indicator:\n",
      "[[0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0.]]\n",
      ">>> featA_X_featB_indicator:\n",
      "[[0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      ">>> featA_X_featB_embedding:\n",
      "[[ 0.1938113   0.90250343 -0.14478655  0.33468357]\n",
      " [-0.7732629  -0.28058475 -0.0794202   0.37073213]]\n",
      ">>> featA_X_featB_X_featC_indicator:\n",
      "[[0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]]\n",
      ">>> featA_X_featB_X_featC_embedding:\n",
      "[[ 0.10227173  0.57872885 -0.2891431   0.24426556]\n",
      " [-0.60909116  0.74897605  0.09729668 -0.6149847 ]]\n",
      ">>> wideFeatures_indicator:\n",
      "[[0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0.]]\n",
      ">>> featA_X_featB_shared_embedding:\n",
      "[[ 0.8310542  -0.45170856  0.2920446  -0.31778577]\n",
      " [-0.3894919  -0.28315765  0.04518253  0.45651633]]\n",
      ">>> featA_X_featB_X_featC_shared_embedding:\n",
      "[[ 0.12313633  0.2953795   0.33781737  0.47496468]\n",
      " [ 0.01616773 -0.10966956  0.5724055   0.02429409]]\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "# codec column\n",
    "fc_wide = fc.categorical_column_with_identity(\"wideFeatures\", 100)\n",
    "fc_idx_wide = fc.indicator_column(fc_wide)\n",
    "\n",
    "featA_cat_column = fc.categorical_column_with_identity(\"featA\", num_buckets_deep, 0)\n",
    "featA_idx_column = fc.indicator_column(featA_cat_column)\n",
    "\n",
    "featB_cat_column = fc.categorical_column_with_identity(\"featB\", num_buckets_deep, 0)\n",
    "featB_idx_column = fc.indicator_column(featB_cat_column)\n",
    "\n",
    "featC_cat_column = fc.categorical_column_with_identity(\"featC\", num_buckets_deep, 0)\n",
    "featC_idx_column = fc.indicator_column(featC_cat_column)\n",
    "\n",
    "featAxB_cat_column = fc.crossed_column([\"featA\",\"featB\"], hash_bucket_size=10)\n",
    "featAxB_idx_column = fc.indicator_column(featAxB_cat_column)\n",
    "featAxB_emb_column = fc.embedding_column(featAxB_cat_column, dimension=dims_embs_deep)\n",
    "\n",
    "featAxBxC_cat_column = fc.crossed_column([\"featA\",\"featB\",\"featC\"], hash_bucket_size=10)\n",
    "featAxBxC_idx_column = fc.indicator_column(featAxBxC_cat_column)\n",
    "featAxBxC_emb_column = fc.embedding_column(featAxBxC_cat_column, dimension=dims_embs_deep)\n",
    "\n",
    "shaerd_emb_columns = fc.shared_embedding_columns([featAxB_cat_column, featAxBxC_cat_column], dimension=dims_embs_deep)\n",
    "\n",
    "to_be_input = [featA_idx_column,\n",
    "               featB_idx_column,\n",
    "               featC_idx_column,\n",
    "               featAxB_idx_column,\n",
    "               featAxB_emb_column,\n",
    "               featAxBxC_idx_column,\n",
    "               featAxBxC_emb_column,\n",
    "               fc_idx_wide]+shaerd_emb_columns\n",
    "inp_layer_list = [fc.input_layer(features,i) for i in to_be_input]\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer()) \n",
    "    inp_list = sess.run(inp_layer_list)\n",
    "    for k,v in zip([i.name for i in to_be_input], inp_list):\n",
    "        print(\">>> {}:\\n{}\".format(k,v))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 如何直接把cross加到wideFeatures里(list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-09T07:56:07.879683Z",
     "start_time": "2020-12-09T07:56:07.755798Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_IdentityCategoricalColumn(key='wideFeatures', num_buckets=100, default_value=None)"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "_CrossedColumn(keys=('featA', 'featB'), hash_bucket_size=10, hash_key=None)"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "[_IdentityCategoricalColumn(key='wideFeatures', num_buckets=100, default_value=None),\n",
       " _CrossedColumn(keys=('featA', 'featB'), hash_bucket_size=10, hash_key=None)]"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "_IndicatorColumn(categorical_column=[_IdentityCategoricalColumn(key='wideFeatures', num_buckets=100, default_value=None), _CrossedColumn(keys=('featA', 'featB'), hash_bucket_size=10, hash_key=None)])"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'input_layer_15/concat:0' shape=(2, 110) dtype=float32>"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fc_wide\n",
    "featAxB_cat_column\n",
    "fc_inp = [fc_wide, featAxB_cat_column]\n",
    "fc_inp\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    # 注意在LinearModel里传给它的featureColumn用fc_inp就行了，它内部自行做了indicator_column\n",
    "    fc.input_layer(features, [fc.indicator_column(i) for i in fc_inp])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interact emb by Add | fake Positional-Emb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### basic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-28T16:15:51.957830Z",
     "start_time": "2020-12-28T16:15:51.892950Z"
    },
    "run_control": {
     "marked": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'featA': array([[ 6],\n",
      "       [10]]), 'pickupHash': array([[ 9],\n",
      "       [17]]), 'time_idx': array([[2],\n",
      "       [5]]), 'numericB': [[0.5], [0.2]], 'numericA': [[0.5], [1.2]], 'link': [[-1], [-1]], 'hour_idx': array([[12],\n",
      "       [ 5]]), 'whateverB': array([[10],\n",
      "       [ 6]]), 'whateverA': array([[8],\n",
      "       [9]]), 'featB': array([[8],\n",
      "       [5]])}\n"
     ]
    }
   ],
   "source": [
    "##############\n",
    "# h-params\n",
    "##############\n",
    "num_buckets_deep=20\n",
    "dims_embs_deep=4\n",
    "_batch_size=2\n",
    "\n",
    "##############\n",
    "# features\n",
    "##############\n",
    "features = {}\n",
    "features[\"featA\"] = np.random.randint(num_buckets_deep, size=(_batch_size, 1))\n",
    "features[\"featB\"] = np.random.randint(num_buckets_deep, size=(_batch_size, 1))\n",
    "features[\"link\"] = [[-1],[-1]]\n",
    "features[\"pickupHash\"] = np.random.randint(num_buckets_deep, size=(_batch_size, 1))\n",
    "features[\"hour_idx\"] = np.random.randint(num_buckets_deep, size=(_batch_size, 1))\n",
    "features[\"time_idx\"] = np.random.randint(num_buckets_deep, size=(_batch_size, 1))\n",
    "features[\"whateverA\"] = np.random.randint(num_buckets_deep, size=(_batch_size, 1))\n",
    "features[\"whateverB\"] = np.random.randint(num_buckets_deep, size=(_batch_size, 1))\n",
    "features[\"numericA\"] = [[0.5],[1.2]]\n",
    "features[\"numericB\"] = [[0.5],[0.2]]\n",
    "print(features)\n",
    "\n",
    "################\n",
    "# build columns\n",
    "################\n",
    "tf.reset_default_graph()\n",
    "# numeric column\n",
    "fc_numeric = list()\n",
    "fc_numeric.append(fc.numeric_column(\"numericA\", default_value=0))\n",
    "fc_numeric.append(fc.numeric_column(\"numericB\", default_value=0))\n",
    "\n",
    "# codec column\n",
    "emb_feat_list = list()\n",
    "for i in [\"featA\",\"featB\",\"link\",\"pickupHash\",\"hour_idx\",\"time_idx\"]:\n",
    "    emb_feat_list.append(fc.categorical_column_with_identity(i, num_buckets_deep, default_value=0))\n",
    "\n",
    "# emb column\n",
    "fc_shared_emb = fc.shared_embedding_columns(\n",
    "                    emb_feat_list,\n",
    "                    dimension=dims_embs_deep, max_norm=np.sqrt(dims_embs_deep),\n",
    "                    trainable=True)\n",
    "\n",
    "# shared_emb 转成KV结构方便后面操作\n",
    "fc_emb_dict = {seCol.categorical_column.key: seCol for seCol in fc_shared_emb}\n",
    "\n",
    "# 指出将要相加的emb\n",
    "emb_to_add=[['link','hour_idx'],\n",
    "            ['pickupHash','time_idx']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 说明一下获取column的名字"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-28T16:28:39.239804Z",
     "start_time": "2020-12-28T16:28:39.221162Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'featA_shared_embedding'"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "'featA'"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "'numericA'"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fc_emb_dict['featA'].name\n",
    "fc_emb_dict['featA'].categorical_column.key\n",
    "fc_numeric[0].name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### input_layer搞事情"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-18T07:27:51.079465Z",
     "start_time": "2020-11-18T07:27:48.288221Z"
    }
   },
   "outputs": [],
   "source": [
    "emb_added = []\n",
    "for f1,f2 in emb_to_add:\n",
    "    net1 = fc.input_layer(features, fc_emb_dict[f1])\n",
    "    net2 = fc.input_layer(features, fc_emb_dict[f2])\n",
    "    emb_added.append(net1+net2)\n",
    "\n",
    "emb_to_add_flatten = [i for sub in emb_to_add for i in sub]\n",
    "net_ori = fc.input_layer(features, [v for k,v in fc_emb_dict.items() if k not in emb_to_add_flatten])\n",
    "\n",
    "net = tf.concat([net_ori]+emb_added,axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### run起来看看结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-18T07:28:29.698347Z",
     "start_time": "2020-11-18T07:28:27.741264Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('net_ori',\n",
       " array([[-0.03376518, -0.75637287,  0.01980731,  0.57804877, -0.41253054,\n",
       "          0.76133245,  0.21623057,  0.34377784],\n",
       "        [-0.03376518, -0.75637287,  0.01980731,  0.57804877,  0.5149282 ,\n",
       "         -0.06459129, -0.2458921 , -0.7198298 ]], dtype=float32))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "('emb_added', [array([[-0.2679743 , -0.51504064,  0.3350453 ,  0.47310346],\n",
       "         [ 0.5149282 , -0.06459129, -0.2458921 , -0.7198298 ]],\n",
       "        dtype=float32),\n",
       "  array([[-0.02274224,  1.3028573 , -0.6851692 ,  0.33623815],\n",
       "         [ 0.28992552,  1.1738838 ,  0.27457008,  0.5552162 ]],\n",
       "        dtype=float32)])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "('net',\n",
       " array([[-0.03376518, -0.75637287,  0.01980731,  0.57804877, -0.41253054,\n",
       "          0.76133245,  0.21623057,  0.34377784, -0.2679743 , -0.51504064,\n",
       "          0.3350453 ,  0.47310346, -0.02274224,  1.3028573 , -0.6851692 ,\n",
       "          0.33623815],\n",
       "        [-0.03376518, -0.75637287,  0.01980731,  0.57804877,  0.5149282 ,\n",
       "         -0.06459129, -0.2458921 , -0.7198298 ,  0.5149282 , -0.06459129,\n",
       "         -0.2458921 , -0.7198298 ,  0.28992552,  1.1738838 ,  0.27457008,\n",
       "          0.5552162 ]], dtype=float32))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> details of emb_added\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('link', array([[0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.]], dtype=float32))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "('hour_idx', array([[-0.2679743 , -0.51504064,  0.3350453 ,  0.47310346],\n",
       "        [ 0.5149282 , -0.06459129, -0.2458921 , -0.7198298 ]],\n",
       "       dtype=float32))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "('pickupHash', array([[-0.5041411 ,  0.9702361 , -0.7937973 , -0.33923268],\n",
       "        [ 0.70245606,  0.41255134,  0.05833951,  0.21143836]],\n",
       "       dtype=float32))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "('time_idx', array([[ 0.48139885,  0.33262116,  0.10862812,  0.6754708 ],\n",
       "        [-0.41253054,  0.76133245,  0.21623057,  0.34377784]],\n",
       "       dtype=float32))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \"net_ori\",sess.run(net_ori)\n",
    "    \"emb_added\",sess.run(emb_added)\n",
    "    \"net\",sess.run(net)\n",
    "    print(\">>> details of emb_added\")\n",
    "    for f1,f2 in emb_to_add:\n",
    "        net1 = fc.input_layer(features, fc_emb_dict[f1])\n",
    "        net2 = fc.input_layer(features, fc_emb_dict[f2])\n",
    "        f1,sess.run(net1)\n",
    "        f2,sess.run(net2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interact emb by Inner-product"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### basic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-07T13:18:24.328638Z",
     "start_time": "2020-12-07T13:18:24.298252Z"
    },
    "run_control": {
     "marked": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "featA\n",
      "[[3]\n",
      " [4]]\n",
      "pickupHash\n",
      "[[6]\n",
      " [2]]\n",
      "featB\n",
      "[[6]\n",
      " [1]]\n",
      "link\n",
      "[[-1], [2]]\n",
      "featC\n",
      "[[5]\n",
      " [2]]\n",
      "hour_idx\n",
      "[[5]\n",
      " [3]]\n",
      "time_idx\n",
      "[[0]\n",
      " [1]]\n"
     ]
    }
   ],
   "source": [
    "num_buckets_deep=7\n",
    "dims_embs_deep=3\n",
    "_batch_size=2\n",
    "\n",
    "# features\n",
    "features = {}\n",
    "features[\"featA\"] = np.random.randint(num_buckets_deep, size=(_batch_size, 1))\n",
    "features[\"featB\"] = np.random.randint(num_buckets_deep, size=(_batch_size, 1))\n",
    "features[\"featC\"] = np.random.randint(num_buckets_deep, size=(_batch_size, 1))\n",
    "features[\"link\"] = [[-1],[2]]\n",
    "assert(len(features[\"link\"]) == _batch_size)\n",
    "features[\"pickupHash\"] = np.random.randint(num_buckets_deep, size=(_batch_size, 1))\n",
    "features[\"hour_idx\"] = np.random.randint(num_buckets_deep, size=(_batch_size, 1))\n",
    "features[\"time_idx\"] = np.random.randint(num_buckets_deep, size=(_batch_size, 1))\n",
    "\n",
    "for k,v in features.items():\n",
    "    print(k)\n",
    "    print(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-07T16:01:46.213898Z",
     "start_time": "2020-12-07T16:01:46.192106Z"
    },
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "# numeric column\n",
    "fc_numeric = list()\n",
    "fc_numeric.append(fc.numeric_column(\"numericA\", default_value=0))\n",
    "fc_numeric.append(fc.numeric_column(\"numericB\", default_value=0))\n",
    "\n",
    "# codec column\n",
    "emb_feat_list = list()\n",
    "for i in [\"featA\",\"featB\",\"featC\",\"link\",\"pickupHash\",\"hour_idx\",\"time_idx\"]:\n",
    "    emb_feat_list.append(fc.categorical_column_with_identity(i, num_buckets_deep, default_value=0))\n",
    "\n",
    "# emb column\n",
    "fc_shared_emb = fc.shared_embedding_columns(\n",
    "                    emb_feat_list,\n",
    "                    dimension=dims_embs_deep, max_norm=np.sqrt(dims_embs_deep),\n",
    "                    trainable=True)\n",
    "\n",
    "# shared_emb 转成KV结构方便后面操作\n",
    "fc_emb_dict = {seCol.categorical_column.key: seCol for seCol in fc_shared_emb}\n",
    "\n",
    "# 指出将要相加的emb\n",
    "emb_to_add=[['link','hour_idx'],\n",
    "            ['pickupHash','time_idx'],\n",
    "            ['featA','featB']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-07T16:07:24.659568Z",
     "start_time": "2020-12-07T16:07:22.703016Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(u'link_shared_embedding_hour_idx_shared_embedding_3:0',\n",
       " array([ 0.        , -0.05191137], dtype=float32))"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(u'pickupHash_shared_embedding_time_idx_shared_embedding_3:0',\n",
       " array([0.05215313, 0.04003584], dtype=float32))"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(u'featA_shared_embedding_featB_shared_embedding_3:0',\n",
       " array([-0.04963669, -0.01114402], dtype=float32))"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "array([[ 0.        ,  0.05215313, -0.04963669],\n",
       "       [-0.05191137,  0.04003584, -0.01114402]], dtype=float32)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "array([[ 0.3013491 ,  0.99670464,  0.9063871 ],\n",
       "       [-0.3013491 , -0.99670464, -0.9063875 ]], dtype=float32)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fc_cross_2Dlist=list()\n",
    "for emb_list in emb_to_add:\n",
    "    fc_cross_2Dlist.append([fc_emb_dict[i] for i in emb_list])\n",
    "\n",
    "cross_logits = list()\n",
    "for fc_cross_list in fc_cross_2Dlist:\n",
    "    assert len(fc_cross_list) == 2\n",
    "    fc0, fc1 = fc_cross_list\n",
    "    net0 = fc.input_layer(features, fc0)\n",
    "    net1 = fc.input_layer(features, fc1)\n",
    "    cross_logits.append(tf.reduce_sum(net0*net1, axis=1, name=\"%s_%s\" % (fc0.name,fc1.name)))\n",
    "    \n",
    "# 直接把点积加起来作为interaction_logits | 不行，logits是四分类，四维的\n",
    "all_emb_dot = tf.stack(cross_logits, axis=1)\n",
    "all_emb_dot_bn = tf.contrib.layers.batch_norm(all_emb_dot, is_training=True, updates_collections=None)\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for i in cross_logits:\n",
    "        i.name,i.eval()\n",
    "    all_emb_dot_bn.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-07T15:49:39.153660Z",
     "start_time": "2020-12-07T15:49:38.201997Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([Dimension(2), Dimension(3)])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "array([[ 0.6932142 ,  0.49308798,  0.02976007],\n",
       "       [-0.5361837 , -0.19622478,  0.5870663 ]], dtype=float32)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "array([[-0.5361837 , -0.19622478,  0.5870663 ],\n",
       "       [-0.51281106,  0.42062   , -0.9718037 ]], dtype=float32)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "-0.37169015"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "-0.09675608"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "array([[-0.37169015, -0.09675608,  0.01747113],\n",
       "       [ 0.27496094, -0.08253606, -0.5705132 ]], dtype=float32)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "array([-0.4509751 , -0.37808833], dtype=float32)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fc.input_layer(features,fc_emb_dict[\"hour_idx\"]).shape\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    fc0=fc.input_layer(features,fc_emb_dict[\"hour_idx\"])\n",
    "    fc1=fc.input_layer(features,fc_emb_dict[\"featA\"])\n",
    "    fc0.eval()\n",
    "    fc1.eval()\n",
    "    fc0.eval()[0][0] * fc1.eval()[0][0]\n",
    "    fc0.eval()[0][1] * fc1.eval()[0][1]\n",
    "    (fc0*fc1).eval()\n",
    "    tf.reduce_sum(fc0*fc1, axis=1).eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### input_layer操作起来\n",
    "\n",
    "format by: [[\"hour_idx\",\"featA\"], [\"hour_idx\",\"featB\"], [\"hour_idx\", \"featC\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-07T13:18:27.298142Z",
     "start_time": "2020-12-07T13:18:26.022675Z"
    },
    "run_control": {
     "marked": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> last-one as demo\n",
      "pickupHash\n",
      "[[-0.4284786  -0.26747006 -0.17833763]\n",
      " [ 0.16898142  0.38039657 -0.26833487]]\n",
      "hour_idx\n",
      "[[-0.7861729   0.2811521  -0.30317506]\n",
      " [ 0.6207778   0.12892464  0.10991306]]\n",
      "_res | f1 f2做内积得到\n",
      "[[ 0.33685827 -0.07519977  0.05406752]\n",
      " [ 0.10489991  0.04904249 -0.02949351]]\n",
      "res | _res做reduce_sum得到\n",
      "[0.315726  0.1244489]\n",
      ">>> emb_dot\n",
      "[array([0.       , 0.1244489], dtype=float32), array([0.315726 , 0.1244489], dtype=float32)]\n",
      ">>> fm_2nd_res\n",
      "[0.315726  0.2488978]\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "emb_to_innerProduct = [\"link\",\"pickupHash\"]\n",
    "emb_to_innerProduct = [[i,\"hour_idx\"] for i in emb_to_innerProduct]\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    emb_dot = []\n",
    "    for f1,f2 in emb_to_innerProduct:\n",
    "        net1 = fc.input_layer(features, fc_emb_dict[f1])\n",
    "        net2 = fc.input_layer(features, fc_emb_dict[f2])\n",
    "        _res = net1 * net2\n",
    "        res = tf.reduce_sum(net1*net2, axis=1)\n",
    "        emb_dot.append(res)\n",
    "\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    print(\">>> last-one as demo\")\n",
    "    print(f1)\n",
    "    print(sess.run(net1))\n",
    "    print(f2)\n",
    "    print(sess.run(net2))\n",
    "    print(\"_res | f1 f2做内积得到\")\n",
    "    print(sess.run(_res))\n",
    "    print(\"res | _res做reduce_sum得到\")\n",
    "    print(sess.run(res))\n",
    "    \n",
    "    \n",
    "    print(\">>> emb_dot\")\n",
    "    print(sess.run(emb_dot))\n",
    "    fm_2nd_res = tf.reduce_sum(emb_dot, axis=0)\n",
    "    print(\">>> fm_2nd_res\")\n",
    "    print(sess.run(fm_2nd_res))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-07T13:19:29.896803Z",
     "start_time": "2020-12-07T13:19:29.871174Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Variable 'abc/Variable:0' shape=(1,) dtype=float32_ref>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<tf.Variable 'bbc/Variable:0' shape=(1,) dtype=float32_ref>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with tf.variable_scope(\"abc\"):\n",
    "    tf.Variable([1.0],\"m\")\n",
    "    \n",
    "with tf.variable_scope(\"bbc\"):\n",
    "    tf.Variable([1.0],\"k\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-07T13:21:22.634414Z",
     "start_time": "2020-12-07T13:21:22.267800Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Variable 'abc/Variable:0' shape=(1,) dtype=float32_ref>,\n",
       " <tf.Variable 'bbc/Variable:0' shape=(1,) dtype=float32_ref>]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "ename": "TypeError",
     "evalue": "unhashable type: 'list'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-c853cde073ae>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_collection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"trainable_variables\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscope\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"abc\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_collection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"trainable_variables\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscope\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"bbc\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_collection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"trainable_variables\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscope\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"abc\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"bbc\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp-data/luban/anaconda3/envs/tf1.12/lib/python2.7/site-packages/tensorflow/python/framework/ops.pyc\u001b[0m in \u001b[0;36mget_collection\u001b[0;34m(key, scope)\u001b[0m\n\u001b[1;32m   5925\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mend_compatibility\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5926\u001b[0m   \"\"\"\n\u001b[0;32m-> 5927\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mget_default_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_collection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5928\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5929\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp-data/luban/anaconda3/envs/tf1.12/lib/python2.7/site-packages/tensorflow/python/framework/ops.pyc\u001b[0m in \u001b[0;36mget_collection\u001b[0;34m(self, name, scope)\u001b[0m\n\u001b[1;32m   3845\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3846\u001b[0m         \u001b[0mc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3847\u001b[0;31m         \u001b[0mregex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3848\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcollection\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3849\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"name\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mregex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp-data/luban/anaconda3/envs/tf1.12/lib/python2.7/re.pyc\u001b[0m in \u001b[0;36mcompile\u001b[0;34m(pattern, flags)\u001b[0m\n\u001b[1;32m    192\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpattern\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    193\u001b[0m     \u001b[0;34m\"Compile a regular expression pattern, returning a pattern object.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 194\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_compile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpattern\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    195\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mpurge\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp-data/luban/anaconda3/envs/tf1.12/lib/python2.7/re.pyc\u001b[0m in \u001b[0;36m_compile\u001b[0;34m(*key)\u001b[0m\n\u001b[1;32m    235\u001b[0m         \u001b[0mcachekey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 237\u001b[0;31m             \u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_cache\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcachekey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    238\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mloc\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mloc\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_locale\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetlocale\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_locale\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLC_CTYPE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    239\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: unhashable type: 'list'"
     ]
    }
   ],
   "source": [
    "tf.get_collection(\"trainable_variables\", scope=\"abc\") + tf.get_collection(\"trainable_variables\", scope=\"bbc\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### input_layer操作起来 | 字典形式\n",
    "\n",
    "format by: {\"hour_idx\": [\"featA\", \"featB\", \"featC\"], \"time_idx\": [\"featA\", \"featB\", \"featC\"]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-19T08:28:52.679488Z",
     "start_time": "2020-11-19T08:28:52.668988Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function batch_norm in module tensorflow.contrib.layers.python.layers.layers:\n",
      "\n",
      "batch_norm(*args, **kwargs)\n",
      "    Adds a Batch Normalization layer from http://arxiv.org/abs/1502.03167.\n",
      "    \n",
      "      \"Batch Normalization: Accelerating Deep Network Training by Reducing\n",
      "      Internal Covariate Shift\"\n",
      "    \n",
      "      Sergey Ioffe, Christian Szegedy\n",
      "    \n",
      "    Can be used as a normalizer function for conv2d and fully_connected. The\n",
      "    normalization is over all but the last dimension if `data_format` is `NHWC`\n",
      "    and all but the second dimension if `data_format` is `NCHW`.  In case of a 2D\n",
      "    tensor this corresponds to the batch dimension, while in case of a 4D tensor\n",
      "    this\n",
      "    corresponds to the batch and space dimensions.\n",
      "    \n",
      "    Note: when training, the moving_mean and moving_variance need to be updated.\n",
      "    By default the update ops are placed in `tf.GraphKeys.UPDATE_OPS`, so they\n",
      "    need to be added as a dependency to the `train_op`. For example:\n",
      "    \n",
      "    ```python\n",
      "      update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
      "      with tf.control_dependencies(update_ops):\n",
      "        train_op = optimizer.minimize(loss)\n",
      "    ```\n",
      "    \n",
      "    One can set updates_collections=None to force the updates in place, but that\n",
      "    can have a speed penalty, especially in distributed settings.\n",
      "    \n",
      "    Args:\n",
      "      inputs: A tensor with 2 or more dimensions, where the first dimension has\n",
      "        `batch_size`. The normalization is over all but the last dimension if\n",
      "        `data_format` is `NHWC` and the second dimension if `data_format` is\n",
      "        `NCHW`.\n",
      "      decay: Decay for the moving average. Reasonable values for `decay` are close\n",
      "        to 1.0, typically in the multiple-nines range: 0.999, 0.99, 0.9, etc.\n",
      "        Lower `decay` value (recommend trying `decay`=0.9) if model experiences\n",
      "        reasonably good training performance but poor validation and/or test\n",
      "        performance. Try zero_debias_moving_mean=True for improved stability.\n",
      "      center: If True, add offset of `beta` to normalized tensor. If False, `beta`\n",
      "        is ignored.\n",
      "      scale: If True, multiply by `gamma`. If False, `gamma` is\n",
      "        not used. When the next layer is linear (also e.g. `nn.relu`), this can be\n",
      "        disabled since the scaling can be done by the next layer.\n",
      "      epsilon: Small float added to variance to avoid dividing by zero.\n",
      "      activation_fn: Activation function, default set to None to skip it and\n",
      "        maintain a linear activation.\n",
      "      param_initializers: Optional initializers for beta, gamma, moving mean and\n",
      "        moving variance.\n",
      "      param_regularizers: Optional regularizer for beta and gamma.\n",
      "      updates_collections: Collections to collect the update ops for computation.\n",
      "        The updates_ops need to be executed with the train_op.\n",
      "        If None, a control dependency would be added to make sure the updates are\n",
      "        computed in place.\n",
      "      is_training: Whether or not the layer is in training mode. In training mode\n",
      "        it would accumulate the statistics of the moments into `moving_mean` and\n",
      "        `moving_variance` using an exponential moving average with the given\n",
      "        `decay`. When it is not in training mode then it would use the values of\n",
      "        the `moving_mean` and the `moving_variance`.\n",
      "      reuse: Whether or not the layer and its variables should be reused. To be\n",
      "        able to reuse the layer scope must be given.\n",
      "      variables_collections: Optional collections for the variables.\n",
      "      outputs_collections: Collections to add the outputs.\n",
      "      trainable: If `True` also add variables to the graph collection\n",
      "        `GraphKeys.TRAINABLE_VARIABLES` (see `tf.Variable`).\n",
      "      batch_weights: An optional tensor of shape `[batch_size]`,\n",
      "        containing a frequency weight for each batch item. If present,\n",
      "        then the batch normalization uses weighted mean and\n",
      "        variance. (This can be used to correct for bias in training\n",
      "        example selection.)\n",
      "      fused: if `None` or `True`, use a faster, fused implementation if possible.\n",
      "        If `False`, use the system recommended implementation.\n",
      "      data_format: A string. `NHWC` (default) and `NCHW` are supported.\n",
      "      zero_debias_moving_mean: Use zero_debias for moving_mean. It creates a new\n",
      "        pair of variables 'moving_mean/biased' and 'moving_mean/local_step'.\n",
      "      scope: Optional scope for `variable_scope`.\n",
      "      renorm: Whether to use Batch Renormalization\n",
      "        (https://arxiv.org/abs/1702.03275). This adds extra variables during\n",
      "        training. The inference is the same for either value of this parameter.\n",
      "      renorm_clipping: A dictionary that may map keys 'rmax', 'rmin', 'dmax' to\n",
      "        scalar `Tensors` used to clip the renorm correction. The correction\n",
      "        `(r, d)` is used as `corrected_value = normalized_value * r + d`, with\n",
      "        `r` clipped to [rmin, rmax], and `d` to [-dmax, dmax]. Missing rmax, rmin,\n",
      "        dmax are set to inf, 0, inf, respectively.\n",
      "      renorm_decay: Momentum used to update the moving means and standard\n",
      "        deviations with renorm. Unlike `momentum`, this affects training\n",
      "        and should be neither too small (which would add noise) nor too large\n",
      "        (which would give stale estimates). Note that `decay` is still applied\n",
      "        to get the means and variances for inference.\n",
      "      adjustment: A function taking the `Tensor` containing the (dynamic) shape of\n",
      "        the input tensor and returning a pair (scale, bias) to apply to the\n",
      "        normalized values (before gamma and beta), only during training. For\n",
      "        example,\n",
      "          `adjustment = lambda shape: (\n",
      "            tf.random_uniform(shape[-1:], 0.93, 1.07),\n",
      "            tf.random_uniform(shape[-1:], -0.1, 0.1))`\n",
      "        will scale the normalized value by up to 7% up or down, then shift the\n",
      "        result by up to 0.1 (with independent scaling and bias for each feature\n",
      "        but shared across all examples), and finally apply gamma and/or beta. If\n",
      "        `None`, no adjustment is applied.\n",
      "    \n",
      "    Returns:\n",
      "      A `Tensor` representing the output of the operation.\n",
      "    \n",
      "    Raises:\n",
      "      ValueError: If `data_format` is neither `NHWC` nor `NCHW`.\n",
      "      ValueError: If the rank of `inputs` is undefined.\n",
      "      ValueError: If rank or channels dimension of `inputs` is undefined.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(tf.contrib.layers.batch_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-20T04:46:43.914529Z",
     "start_time": "2020-11-20T04:46:42.347242Z"
    }
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "from tensorflow.python.keras import regularizers\n",
    "from tensorflow.python.ops import init_ops\n",
    "emb_to_innerProduct = {\"hour_idx\":[\"featA\",\"featB\",\"featC\",\"link\",\"pickupHash\"],\n",
    "                       \"time_idx\":[\"featA\",\"featB\",\"featC\",\"link\",\"pickupHash\"],}\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    emb_dot = []\n",
    "    for k, v in emb_to_innerProduct.items():\n",
    "        # k: f1  v: [featA,featB,featC]\n",
    "        # cross: f1*featA + f1*featB + f1*featC \n",
    "        emb1 = fc.input_layer(features, fc_emb_dict[k])\n",
    "        emb2 = fc.input_layer(features, [fc_emb_dict[i] for i in v])\n",
    "        emb1_r = tf.expand_dims(emb1, -1)\n",
    "        emb2_r = tf.reshape(emb2,[-1, len(v), emb1.shape[-1]])\n",
    "        res = tf.squeeze(tf.matmul(emb2_r, emb1_r))\n",
    "        emb_dot.append(res)\n",
    "    all_emb_dot = tf.concat(emb_dot, axis=1)\n",
    "#     fm_2nd_res = tf.reduce_sum(tf.concat(emb_dot, axis=1), axis=1)\n",
    "#     fm_2nd_res = tf.reshape(fm_2nd_res, [-1,1])\n",
    "    \n",
    "    logits = tf.layers.dense(all_emb_dot, 4, kernel_initializer=init_ops.glorot_uniform_initializer(),\n",
    "                        activation=None, kernel_regularizer=regularizers.l2(0.01),\n",
    "                        name=\"logits\")\n",
    "\n",
    "    all_emb_dot_bn = tf.contrib.layers.batch_norm(all_emb_dot,is_training=False,updates_collections=None)\n",
    "    logits_bn = tf.layers.dense(all_emb_dot_bn, 4, kernel_initializer=init_ops.glorot_uniform_initializer(),\n",
    "                        activation=None, kernel_regularizer=regularizers.l2(0.01),\n",
    "                        name=\"logits_bn\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-20T04:48:00.380741Z",
     "start_time": "2020-11-20T04:47:59.599829Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'MatMul_5:0' shape=(2, 5, 1) dtype=float32>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'Squeeze_1:0' shape=(2, 5) dtype=float32>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "array([[[-0.4614984 ],\n",
       "        [-0.13459314],\n",
       "        [ 0.8183997 ],\n",
       "        [ 0.        ],\n",
       "        [-0.1535201 ]],\n",
       "\n",
       "       [[-0.03328903],\n",
       "        [ 0.82868665],\n",
       "        [ 0.55066764],\n",
       "        [ 0.55066764],\n",
       "        [ 0.55066764]]], dtype=float32)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "array([[-0.4614984 , -0.13459314,  0.8183997 ,  0.        , -0.1535201 ],\n",
       "       [-0.03328903,  0.82868665,  0.55066764,  0.55066764,  0.55066764]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    tf.matmul(emb2_r, emb1_r)\n",
    "    res\n",
    "    sess.run(tf.matmul(emb2_r, emb1_r))\n",
    "    sess.run(tf.reshape(tf.matmul(emb2_r, emb1_r),[-1, tf.matmul(emb2_r, emb1_r).shape[1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-19T08:34:04.898297Z",
     "start_time": "2020-11-19T08:34:00.537240Z"
    },
    "run_control": {
     "marked": true
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time_idx\n",
      "['featA', 'featB', 'featC', 'link', 'pickupHash']\n",
      "emb1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 0.41871086,  0.52999526,  0.4945093 ],\n",
       "       [-0.629783  , -0.06730537, -0.3083893 ]], dtype=float32)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "emb2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 0.00486276,  0.46456635,  0.63068444, -0.629783  , -0.06730537,\n",
       "        -0.3083893 ,  0.79900736, -0.19154112, -0.7691954 ,  0.        ,\n",
       "         0.        ,  0.        ,  0.26811442, -0.47049198, -0.89255047],\n",
       "       [ 0.26811442, -0.47049198, -0.89255047, -0.629783  , -0.06730537,\n",
       "        -0.3083893 , -0.27455962,  0.29733664, -0.7572018 , -0.27455962,\n",
       "         0.29733664, -0.7572018 , -0.27455962,  0.29733664, -0.7572018 ]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "emb1_r\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[[ 0.41871086],\n",
       "        [ 0.52999526],\n",
       "        [ 0.4945093 ]],\n",
       "\n",
       "       [[-0.629783  ],\n",
       "        [-0.06730537],\n",
       "        [-0.3083893 ]]], dtype=float32)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "emb2_r\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[[ 0.00486276,  0.46456635,  0.63068444],\n",
       "        [-0.629783  , -0.06730537, -0.3083893 ],\n",
       "        [ 0.79900736, -0.19154112, -0.7691954 ],\n",
       "        [ 0.        ,  0.        ,  0.        ],\n",
       "        [ 0.26811442, -0.47049198, -0.89255047]],\n",
       "\n",
       "       [[ 0.26811442, -0.47049198, -0.89255047],\n",
       "        [-0.629783  , -0.06730537, -0.3083893 ],\n",
       "        [-0.27455962,  0.29733664, -0.7572018 ],\n",
       "        [-0.27455962,  0.29733664, -0.7572018 ],\n",
       "        [-0.27455962,  0.29733664, -0.7572018 ]]], dtype=float32)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "res\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 0.5601334 , -0.45186988, -0.14733711,  0.        , -0.57847065],\n",
       "       [ 0.13806576,  0.49626055,  0.38641354,  0.38641354,  0.38641354]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> 只看batch里的第一个\n",
      "emb1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.41871086, 0.52999526, 0.4945093 ], dtype=float32)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "emb2_r\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 0.00486276,  0.46456635,  0.63068444],\n",
       "       [-0.629783  , -0.06730537, -0.3083893 ],\n",
       "       [ 0.79900736, -0.19154112, -0.7691954 ],\n",
       "       [ 0.        ,  0.        ,  0.        ],\n",
       "       [ 0.26811442, -0.47049198, -0.89255047]], dtype=float32)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "res\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 0.5601334 , -0.45186988, -0.14733711,  0.        , -0.57847065],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "emb_dot\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[array([[ 0.12446526, -0.48983523,  0.30339354,  0.        , -0.03028464],\n",
       "        [ 0.13806576,  0.49626055,  0.38641354,  0.38641354,  0.38641354]],\n",
       "       dtype=float32),\n",
       " array([[ 0.5601334 , -0.45186988, -0.14733711,  0.        , -0.57847065],\n",
       "        [ 0.13806576,  0.49626055,  0.38641354,  0.38641354,  0.38641354]],\n",
       "       dtype=float32)]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all_emb_dot | concat emb_dot\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 0.12446526, -0.48983523,  0.30339354,  0.        , -0.03028464,\n",
       "         0.5601334 , -0.45186988, -0.14733711,  0.        , -0.57847065],\n",
       "       [ 0.13806576,  0.49626055,  0.38641354,  0.38641354,  0.38641354,\n",
       "         0.13806576,  0.49626055,  0.38641354,  0.38641354,  0.38641354]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logits | tf.concat(all_emb_dot, axis=1) & 4分类\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[-0.03423132,  0.16598332,  0.17956713, -0.2545311 ],\n",
       "       [ 0.16397662, -0.02424657, -1.1411203 ,  0.46004915]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all_emb_dot_bn | concat emb_dot\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 0.12440307, -0.4895905 ,  0.30324197,  0.        , -0.03026951,\n",
       "         0.55985355, -0.45164412, -0.1472635 ,  0.        , -0.5781816 ],\n",
       "       [ 0.13799678,  0.49601263,  0.38622048,  0.38622048,  0.38622048,\n",
       "         0.13799678,  0.49601263,  0.38622048,  0.38622048,  0.38622048]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logits_bn | tf.concat(all_emb_dot_bn, axis=1) & 4分类\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[-0.04481216,  0.5127205 , -0.04209074, -0.35768437],\n",
       "       [-0.33998445, -0.2264207 , -0.0203252 , -0.3708216 ]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(k)\n",
    "print(v)\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    print(\"emb1\")\n",
    "    sess.run(emb1)\n",
    "    print(\"emb2\")\n",
    "    sess.run(emb2)\n",
    "    print(\"emb1_r\")\n",
    "    sess.run(emb1_r)\n",
    "    print(\"emb2_r\")\n",
    "    sess.run(emb2_r)\n",
    "    print(\"res\")\n",
    "    sess.run(res)\n",
    "    \n",
    "    print(\">>> 只看batch里的第一个\")\n",
    "    print(\"emb1\")\n",
    "    sess.run(emb1)[0]\n",
    "#     print(\"emb2\")\n",
    "#     sess.run(emb2)[0]\n",
    "#     print(\"emb1_r\")\n",
    "#     sess.run(emb1_r)[0]\n",
    "    print(\"emb2_r\")\n",
    "    sess.run(emb2_r)[0]\n",
    "    print(\"res\")\n",
    "    sess.run(res)[0]\n",
    "    \n",
    "    print(\"emb_dot\")\n",
    "    sess.run(emb_dot)\n",
    "    print(\"all_emb_dot | concat emb_dot\")\n",
    "    sess.run(all_emb_dot)\n",
    "#     print(\"tf.concat(emb_dot, axis=0)\")\n",
    "#     sess.run(tf.concat(emb_dot, axis=1))\n",
    "#     print(\"fm_2nd_res | tf.reduce_sum(tf.concat(emb_dot, axis=1), axis=1)\")\n",
    "#     sess.run(fm_2nd_res)\n",
    "    print(\"logits | tf.concat(all_emb_dot, axis=1) & 4分类\")\n",
    "    sess.run(logits)\n",
    "\n",
    "    print(\"all_emb_dot_bn | concat emb_dot\")\n",
    "    sess.run(all_emb_dot_bn)\n",
    "    print(\"logits_bn | tf.concat(all_emb_dot_bn, axis=1) & 4分类\")\n",
    "    sess.run(logits_bn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-19T08:30:45.214216Z",
     "start_time": "2020-11-19T08:30:45.203254Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.2338353800000001"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "3.66484891"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum([ 0.12115489,  0.39662713, -0.45657066,  0.        , -0.11924169,\n",
    "        -0.06230509, -0.43095323,  0.20267147,  0.        ,  0.1147818 ])\n",
    "sum([ 0.98669964, -0.9935233 , -0.96973526,  0.9557099 ,  0.80604684,\n",
    "         0.95365196, -0.9989523 ,  0.9882004 ,  0.9557099 ,  0.98104113])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-19T07:48:16.846625Z",
     "start_time": "2020-11-19T07:48:16.827614Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.5709219853572799, -0.013578032623862401, -0.093756372889564]"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "0.4635875798438535"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# copy 出来手工计算\n",
    "emb1 = [ 0.91038424, -0.03854756, -0.2341526 ]\n",
    "emb2 = [ 0.627122  ,  0.35224104,  0.40040714]\n",
    "\n",
    "[a*b for a,b in zip(emb1,emb2)]\n",
    "sum([a*b for a,b in zip(emb1,emb2)])\n",
    "\n",
    "# sum([ 0.3972234 , -0.308128  ,  0.        , -0.07037204, -0.308128  ,\n",
    "#          1.4594309 ,  0.        , -0.29277474])\n",
    "# sum([ 0.5564581 ,  0.41053557,  0.        ,  0.5564581 ,  0.4649981 , 0.17164305,  0.        ,  0.4649981 ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \"net\",sess.run(net)\n",
    "    \"emb_added\",sess.run(emb_added)\n",
    "    \"net_final\",sess.run(net_final)\n",
    "    print(\">>> details of emb_added\")\n",
    "    for f1,f2 in emb_to_add:\n",
    "        net1 = fc.input_layer(features, fc_emb_dict[f1])\n",
    "        net2 = fc.input_layer(features, fc_emb_dict[f2])\n",
    "        f1,sess.run(net1)\n",
    "        f2,sess.run(net2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def fm(inputs):\n",
    "#     if K.ndim(inputs) != 3:\n",
    "#         raise ValueError(\n",
    "#             \"Unexpected inputs dimensions %d, expect to be 3 dimensions\"\n",
    "#             % (K.ndim(inputs)))\n",
    "\n",
    "#     concated_embeds_value = inputs\n",
    "\n",
    "#     square_of_sum = tf.square(reduce_sum(\n",
    "#         concated_embeds_value, axis=1, keep_dims=True))\n",
    "#     sum_of_square = reduce_sum(\n",
    "#         concated_embeds_value * concated_embeds_value, axis=1, keep_dims=True)\n",
    "#     cross_term = square_of_sum - sum_of_square\n",
    "#     cross_term = 0.5 * reduce_sum(cross_term, axis=2, keep_dims=False)\n",
    "\n",
    "#     return cross_term\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interact emb by Co-Action Unit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### basic | imitation hparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-02T11:06:25.665748Z",
     "start_time": "2020-12-02T11:06:25.650280Z"
    },
    "run_control": {
     "marked": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'batch_size': 2,\n",
       " 'emb_mlp_feature_size': 100,\n",
       " 'emb_mlp_units': [8, 8, 8],\n",
       " 'embedding_feature_size': 20,\n",
       " 'embedding_size': 4,\n",
       " 'n_classes': [4, 4]}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hparams = {}\n",
    "hparams['batch_size'] = 2\n",
    "hparams['n_classes'] = [4,4]  # idx=1是跨路label\n",
    "hparams['embedding_size'] = 4\n",
    "hparams[\"embedding_feature_size\"]=20\n",
    "hparams['emb_mlp_units'] = [8,8,8]\n",
    "hparams['emb_mlp_feature_size'] = 100\n",
    "\n",
    "hparams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### basic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-02T11:06:25.791993Z",
     "start_time": "2020-12-02T11:06:25.733144Z"
    },
    "run_control": {
     "marked": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'featA': array([[1],\n",
      "       [6]]), 'pickupHash': array([[18],\n",
      "       [ 5]]), 'time_idx': array([[16],\n",
      "       [17]]), 'numericB': [[0.5], [0.2]], 'numericA': [[0.5], [1.2]], 'link': [[-1], [-1]], 'hour_idx': array([[15],\n",
      "       [ 3]]), 'whateverB': array([[14],\n",
      "       [14]]), 'whateverA': array([[5],\n",
      "       [6]]), 'featB': array([[ 5],\n",
      "       [17]])}\n"
     ]
    }
   ],
   "source": [
    "##############\n",
    "# h-params\n",
    "##############\n",
    "num_buckets_deep=hparams[\"embedding_feature_size\"]\n",
    "dims_embs_deep=hparams['embedding_size']\n",
    "_batch_size=hparams['batch_size']\n",
    "\n",
    "##############\n",
    "# features\n",
    "##############\n",
    "features = {}\n",
    "features[\"featA\"] = np.random.randint(num_buckets_deep, size=(_batch_size, 1))\n",
    "features[\"featB\"] = np.random.randint(num_buckets_deep, size=(_batch_size, 1))\n",
    "features[\"link\"] = [[-1],[-1]]\n",
    "features[\"pickupHash\"] = np.random.randint(num_buckets_deep, size=(_batch_size, 1))\n",
    "features[\"hour_idx\"] = np.random.randint(num_buckets_deep, size=(_batch_size, 1))\n",
    "features[\"time_idx\"] = np.random.randint(num_buckets_deep, size=(_batch_size, 1))\n",
    "features[\"whateverA\"] = np.random.randint(num_buckets_deep, size=(_batch_size, 1))\n",
    "features[\"whateverB\"] = np.random.randint(num_buckets_deep, size=(_batch_size, 1))\n",
    "features[\"numericA\"] = [[0.5],[1.2]]\n",
    "features[\"numericB\"] = [[0.5],[0.2]]\n",
    "print(features)\n",
    "\n",
    "################\n",
    "# build columns\n",
    "################\n",
    "tf.reset_default_graph()\n",
    "# numeric column\n",
    "fc_numeric = list()\n",
    "fc_numeric.append(fc.numeric_column(\"numericA\", default_value=0))\n",
    "fc_numeric.append(fc.numeric_column(\"numericB\", default_value=0))\n",
    "\n",
    "# codec column\n",
    "emb_feat_list = list()\n",
    "for i in [\"featA\",\"featB\",\"link\",\"pickupHash\",\"hour_idx\",\"time_idx\"]:\n",
    "    emb_feat_list.append(fc.categorical_column_with_identity(i, num_buckets_deep, default_value=0))\n",
    "\n",
    "# emb column\n",
    "fc_shared_emb = fc.shared_embedding_columns(\n",
    "                    emb_feat_list,\n",
    "                    dimension=dims_embs_deep, max_norm=np.sqrt(dims_embs_deep),\n",
    "                    trainable=True)\n",
    "\n",
    "# shared_emb 转成KV结构方便后面操作\n",
    "fc_emb_dict = {seCol.categorical_column.key: seCol for seCol in fc_shared_emb}\n",
    "\n",
    "# 指出将要操作的emb\n",
    "emb_to_add=[['link','hour_idx'],\n",
    "            ['pickupHash','time_idx']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-02T11:06:27.391034Z",
     "start_time": "2020-12-02T11:06:27.356136Z"
    },
    "code_folding": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[_EmbeddingColumn(categorical_column=_HashedCategoricalColumn(key='pickupHash', hash_bucket_size=100, dtype=tf.int64), dimension=32, combiner='mean', layer_creator=<function _creator at 0x7f94cdf64b18>, ckpt_to_load_from=None, tensor_name_in_ckpt=None, max_norm=5.656854249492381, trainable=True),\n",
       " _EmbeddingColumn(categorical_column=_HashedCategoricalColumn(key='pickupHash', hash_bucket_size=100, dtype=tf.int64), dimension=64, combiner='mean', layer_creator=<function _creator at 0x7f94cdf64c08>, ckpt_to_load_from=None, tensor_name_in_ckpt=None, max_norm=8.0, trainable=True),\n",
       " _EmbeddingColumn(categorical_column=_HashedCategoricalColumn(key='pickupHash', hash_bucket_size=100, dtype=tf.int64), dimension=64, combiner='mean', layer_creator=<function _creator at 0x7f94cdf64938>, ckpt_to_load_from=None, tensor_name_in_ckpt=None, max_norm=8.0, trainable=True)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "[_EmbeddingColumn(categorical_column=_HashedCategoricalColumn(key='pickupHash', hash_bucket_size=100, dtype=tf.int64), dimension=8, combiner='mean', layer_creator=<function _creator at 0x7f94cdf64ed8>, ckpt_to_load_from=None, tensor_name_in_ckpt=None, max_norm=2.8284271247461903, trainable=True),\n",
       " _EmbeddingColumn(categorical_column=_HashedCategoricalColumn(key='pickupHash', hash_bucket_size=100, dtype=tf.int64), dimension=8, combiner='mean', layer_creator=<function _creator at 0x7f94cdf64e60>, ckpt_to_load_from=None, tensor_name_in_ckpt=None, max_norm=2.8284271247461903, trainable=True),\n",
       " _EmbeddingColumn(categorical_column=_HashedCategoricalColumn(key='pickupHash', hash_bucket_size=100, dtype=tf.int64), dimension=8, combiner='mean', layer_creator=<function _creator at 0x7f94cdf64d70>, ckpt_to_load_from=None, tensor_name_in_ckpt=None, max_norm=2.8284271247461903, trainable=True)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def co_action_unit_fc(hparams):\n",
    "    # fc_target_item = fc.categorical_column_with_identity(hparams[\"emb_mlp_feat\"], hparams['mlp_embedding_feature_size'], default_value=0)\n",
    "    fc_target_item = fc.categorical_column_with_hash_bucket(\"pickupHash\", hash_bucket_size=hparams['emb_mlp_feature_size'], dtype=tf.int64)\n",
    "\n",
    "    fc_emb_mlp_kernel_list = []\n",
    "    fc_emb_mlp_bias_list = []\n",
    "    inp_size = hparams['embedding_size']\n",
    "    for idx, i in enumerate(hparams['emb_mlp_units']):\n",
    "        kernel_size = inp_size*i\n",
    "        bias_size = i\n",
    "        _fc_emb_kernel = fc.embedding_column(fc_target_item,\n",
    "                                             dimension=kernel_size, max_norm=np.sqrt(kernel_size),\n",
    "                                             trainable=True)\n",
    "        fc_emb_mlp_kernel_list.append(_fc_emb_kernel)\n",
    "        _fc_emb_bias = fc.embedding_column(fc_target_item,\n",
    "                                           dimension=bias_size, max_norm=np.sqrt(bias_size),\n",
    "                                           trainable=True)\n",
    "        fc_emb_mlp_bias_list.append(_fc_emb_bias)\n",
    "\n",
    "        # # >>>deubg\n",
    "        # kernel = np.random.random([inp_size,i])\n",
    "        # bias = np.random.random(bias_size)\n",
    "        # print(\"kernel shape:%s size:%s\" % (kernel.shape, kernel_size))\n",
    "        # print(\"bias shape:%s size:%s\" % (bias.shape, bias_size))\n",
    "        # # <<<deubg\n",
    "\n",
    "        inp_size = i  # IMPORTANT\n",
    "\n",
    "    # [fc_emb.dimension for fc_emb in fc_emb_mlp_kernel_list]\n",
    "    # [fc_emb.dimension for fc_emb in fc_emb_mlp_bias_list]\n",
    "    return fc_emb_mlp_kernel_list, fc_emb_mlp_bias_list\n",
    "fc_emb_mlp_kernel_list, fc_emb_mlp_bias_list = co_action_unit_fc(hparams)\n",
    "fc_emb_mlp_kernel_list\n",
    "fc_emb_mlp_bias_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-02T11:07:36.930994Z",
     "start_time": "2020-12-02T11:07:36.519843Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'Reshape_2:0' shape=(2, 4, 8) dtype=float32>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'Reshape_3:0' shape=(2, 8, 8) dtype=float32>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'Reshape_4:0' shape=(2, 8, 8) dtype=float32>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'ExpandDims_5:0' shape=(2, 1, 8) dtype=float32>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fc_emb_inp_list = [fc_emb_dict[i] for i in ['featA','featB']]\n",
    "\n",
    "emb_inp = fc.input_layer(features, fc_emb_inp)  # (batch_size, D)\n",
    "emb_inp = tf.math.pow(emb_inp, 1) + tf.math.pow(emb_inp, 2) + tf.math.pow(emb_inp, 3)\n",
    "emb_inp = tf.expand_dims(emb_inp, 1)\n",
    "\n",
    "net = emb_inp\n",
    "for idx, fc_emb in enumerate(fc_emb_mlp_kernel_list):\n",
    "    emb_mlp_kernel = fc.input_layer(features, fc_emb)\n",
    "    cur_output = hparams['emb_mlp_units'][idx]\n",
    "    emb_mlp_kernel = tf.reshape(emb_mlp_kernel, [-1, fc_emb.dimension/cur_output, cur_output])\n",
    "    emb_mlp_kernel\n",
    "#     emb_mlp_bias = fc.input_layer(features, fc_emb_mlp_bias_list[idx])\n",
    "#     emb_mlp_bias = tf.expand_dims(emb_mlp_bias, 1)\n",
    "#     # wx+b\n",
    "#     res = tf.einsum(\"bik,bkj->bij\", net, emb_mlp_kernel) + emb_mlp_bias\n",
    "#     # activation\n",
    "#     res = tf.nn.relu(res)\n",
    "#     net = res\n",
    "# net = tf.squeeze(net)\n",
    "net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-02T11:09:21.319787Z",
     "start_time": "2020-12-02T11:09:19.985205Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Squeeze:0\", shape=(2, 8), dtype=float32)\n",
      "Tensor(\"Squeeze_1:0\", shape=(2, 8), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "def co_action_unit(features, fc_emb_inp, fc_emb_mlp_kernel_list, fc_emb_mlp_bias_list, hparams, mode):\n",
    "    \"\"\"\n",
    "\n",
    "    :param features:\n",
    "    :param fc_emb_inp: emb_feature_column\n",
    "    :param fc_emb_mlp_kernel_list: list[emb_feature_column]\n",
    "    :param fc_emb_mlp_bias_list: list[emb_feature_column]\n",
    "    :param hparams: params\n",
    "    :param mode: TRAIN EVALUATION .etc\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    emb_inp = fc.input_layer(features, fc_emb_inp)  # (batch_size, D)\n",
    "    emb_inp = tf.math.pow(emb_inp, 1) + tf.math.pow(emb_inp, 2) + tf.math.pow(emb_inp, 3)\n",
    "    emb_inp = tf.expand_dims(emb_inp, 1)\n",
    "\n",
    "    net = emb_inp\n",
    "    for idx, fc_emb in enumerate(fc_emb_mlp_kernel_list):\n",
    "        emb_mlp_kernel = fc.input_layer(features, fc_emb)\n",
    "        cur_output = hparams['emb_mlp_units'][idx]\n",
    "        emb_mlp_kernel = tf.reshape(emb_mlp_kernel, [-1, fc_emb.dimension/cur_output, cur_output])\n",
    "        emb_mlp_bias = fc.input_layer(features, fc_emb_mlp_bias_list[idx])\n",
    "        emb_mlp_bias = tf.expand_dims(emb_mlp_bias, 1)\n",
    "        # wx+b\n",
    "        res = tf.einsum(\"bik,bkj->bij\", net, emb_mlp_kernel) + emb_mlp_bias\n",
    "        # activation\n",
    "        res = tf.nn.relu(res)\n",
    "        net = res\n",
    "    net = tf.squeeze(net)\n",
    "    return net\n",
    "\n",
    "fc_emb_inp_list = [fc_emb_dict[i] for i in ['featA','featB']]\n",
    "emb_can_list = []\n",
    "\n",
    "for fc_emb_inp in fc_emb_inp_list:\n",
    "        emb_can = co_action_unit(features, fc_emb_inp, fc_emb_mlp_kernel_list, fc_emb_mlp_bias_list, hparams, 'TRAIN')\n",
    "        print(emb_can)\n",
    "        emb_can_list.append(emb_can)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-02T11:10:16.056334Z",
     "start_time": "2020-12-02T11:10:15.543578Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'input_layer_28/concat:0' shape=(2, 24) dtype=float32>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor 'Squeeze:0' shape=(2, 8) dtype=float32>,\n",
       " <tf.Tensor 'Squeeze_1:0' shape=(2, 8) dtype=float32>]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'concat:0' shape=(2, 40) dtype=float32>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = fc.input_layer(features, fc_emb_dict.values())\n",
    "net\n",
    "emb_can_list\n",
    "tf.concat([net] + emb_can_list ,axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### basic | emb_mlp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "kernel一张表，bias一张表"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-02T06:34:58.124973Z",
     "start_time": "2020-12-02T06:34:58.069293Z"
    },
    "code_folding": [
     0
    ],
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true,
     "marked": true
    }
   },
   "outputs": [],
   "source": [
    "# emb_mlp size的计算，输入是普通的emb_size输出是n_class，中间是N个全连接\n",
    "hparams['emb_mlp_units'] = [8]*3  # 先用桶状结构吧，有利于后面reshape然后用矩阵乘法计算更高效（不然要用slice）\n",
    "hparams['emb_mlp_bias'] = [8]*3\n",
    "\n",
    "emb_mlp_kernel_size_1st = 0\n",
    "emb_mlp_kernel_size = 0\n",
    "emb_mlp_bias_size = 0\n",
    "inp_size = hparams['embedding_size']\n",
    "for idx,i in enumerate(hparams['emb_mlp_units']):\n",
    "    kernel_size = inp_size*i\n",
    "    bias_size = i\n",
    "    kernel = np.random.random([inp_size,i])\n",
    "    bias = np.random.random(bias_size)\n",
    "    print(\"shape:%s size:%s\" % (kernel.shape, kernel_size))\n",
    "    print(\"shape:%s size:%s\" % (bias.shape, bias_size))\n",
    "    if idx == 0:\n",
    "        emb_mlp_kernel_size_1st = kernel_size\n",
    "    \n",
    "    total_emb_mlp_kernel_size += kernel_size\n",
    "    total_emb_mlp_bias_size += bias_size\n",
    "    inp_size = i\n",
    "\n",
    "hparams['emb_mlp_kernel_size'] = total_emb_mlp_kernel_size\n",
    "hparams['emb_mlp_bias_size'] = total_emb_mlp_bias_size\n",
    "total_emb_mlp_kernel_size\n",
    "total_emb_mlp_bias_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "每个kernel各自一张表，每个bias各自一张表"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-02T07:05:43.154016Z",
     "start_time": "2020-12-02T07:05:43.146280Z"
    }
   },
   "outputs": [],
   "source": [
    "# emb_mlp size的计算，输入是普通的emb_size输出是n_class，中间是N个全连接\n",
    "hparams['emb_mlp_units'] = [8,16,8]\n",
    "# 应该是独立去重计算，但是这里不像重新建索引，就还是偷个懒用之前的size\n",
    "hparams[\"mlp_embedding_feature_size\"] = hparams[\"embedding_feature_size\"]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-02T07:05:45.178269Z",
     "start_time": "2020-12-02T07:05:43.851577Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kernel shape:(4, 8) size:32\n",
      "bias shape:(8,) size:8\n",
      "kernel shape:(8, 16) size:128\n",
      "bias shape:(16,) size:16\n",
      "kernel shape:(16, 8) size:128\n",
      "bias shape:(8,) size:8\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[32, 128, 128]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "[8, 16, 8]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'ExpandDims:0' shape=(2, 1, 4) dtype=float32>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> at idx: 0 \n",
      ">>> at idx: 1 \n",
      ">>> at idx: 2 \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'Relu_2:0' shape=(2, 1, 8) dtype=float32>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "targetItemCol = fc.categorical_column_with_hash_bucket(\"pickupHash\", hash_bucket_size=hparams['embedding_feature_size'], dtype=tf.int64)\n",
    "fc_emb_mlp_kernel = []\n",
    "fc_emb_mlp_bias = []\n",
    "inp_size = hparams['embedding_size']\n",
    "for idx,i in enumerate(hparams['emb_mlp_units']):\n",
    "    kernel_size = inp_size*i\n",
    "    bias_size = i\n",
    "    _fc_emb_kernel = fc.embedding_column(targetItemCol,\n",
    "                                         dimension=kernel_size, max_norm=np.sqrt(kernel_size),\n",
    "                                         trainable=True)\n",
    "    fc_emb_mlp_kernel.append(_fc_emb_kernel)\n",
    "    _fc_emb_bias = fc.embedding_column(targetItemCol,\n",
    "                                       dimension=bias_size, max_norm=np.sqrt(bias_size),\n",
    "                                       trainable=True)\n",
    "    fc_emb_mlp_bias.append(_fc_emb_bias)\n",
    "    \n",
    "    # >>>deubg\n",
    "    kernel = np.random.random([inp_size,i])\n",
    "    bias = np.random.random(bias_size)\n",
    "    print(\"kernel shape:%s size:%s\" % (kernel.shape, kernel_size))\n",
    "    print(\"bias shape:%s size:%s\" % (bias.shape, bias_size))\n",
    "    # <<<deubg\n",
    "    \n",
    "    inp_size = i  # IMPORTANT\n",
    "\n",
    "[fc_emb.dimension for fc_emb in fc_emb_mlp_kernel]\n",
    "[fc_emb.dimension for fc_emb in fc_emb_mlp_bias]\n",
    "\n",
    "\n",
    "# 输入侧的emb column\n",
    "# fc_emb_inp = [fc_emb_dict[i] for i in ['link','hour_idx']]\n",
    "fc_emb_inp = fc_emb_dict['hour_idx']  # 一个个来吧，考虑到还要做1~3次幂\n",
    "emb_inp = fc.input_layer(features, fc_emb_inp) # (batch_size, D)\n",
    "emb_inp = tf.math.pow(emb_inp,1) + tf.math.pow(emb_inp,2) + tf.math.pow(emb_inp,3)\n",
    "emb_inp = tf.expand_dims(emb_inp,1)\n",
    "emb_inp\n",
    "\n",
    "inp = emb_inp\n",
    "for idx,fc_emb in enumerate(fc_emb_mlp_kernel):\n",
    "    print(\">>> at idx: %s \" % idx)\n",
    "    emb_mlp_kernel = fc.input_layer(features, fc_emb)\n",
    "    cur_output = hparams['emb_mlp_units'][idx]\n",
    "    emb_mlp_kernel = tf.reshape(emb_mlp_kernel,[-1, fc_emb.dimension/cur_output, cur_output])\n",
    "    emb_mlp_bias = fc.input_layer(features, fc_emb_mlp_bias[idx])\n",
    "    emb_mlp_bias = tf.expand_dims(emb_mlp_bias,1)\n",
    "    res = tf.einsum(\"bik,bkj->bij\", inp, emb_mlp_kernel) + emb_mlp_bias\n",
    "    res = tf.nn.relu(res)\n",
    "    inp = res\n",
    "\n",
    "res\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## fc on RNN "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### basic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-08T12:23:20.445593Z",
     "start_time": "2021-06-08T12:23:20.347297Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>driver_phone_ori</th>\n",
       "      <th>date</th>\n",
       "      <th>leader_uid_cnt</th>\n",
       "      <th>label</th>\n",
       "      <th>uid_feats</th>\n",
       "      <th>cat_feats</th>\n",
       "      <th>num_feats</th>\n",
       "      <th>leader_atas</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>13086618239</td>\n",
       "      <td>20210302</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1400.0</td>\n",
       "      <td>[0, 1]</td>\n",
       "      <td>[[1, 7, 9, 11], [1, 7, 9, 11]]</td>\n",
       "      <td>[[32.0, 19.5, 7.0], [32.0, 19.5, 7.0]]</td>\n",
       "      <td>[1200, 1400]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>13086618239</td>\n",
       "      <td>20210302</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1400.0</td>\n",
       "      <td>[0, 2]</td>\n",
       "      <td>[[1, 7, 9, 11], [1, 7, 9, 11]]</td>\n",
       "      <td>[[32.0, 19.5, 7.0], [32.0, 19.5, 7.0]]</td>\n",
       "      <td>[1200, 1400]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>18573371766</td>\n",
       "      <td>20210310</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1900.0</td>\n",
       "      <td>[1, 2, 3, 4]</td>\n",
       "      <td>[[1, 8, 9, 14], [1, 8, 9, 14], [1, 8, 9, 14], ...</td>\n",
       "      <td>[[32.0, 19.5, 7.0], [32.0, 19.5, 7.0], [32.0, ...</td>\n",
       "      <td>[1600, 1900, 2100, 2800]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  driver_phone_ori      date  leader_uid_cnt   label     uid_feats  \\\n",
       "0      13086618239  20210302             2.0  1400.0        [0, 1]   \n",
       "1      13086618239  20210302             2.0  1400.0        [0, 2]   \n",
       "2      18573371766  20210310             4.0  1900.0  [1, 2, 3, 4]   \n",
       "\n",
       "                                           cat_feats  \\\n",
       "0                     [[1, 7, 9, 11], [1, 7, 9, 11]]   \n",
       "1                     [[1, 7, 9, 11], [1, 7, 9, 11]]   \n",
       "2  [[1, 8, 9, 14], [1, 8, 9, 14], [1, 8, 9, 14], ...   \n",
       "\n",
       "                                           num_feats               leader_atas  \n",
       "0             [[32.0, 19.5, 7.0], [32.0, 19.5, 7.0]]              [1200, 1400]  \n",
       "1             [[32.0, 19.5, 7.0], [32.0, 19.5, 7.0]]              [1200, 1400]  \n",
       "2  [[32.0, 19.5, 7.0], [32.0, 19.5, 7.0], [32.0, ...  [1600, 1900, 2100, 2800]  "
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "[[1, 7, 9, 11], [1, 7, 9, 11], [-1, -1, -1, -1], [-1, -1, -1, -1]]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "[[1, 7, 9, 11], [1, 7, 9, 11], [-1, -1, -1, -1], [-1, -1, -1, -1]]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "[[1, 8, 9, 14], [1, 8, 9, 14], [1, 8, 9, 14], [1, 8, 9, 14]]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "[[32.0, 19.5, 7.0], [32.0, 19.5, 7.0], [0.0, 0.0, 0.0], [0.0, 0.0, 0.0]]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "[[32.0, 19.5, 7.0], [32.0, 19.5, 7.0], [0.0, 0.0, 0.0], [0.0, 0.0, 0.0]]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "[[32.0, 19.5, 7.0], [32.0, 19.5, 7.0], [32.0, 19.5, 7.0], [32.0, 19.5, 7.0]]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "[1200, 1400, 0.0, 0.0]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "[1200, 1400, 0.0, 0.0]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "[1600, 1900, 2100, 2800]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>driver_phone_ori</th>\n",
       "      <th>date</th>\n",
       "      <th>leader_uid_cnt</th>\n",
       "      <th>label</th>\n",
       "      <th>uid_feats</th>\n",
       "      <th>cat_feats</th>\n",
       "      <th>num_feats</th>\n",
       "      <th>leader_atas</th>\n",
       "      <th>seq_len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>13086618239</td>\n",
       "      <td>20210302</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1400.0</td>\n",
       "      <td>[0, 1, -1, -1]</td>\n",
       "      <td>[[1, 7, 9, 11], [1, 7, 9, 11], [-1, -1, -1, -1...</td>\n",
       "      <td>[[32.0, 19.5, 7.0], [32.0, 19.5, 7.0], [0.0, 0...</td>\n",
       "      <td>[1200, 1400, 0.0, 0.0]</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>13086618239</td>\n",
       "      <td>20210302</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1400.0</td>\n",
       "      <td>[0, 2, -1, -1]</td>\n",
       "      <td>[[1, 7, 9, 11], [1, 7, 9, 11], [-1, -1, -1, -1...</td>\n",
       "      <td>[[32.0, 19.5, 7.0], [32.0, 19.5, 7.0], [0.0, 0...</td>\n",
       "      <td>[1200, 1400, 0.0, 0.0]</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>18573371766</td>\n",
       "      <td>20210310</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1900.0</td>\n",
       "      <td>[1, 2, 3, 4]</td>\n",
       "      <td>[[1, 8, 9, 14], [1, 8, 9, 14], [1, 8, 9, 14], ...</td>\n",
       "      <td>[[32.0, 19.5, 7.0], [32.0, 19.5, 7.0], [32.0, ...</td>\n",
       "      <td>[1600, 1900, 2100, 2800]</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  driver_phone_ori      date  leader_uid_cnt   label       uid_feats  \\\n",
       "0      13086618239  20210302             2.0  1400.0  [0, 1, -1, -1]   \n",
       "1      13086618239  20210302             2.0  1400.0  [0, 2, -1, -1]   \n",
       "2      18573371766  20210310             4.0  1900.0    [1, 2, 3, 4]   \n",
       "\n",
       "                                           cat_feats  \\\n",
       "0  [[1, 7, 9, 11], [1, 7, 9, 11], [-1, -1, -1, -1...   \n",
       "1  [[1, 7, 9, 11], [1, 7, 9, 11], [-1, -1, -1, -1...   \n",
       "2  [[1, 8, 9, 14], [1, 8, 9, 14], [1, 8, 9, 14], ...   \n",
       "\n",
       "                                           num_feats  \\\n",
       "0  [[32.0, 19.5, 7.0], [32.0, 19.5, 7.0], [0.0, 0...   \n",
       "1  [[32.0, 19.5, 7.0], [32.0, 19.5, 7.0], [0.0, 0...   \n",
       "2  [[32.0, 19.5, 7.0], [32.0, 19.5, 7.0], [32.0, ...   \n",
       "\n",
       "                leader_atas  seq_len  \n",
       "0    [1200, 1400, 0.0, 0.0]        2  \n",
       "1    [1200, 1400, 0.0, 0.0]        2  \n",
       "2  [1600, 1900, 2100, 2800]        4  "
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "foo = [\n",
    "        [\"13086618239\", \"20210302\", 2.0, 1400.0, [0,1],[[1, 7, 9, 11]]*2, [[32.0, 19.5, 7.0]]*2, [1200, 1400]],\n",
    "        [\"13086618239\", \"20210302\", 2.0, 1400.0, [0,2],[[1, 7, 9, 11]]*2, [[32.0, 19.5, 7.0]]*2, [1200, 1400]],\n",
    "        [\"18573371766\", \"20210310\", 4.0, 1900.0, [1,2,3,4],[[1, 8, 9, 14]]*4, [[32.0, 19.5, 7.0]]*4, [1600, 1900, 2100, 2800]],\n",
    "    ]\n",
    "\n",
    "dataDF=pd.DataFrame(foo, columns=[\"driver_phone_ori\",\"date\",\"leader_uid_cnt\",\"label\",\"uid_feats\",\"cat_feats\",\"num_feats\",\"leader_atas\"])\n",
    "dataDF\n",
    "\n",
    "def pad_leaders(inpDF,pad_size=None):\n",
    "    pad_size = inpDF['cat_feats'].apply(lambda x:len(x)).max() if pad_size is None else pad_size\n",
    "    \n",
    "    cat_pad_item = [-1]*len(inpDF['cat_feats'].iloc[0][0])\n",
    "    num_pad_item = [0.0]*len(inpDF['num_feats'].iloc[0][0])\n",
    "    leader_ata_pad_item = 0.0\n",
    "    uid_feats_pad_item = -1\n",
    "    inpDF['seq_len'] = inpDF['cat_feats'].apply(lambda x: len(x))\n",
    "    inpDF['cat_feats']=inpDF['cat_feats'].apply(lambda x: (x+[cat_pad_item]*pad_size)[:pad_size])\n",
    "    inpDF['num_feats']=inpDF['num_feats'].apply(lambda x: (x+[num_pad_item]*pad_size)[:pad_size])\n",
    "    inpDF['leader_atas'] = inpDF['leader_atas'].apply(lambda x: (x + [leader_ata_pad_item] * pad_size)[:pad_size])\n",
    "    inpDF['uid_feats'] = inpDF['uid_feats'].apply(lambda x: (x + [uid_feats_pad_item] * pad_size)[:pad_size])\n",
    "    return inpDF\n",
    "\n",
    "dataDF = pad_leaders(dataDF)\n",
    "\n",
    "for i in dataDF['cat_feats']:\n",
    "    i\n",
    "    \n",
    "for i in dataDF['num_feats']:\n",
    "    i\n",
    "    \n",
    "for i in dataDF['leader_atas']:\n",
    "    i\n",
    "    \n",
    "dataDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### prepare ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-08T12:23:25.854673Z",
     "start_time": "2021-06-08T12:23:25.771574Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'driver_phone_ori': '13086618239',\n",
       " 'date': '20210302',\n",
       " 'leader_uid_cnt': 2.0,\n",
       " 'label': 1400.0,\n",
       " 'uid_feats': [0, 1, -1, -1],\n",
       " 'cat_feats': [[1, 7, 9, 11],\n",
       "  [1, 7, 9, 11],\n",
       "  [-1, -1, -1, -1],\n",
       "  [-1, -1, -1, -1]],\n",
       " 'num_feats': [[32.0, 19.5, 7.0],\n",
       "  [32.0, 19.5, 7.0],\n",
       "  [0.0, 0.0, 0.0],\n",
       "  [0.0, 0.0, 0.0]],\n",
       " 'leader_atas': [1200, 1400, 0.0, 0.0],\n",
       " 'seq_len': 2}"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Entity <function <lambda> at 0x7fa8025327a0> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: expected exactly one node node, found []\n",
      "WARNING: Entity <function <lambda> at 0x7fa8025327a0> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: expected exactly one node node, found []\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "def df_gen(inpDF):\n",
    "    for _, row in inpDF.iterrows():\n",
    "        yield dict((k, v) for k, v in row.to_dict().items())\n",
    "\n",
    "\n",
    "def parse_feat_lbl(inp,label_col=\"label\"):\n",
    "    labels = tf.reshape(inp.pop(label_col),(-1,1))\n",
    "    feats = inp\n",
    "    return feats,labels\n",
    "\n",
    "df_gen(dataDF).__next__()\n",
    "\n",
    "batch_size=3\n",
    "repeatCnt=100000\n",
    "typeDict = dataDF.dtypes.to_dict()\n",
    "typeDict = {\"driver_phone_ori\":tf.string,\n",
    "            \"date\":tf.string,\n",
    "            \"leader_uid_cnt\":tf.float32,\n",
    "            \"label\":tf.float32,\n",
    "            \"cat_feats\":tf.int32,\n",
    "            \"num_feats\":tf.float32,\n",
    "            \"uid_feats\": tf.int32,\n",
    "            \"leader_atas\":tf.float32,\n",
    "            \"seq_len\": tf.int32,\n",
    "           }\n",
    "shapeDict = {\n",
    "    \"driver_phone_ori\":(),\n",
    "    \"date\":(),\n",
    "    \"leader_uid_cnt\":(),\n",
    "    \"label\":(),\n",
    "    \"cat_feats\":(4,4),\n",
    "    \"num_feats\":(4,3),\n",
    "    \"uid_feats\": (4,),\n",
    "    \"leader_atas\":(4,),\n",
    "    \"seq_len\": (),\n",
    "}\n",
    "ds = tf.data.Dataset.from_generator(lambda : df_gen(dataDF), output_types=typeDict,output_shapes=shapeDict) \\\n",
    "    .map(lambda x: parse_feat_lbl(x,label_col=\"label\")) \\\n",
    "    .batch(batch_size).repeat(repeatCnt).prefetch(tf.contrib.data.AUTOTUNE)\n",
    "\n",
    "features,labels = ds.make_one_shot_iterator().get_next()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 原始数据inspect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-08T11:47:30.789782Z",
     "start_time": "2021-06-08T11:47:30.708660Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3, 4, 3], dtype=int32)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "{'driver_phone_ori': array([b'13086618239', b'13086618239', b'18573371766'], dtype=object),\n",
       " 'date': array([b'20210302', b'20210302', b'20210310'], dtype=object),\n",
       " 'leader_uid_cnt': array([2., 2., 4.], dtype=float32),\n",
       " 'cat_feats': array([[[ 1,  7,  9, 11],\n",
       "         [ 1,  7,  9, 11],\n",
       "         [-1, -1, -1, -1],\n",
       "         [-1, -1, -1, -1]],\n",
       " \n",
       "        [[ 1,  7,  9, 11],\n",
       "         [ 1,  7,  9, 11],\n",
       "         [-1, -1, -1, -1],\n",
       "         [-1, -1, -1, -1]],\n",
       " \n",
       "        [[ 1,  8,  9, 14],\n",
       "         [ 1,  8,  9, 14],\n",
       "         [ 1,  8,  9, 14],\n",
       "         [ 1,  8,  9, 14]]], dtype=int32),\n",
       " 'num_feats': array([[[32. , 19.5,  7. ],\n",
       "         [32. , 19.5,  7. ],\n",
       "         [ 0. ,  0. ,  0. ],\n",
       "         [ 0. ,  0. ,  0. ]],\n",
       " \n",
       "        [[32. , 19.5,  7. ],\n",
       "         [32. , 19.5,  7. ],\n",
       "         [ 0. ,  0. ,  0. ],\n",
       "         [ 0. ,  0. ,  0. ]],\n",
       " \n",
       "        [[32. , 19.5,  7. ],\n",
       "         [32. , 19.5,  7. ],\n",
       "         [32. , 19.5,  7. ],\n",
       "         [32. , 19.5,  7. ]]], dtype=float32),\n",
       " 'uid_feats': array([[ 1,  2, -1, -1],\n",
       "        [ 1,  2, -1, -1],\n",
       "        [ 1,  2,  3,  4]], dtype=int32),\n",
       " 'leader_atas': array([[1200., 1400.,    0.,    0.],\n",
       "        [1200., 1400.,    0.,    0.],\n",
       "        [1600., 1900., 2100., 2800.]], dtype=float32),\n",
       " 'seq_len': array([2, 2, 4], dtype=int32)}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 原始数据inspect\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "with tf.Session(config=config) as sess:\n",
    "    feats=sess.run(features)\n",
    "    catF = feats['cat_feats']\n",
    "    numF = feats['num_feats']\n",
    "    sess.run(tf.shape(numF))\n",
    "    feats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### parse fc | 理想情况下\n",
    "1. `linear_model` 直接对 `[batch_size,seq_len,sample_features]` 做FC\n",
    "2. 并且使用`linear_model`的`units=128`参数得到一个128维的向量作为emb，再过RNN\n",
    "3. 问题在于 sequence_numeric_column 似乎有bug，它不是 _FeatureColumn 的subclass？\n",
    "\n",
    "目前的结论如下\n",
    "- categorical特征用 linear_model、sequence_input_layer 都可以触发解析\n",
    "- numeric特征用 sequence_numeric_column 解析时使用 linear_model、sequence_input_layer、SequenceFeatures 都会报错\n",
    "    - 折中办法，用 numeric_column 来解析feature，使用 input_layer 触发解析然后reshape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-15T06:52:45.348802Z",
     "start_time": "2021-05-15T06:52:44.868872Z"
    }
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Items of feature_columns must be a _FeatureColumn. Given (type <class 'tensorflow.python.feature_column.sequence_feature_column.SequenceNumericColumn'>): SequenceNumericColumn(key='num_feats', shape=(1,), default_value=0.0, dtype=tf.float32, normalizer_fn=None).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-15231bcd7a94>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mnumFeats_fc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msequence_numeric_column\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"num_feats\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0minp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcatFeats_fc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumFeats_fc\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munits\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mconfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mConfigProto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/zt/lib/python3.7/site-packages/tensorflow_core/python/feature_column/feature_column.py\u001b[0m in \u001b[0;36mlinear_model\u001b[0;34m(features, feature_columns, units, sparse_combiner, weight_collections, trainable, cols_to_vars)\u001b[0m\n\u001b[1;32m    498\u001b[0m       \u001b[0mweight_collections\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mweight_collections\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    499\u001b[0m       \u001b[0mtrainable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrainable\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 500\u001b[0;31m       name=model_name)\n\u001b[0m\u001b[1;32m    501\u001b[0m   \u001b[0mretval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlinear_model_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    502\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mcols_to_vars\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/zt/lib/python3.7/site-packages/tensorflow_core/python/feature_column/feature_column.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, feature_columns, units, sparse_combiner, weight_collections, trainable, name, **kwargs)\u001b[0m\n\u001b[1;32m    633\u001b[0m     \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_LinearModel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    634\u001b[0m     self._feature_columns = _normalize_feature_columns(\n\u001b[0;32m--> 635\u001b[0;31m         feature_columns)\n\u001b[0m\u001b[1;32m    636\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_weight_collections\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight_collections\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    637\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGraphKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGLOBAL_VARIABLES\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_weight_collections\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/zt/lib/python3.7/site-packages/tensorflow_core/python/feature_column/feature_column.py\u001b[0m in \u001b[0;36m_normalize_feature_columns\u001b[0;34m(feature_columns)\u001b[0m\n\u001b[1;32m   2298\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolumn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_FeatureColumn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2299\u001b[0m       raise ValueError('Items of feature_columns must be a _FeatureColumn. '\n\u001b[0;32m-> 2300\u001b[0;31m                        'Given (type {}): {}.'.format(type(column), column))\n\u001b[0m\u001b[1;32m   2301\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mfeature_columns\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2302\u001b[0m     \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'feature_columns must not be empty.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Items of feature_columns must be a _FeatureColumn. Given (type <class 'tensorflow.python.feature_column.sequence_feature_column.SequenceNumericColumn'>): SequenceNumericColumn(key='num_feats', shape=(1,), default_value=0.0, dtype=tf.float32, normalizer_fn=None)."
     ]
    }
   ],
   "source": [
    "catFeats_fc = fc.sequence_categorical_column_with_identity(\"cat_feats\", num_buckets=20)\n",
    "catFeats_fc = fc.indicator_column(catFeats_fc)\n",
    "numFeats_fc = fc.sequence_numeric_column(\"num_feats\")\n",
    "\n",
    "inp = fc.linear_model(features,[catFeats_fc, numFeats_fc], units=128)\n",
    "\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "with tf.Session(config=config) as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    inp=sess.run(inp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### parse fc | sequence_categorical_column_with_identity 尝试\n",
    "- input_layer | 报错 \n",
    "    - *AttributeError: 'IndicatorColumn' object has no attribute '_get_sparse_tensors'*\n",
    "- sequence_input_layer | 正常\n",
    "- SequenceFeatures | 正常\n",
    "- linear_model | 正常"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-15T06:52:46.873684Z",
     "start_time": "2021-05-15T06:52:46.665865Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/luban/miniconda3/envs/zt/lib/python3.7/site-packages/tensorflow_core/contrib/feature_column/python/feature_column/sequence_feature_column.py:121: IndicatorColumn._get_sequence_dense_tensor (from tensorflow.python.feature_column.feature_column_v2) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "The old _FeatureColumn APIs are being deprecated. Please use the new FeatureColumn APIs instead.\n",
      "WARNING:tensorflow:From /home/luban/miniconda3/envs/zt/lib/python3.7/site-packages/tensorflow_core/python/feature_column/feature_column.py:2158: IndicatorColumn._transform_feature (from tensorflow.python.feature_column.feature_column_v2) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "The old _FeatureColumn APIs are being deprecated. Please use the new FeatureColumn APIs instead.\n",
      "WARNING:tensorflow:From /home/luban/miniconda3/envs/zt/lib/python3.7/site-packages/tensorflow_core/python/feature_column/feature_column_v2.py:4300: SequenceCategoricalColumn._get_sparse_tensors (from tensorflow.python.feature_column.feature_column_v2) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "The old _FeatureColumn APIs are being deprecated. Please use the new FeatureColumn APIs instead.\n",
      "WARNING:tensorflow:From /home/luban/miniconda3/envs/zt/lib/python3.7/site-packages/tensorflow_core/python/feature_column/feature_column_v2.py:4570: IdentityCategoricalColumn._get_sparse_tensors (from tensorflow.python.feature_column.feature_column_v2) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "The old _FeatureColumn APIs are being deprecated. Please use the new FeatureColumn APIs instead.\n",
      "WARNING:tensorflow:From /home/luban/miniconda3/envs/zt/lib/python3.7/site-packages/tensorflow_core/python/feature_column/feature_column.py:2158: IdentityCategoricalColumn._transform_feature (from tensorflow.python.feature_column.feature_column_v2) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "The old _FeatureColumn APIs are being deprecated. Please use the new FeatureColumn APIs instead.\n",
      "WARNING:tensorflow:From /home/luban/miniconda3/envs/zt/lib/python3.7/site-packages/tensorflow_core/python/feature_column/feature_column_v2.py:4271: IndicatorColumn._variable_shape (from tensorflow.python.feature_column.feature_column_v2) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "The old _FeatureColumn APIs are being deprecated. Please use the new FeatureColumn APIs instead.\n",
      "WARNING:tensorflow:From /home/luban/miniconda3/envs/zt/lib/python3.7/site-packages/tensorflow_core/python/feature_column/feature_column_v2.py:4326: SequenceCategoricalColumn._num_buckets (from tensorflow.python.feature_column.feature_column_v2) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "The old _FeatureColumn APIs are being deprecated. Please use the new FeatureColumn APIs instead.\n",
      "WARNING:tensorflow:From /home/luban/miniconda3/envs/zt/lib/python3.7/site-packages/tensorflow_core/python/feature_column/feature_column_v2.py:4525: IdentityCategoricalColumn._num_buckets (from tensorflow.python.feature_column.feature_column_v2) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "The old _FeatureColumn APIs are being deprecated. Please use the new FeatureColumn APIs instead.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(3, 4, 20)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "array([[[0., 1., 0., 0., 0., 0., 0., 1., 0., 1., 0., 1., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0.],\n",
       "        [0., 1., 0., 0., 0., 0., 0., 1., 0., 1., 0., 1., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0.]],\n",
       "\n",
       "       [[0., 1., 0., 0., 0., 0., 0., 1., 0., 1., 0., 1., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0.],\n",
       "        [0., 1., 0., 0., 0., 0., 0., 1., 0., 1., 0., 1., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0.]],\n",
       "\n",
       "       [[0., 1., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 1., 0.,\n",
       "         0., 0., 0., 0.],\n",
       "        [0., 1., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 1., 0.,\n",
       "         0., 0., 0., 0.],\n",
       "        [0., 1., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 1., 0.,\n",
       "         0., 0., 0., 0.],\n",
       "        [0., 1., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 1., 0.,\n",
       "         0., 0., 0., 0.]]], dtype=float32)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# feature_column解析\n",
    "_catFeats_fc = fc.sequence_categorical_column_with_identity(\"cat_feats\", num_buckets=20)\n",
    "catFeats_fc = fc.indicator_column(_catFeats_fc)\n",
    "\n",
    "\n",
    "# catFeats = fc.input_layer(features, fc.indicator_column(catFeats_fc))\n",
    "catFeats,catFeats_len = tf.contrib.feature_column.sequence_input_layer(features, [catFeats_fc])\n",
    "# catFeats,catFeats_len=tf.keras.experimental.SequenceFeatures(catFeats_fc)(features)\n",
    "# catFeats = fc.linear_model(features,_catFeats_fc,units=128)\n",
    "\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "with tf.Session(config=config) as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    catFeats=sess.run(catFeats)\n",
    "    catFeats.shape\n",
    "    catFeats\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### parse fc | sequence_numeric_column 尝试\n",
    "- input_layer | 报错 \n",
    "    - *AttributeError: 'SequenceNumericColumn' object has no attribute '_get_sparse_tensors'*\n",
    "- sequence_input_layer | 报错 \n",
    "    - *ValueError: Items of feature_columns must be a _FeatureColumn. Given (type <class 'tensorflow.python.feature_column.sequence_feature_column.SequenceNumericColumn'>): SequenceNumericColumn(key='num_feats', shape=(1,), default_value=0.0, dtype=tf.float32, normalizer_fn=None).*\n",
    "\n",
    "- SequenceFeatures | 报错 \n",
    "    - *TypeError: Input must be a SparseTensor.*\n",
    "- linear_model | 报错 \n",
    "    - *ValueError: Items of feature_columns must be a _FeatureColumn. Given (type <class 'str'>): num_feats.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-15T06:53:54.280439Z",
     "start_time": "2021-05-15T06:53:54.249969Z"
    }
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "in converted code:\n    relative to /home/luban/miniconda3/envs/zt/lib/python3.7/site-packages/tensorflow_core/python:\n\n    feature_column/sequence_feature_column.py:144 call\n        transformation_cache, self._state_manager)\n    feature_column/sequence_feature_column.py:559 get_sequence_dense_tensor\n        sp_tensor, default_value=self.default_value)\n    ops/sparse_ops.py:1488 sparse_tensor_to_dense\n        sp_input = _convert_to_sparse_tensor(sp_input)\n    ops/sparse_ops.py:69 _convert_to_sparse_tensor\n        raise TypeError(\"Input must be a SparseTensor.\")\n\n    TypeError: Input must be a SparseTensor.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-628e85672504>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# feats = fc.input_layer(features, fc.indicator_column(feats_fc))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# feats,feats_len = tf.contrib.feature_column.sequence_input_layer(features, [feats_fc])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mfeats\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfeats_len\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSequenceFeatures\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeats_fc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;31m# feats = fc.linear_model(features,feats_fc,units=128)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/zt/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    852\u001b[0m                     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbase_layer_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmark_as_return\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0macd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    853\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 854\u001b[0;31m                   \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcall_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcast_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    855\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    856\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOperatorNotAllowedInGraphError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/zt/lib/python3.7/site-packages/tensorflow_core/python/autograph/impl/api.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    235\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint:disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'ag_error_metadata'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 237\u001b[0;31m           \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    238\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    239\u001b[0m           \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: in converted code:\n    relative to /home/luban/miniconda3/envs/zt/lib/python3.7/site-packages/tensorflow_core/python:\n\n    feature_column/sequence_feature_column.py:144 call\n        transformation_cache, self._state_manager)\n    feature_column/sequence_feature_column.py:559 get_sequence_dense_tensor\n        sp_tensor, default_value=self.default_value)\n    ops/sparse_ops.py:1488 sparse_tensor_to_dense\n        sp_input = _convert_to_sparse_tensor(sp_input)\n    ops/sparse_ops.py:69 _convert_to_sparse_tensor\n        raise TypeError(\"Input must be a SparseTensor.\")\n\n    TypeError: Input must be a SparseTensor.\n"
     ]
    }
   ],
   "source": [
    "# feature_column解析\n",
    "feats_fc = fc.sequence_numeric_column(\"num_feats\")\n",
    "\n",
    "\n",
    "# feats = fc.input_layer(features, fc.indicator_column(feats_fc))\n",
    "# feats,feats_len = tf.contrib.feature_column.sequence_input_layer(features, [feats_fc])\n",
    "feats,feats_len=tf.keras.experimental.SequenceFeatures(feats_fc)(features)\n",
    "# feats = fc.linear_model(features,feats_fc,units=128)\n",
    "\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "with tf.Session(config=config) as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    catFeats=sess.run(feats)\n",
    "    catFeats.shape\n",
    "    catFeats\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### parse fc | 折中方案\n",
    "- categorical继续使用sequence_categorical_column_with_identity解析，使用sequence_input_layer拿输入\n",
    "- 用 numeric_column 来解析feature，使用 input_layer 触发解析然后reshape\n",
    "- 最后两部分concat起来\n",
    "\n",
    "注：问题在于`sequence_input_layer`是dense的，如果这里有id类特征导致num_buckets太大会OOM\n",
    "- 不能对它单独使用`linear_model` 这样就缺少 categorical和numeric 特征之间的交互了\n",
    "    - 也不是完全不行？categorical特征通过MLP得到一个emb，然后直接和numeric特征进行concat似乎也是合理的？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-15T06:54:08.319249Z",
     "start_time": "2021-05-15T06:54:07.985790Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/luban/miniconda3/envs/zt/lib/python3.7/site-packages/tensorflow_core/python/feature_column/feature_column.py:206: NumericColumn._get_dense_tensor (from tensorflow.python.feature_column.feature_column_v2) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "The old _FeatureColumn APIs are being deprecated. Please use the new FeatureColumn APIs instead.\n",
      "WARNING:tensorflow:From /home/luban/miniconda3/envs/zt/lib/python3.7/site-packages/tensorflow_core/python/feature_column/feature_column.py:2158: NumericColumn._transform_feature (from tensorflow.python.feature_column.feature_column_v2) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "The old _FeatureColumn APIs are being deprecated. Please use the new FeatureColumn APIs instead.\n",
      "WARNING:tensorflow:From /home/luban/miniconda3/envs/zt/lib/python3.7/site-packages/tensorflow_core/python/feature_column/feature_column.py:207: NumericColumn._variable_shape (from tensorflow.python.feature_column.feature_column_v2) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "The old _FeatureColumn APIs are being deprecated. Please use the new FeatureColumn APIs instead.\n",
      "WARNING:tensorflow:From <ipython-input-13-a81f56a6d885>:12: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.Dense instead.\n",
      "WARNING:tensorflow:From /home/luban/miniconda3/envs/zt/lib/python3.7/site-packages/tensorflow_core/python/layers/core.py:187: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `layer.__call__` method instead.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(3, 4, 20)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(3, 4, 3)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(3, 4, 23)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "array([[[ 0. ,  1. ,  0. ,  0. ,  0. ,  0. ,  0. ,  1. ,  0. ,  1. ,\n",
       "          0. ,  1. ,  0. ,  0. ,  0. ,  0. ,  0. ,  0. ,  0. ,  0. ,\n",
       "         32. , 19.5,  7. ],\n",
       "        [ 0. ,  1. ,  0. ,  0. ,  0. ,  0. ,  0. ,  1. ,  0. ,  1. ,\n",
       "          0. ,  1. ,  0. ,  0. ,  0. ,  0. ,  0. ,  0. ,  0. ,  0. ,\n",
       "         32. , 19.5,  7. ],\n",
       "        [ 0. ,  0. ,  0. ,  0. ,  0. ,  0. ,  0. ,  0. ,  0. ,  0. ,\n",
       "          0. ,  0. ,  0. ,  0. ,  0. ,  0. ,  0. ,  0. ,  0. ,  0. ,\n",
       "          0. ,  0. ,  0. ],\n",
       "        [ 0. ,  0. ,  0. ,  0. ,  0. ,  0. ,  0. ,  0. ,  0. ,  0. ,\n",
       "          0. ,  0. ,  0. ,  0. ,  0. ,  0. ,  0. ,  0. ,  0. ,  0. ,\n",
       "          0. ,  0. ,  0. ]],\n",
       "\n",
       "       [[ 0. ,  1. ,  0. ,  0. ,  0. ,  0. ,  0. ,  1. ,  0. ,  1. ,\n",
       "          0. ,  1. ,  0. ,  0. ,  0. ,  0. ,  0. ,  0. ,  0. ,  0. ,\n",
       "         32. , 19.5,  7. ],\n",
       "        [ 0. ,  1. ,  0. ,  0. ,  0. ,  0. ,  0. ,  1. ,  0. ,  1. ,\n",
       "          0. ,  1. ,  0. ,  0. ,  0. ,  0. ,  0. ,  0. ,  0. ,  0. ,\n",
       "         32. , 19.5,  7. ],\n",
       "        [ 0. ,  0. ,  0. ,  0. ,  0. ,  0. ,  0. ,  0. ,  0. ,  0. ,\n",
       "          0. ,  0. ,  0. ,  0. ,  0. ,  0. ,  0. ,  0. ,  0. ,  0. ,\n",
       "          0. ,  0. ,  0. ],\n",
       "        [ 0. ,  0. ,  0. ,  0. ,  0. ,  0. ,  0. ,  0. ,  0. ,  0. ,\n",
       "          0. ,  0. ,  0. ,  0. ,  0. ,  0. ,  0. ,  0. ,  0. ,  0. ,\n",
       "          0. ,  0. ,  0. ]],\n",
       "\n",
       "       [[ 0. ,  1. ,  0. ,  0. ,  0. ,  0. ,  0. ,  0. ,  1. ,  1. ,\n",
       "          0. ,  0. ,  0. ,  0. ,  1. ,  0. ,  0. ,  0. ,  0. ,  0. ,\n",
       "         32. , 19.5,  7. ],\n",
       "        [ 0. ,  1. ,  0. ,  0. ,  0. ,  0. ,  0. ,  0. ,  1. ,  1. ,\n",
       "          0. ,  0. ,  0. ,  0. ,  1. ,  0. ,  0. ,  0. ,  0. ,  0. ,\n",
       "         32. , 19.5,  7. ],\n",
       "        [ 0. ,  1. ,  0. ,  0. ,  0. ,  0. ,  0. ,  0. ,  1. ,  1. ,\n",
       "          0. ,  0. ,  0. ,  0. ,  1. ,  0. ,  0. ,  0. ,  0. ,  0. ,\n",
       "         32. , 19.5,  7. ],\n",
       "        [ 0. ,  1. ,  0. ,  0. ,  0. ,  0. ,  0. ,  0. ,  1. ,  1. ,\n",
       "          0. ,  0. ,  0. ,  0. ,  1. ,  0. ,  0. ,  0. ,  0. ,  0. ,\n",
       "         32. , 19.5,  7. ]]], dtype=float32)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(3, 4, 128)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "array([[[-3.519066 , -3.1454854, -7.773138 , ...,  2.312889 ,\n",
       "          4.77011  ,  2.6220818],\n",
       "        [-3.519066 , -3.1454854, -7.773138 , ...,  2.312889 ,\n",
       "          4.77011  ,  2.6220818],\n",
       "        [ 0.       ,  0.       ,  0.       , ...,  0.       ,\n",
       "          0.       ,  0.       ],\n",
       "        [ 0.       ,  0.       ,  0.       , ...,  0.       ,\n",
       "          0.       ,  0.       ]],\n",
       "\n",
       "       [[-3.519066 , -3.1454854, -7.773138 , ...,  2.312889 ,\n",
       "          4.77011  ,  2.6220818],\n",
       "        [-3.519066 , -3.1454854, -7.773138 , ...,  2.312889 ,\n",
       "          4.77011  ,  2.6220818],\n",
       "        [ 0.       ,  0.       ,  0.       , ...,  0.       ,\n",
       "          0.       ,  0.       ],\n",
       "        [ 0.       ,  0.       ,  0.       , ...,  0.       ,\n",
       "          0.       ,  0.       ]],\n",
       "\n",
       "       [[-3.4586208, -3.118774 , -7.7927527, ...,  2.2993925,\n",
       "          4.6397986,  2.749948 ],\n",
       "        [-3.4586208, -3.118774 , -7.7927527, ...,  2.2993925,\n",
       "          4.6397986,  2.749948 ],\n",
       "        [-3.4586208, -3.118774 , -7.7927527, ...,  2.2993925,\n",
       "          4.6397986,  2.749948 ],\n",
       "        [-3.4586208, -3.118774 , -7.7927527, ...,  2.2993925,\n",
       "          4.6397986,  2.749948 ]]], dtype=float32)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "catFeats_fc = fc.sequence_categorical_column_with_identity(\"cat_feats\", num_buckets=20)\n",
    "catFeats_fc = fc.indicator_column(catFeats_fc)\n",
    "# numFeats_fc = fc.sequence_numeric_column(\"num_feats\")\n",
    "numFeats_fc = fc.numeric_column(\"num_feats\",shape=(4,3))\n",
    "\n",
    "\n",
    "# inp=fc.input_layer(features,[numFeats_fc])\n",
    "cat_inp,cat_len=tf.contrib.feature_column.sequence_input_layer(features,[catFeats_fc])\n",
    "num_inp=fc.input_layer(features,[numFeats_fc])\n",
    "num_inp=tf.reshape(num_inp,(-1,4,3))\n",
    "inp=tf.concat([cat_inp,num_inp], axis=2)\n",
    "emb=tf.layers.dense(inp,128)\n",
    "\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "with tf.Session(config=config) as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    cat_inp,num_inp,inp,emb=sess.run([cat_inp,num_inp,inp,emb])\n",
    "    cat_inp.shape\n",
    "    num_inp.shape\n",
    "    inp.shape\n",
    "    inp\n",
    "    emb.shape\n",
    "    emb\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### parse fc | 其实完全不用feature_column来帮我解析\n",
    "- 这个需要在dataset生成时指定好`output_shapes=`这个参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-08T11:58:33.139871Z",
     "start_time": "2021-06-08T11:58:32.892849Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1,  2, -1, -1],\n",
       "       [ 1,  2, -1, -1],\n",
       "       [ 1,  2,  3,  4]], dtype=int32)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "array([[[ 1],\n",
       "        [ 2],\n",
       "        [-1],\n",
       "        [-1]],\n",
       "\n",
       "       [[ 1],\n",
       "        [ 2],\n",
       "        [-1],\n",
       "        [-1]],\n",
       "\n",
       "       [[ 1],\n",
       "        [ 2],\n",
       "        [ 3],\n",
       "        [ 4]]], dtype=int32)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "array([[[[0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 1., 0., 0., 0., 0., 0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]],\n",
       "\n",
       "\n",
       "       [[[0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 1., 0., 0., 0., 0., 0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]],\n",
       "\n",
       "\n",
       "       [[[0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 1., 0., 0., 0., 0., 0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0., 1., 0., 0., 0., 0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0., 0., 1., 0., 0., 0., 0., 0.]]]], dtype=float32)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "array([[[0., 1., 0., 0., 0., 0., 0., 1., 0., 1., 0., 1., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0.],\n",
       "        [0., 1., 0., 0., 0., 0., 0., 1., 0., 1., 0., 1., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0.]],\n",
       "\n",
       "       [[0., 1., 0., 0., 0., 0., 0., 1., 0., 1., 0., 1., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0.],\n",
       "        [0., 1., 0., 0., 0., 0., 0., 1., 0., 1., 0., 1., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0.]],\n",
       "\n",
       "       [[0., 1., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 1., 0.,\n",
       "         0., 0., 0., 0.],\n",
       "        [0., 1., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 1., 0.,\n",
       "         0., 0., 0., 0.],\n",
       "        [0., 1., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 1., 0.,\n",
       "         0., 0., 0., 0.],\n",
       "        [0., 1., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 1., 0.,\n",
       "         0., 0., 0., 0.]]], dtype=float32)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "with tf.Session(config=config) as sess:\n",
    "    sess.run(features['uid_feats'])\n",
    "    sess.run(tf.expand_dims(features['uid_feats'], axis=-1))\n",
    "    sess.run(tf.one_hot(tf.expand_dims(features['uid_feats'], axis=-1), depth=10))\n",
    "    \n",
    "    sess.run(cat_inp)\n",
    "#     tf.concat([cat_inp])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-26T12:38:12.718791Z",
     "start_time": "2021-05-26T12:38:12.357375Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1,  2, -1, -1],\n",
       "       [ 1,  2, -1, -1],\n",
       "       [ 1,  2,  3,  4]], dtype=int32)"
      ]
     },
     "execution_count": 372,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> num_inp:(3, 4, 3)\n",
      "[[[32.  19.5  7. ]\n",
      "  [32.  19.5  7. ]\n",
      "  [ 0.   0.   0. ]\n",
      "  [ 0.   0.   0. ]]\n",
      "\n",
      " [[32.  19.5  7. ]\n",
      "  [32.  19.5  7. ]\n",
      "  [ 0.   0.   0. ]\n",
      "  [ 0.   0.   0. ]]\n",
      "\n",
      " [[32.  19.5  7. ]\n",
      "  [32.  19.5  7. ]\n",
      "  [32.  19.5  7. ]\n",
      "  [32.  19.5  7. ]]]\n",
      ">>> inp:(3, 4, 31)\n",
      "[[[ 0.9291465   0.47468472  0.15764785 -0.0419147  -0.6017256\n",
      "   -0.47550154 -0.5961838  -0.84414077  0.          1.\n",
      "    0.          0.          0.          0.          0.\n",
      "    1.          0.          1.          0.          1.\n",
      "    0.          0.          0.          0.          0.\n",
      "    0.          0.          0.         32.         19.5\n",
      "    7.        ]\n",
      "  [ 0.16315103 -0.5933628   0.772671    0.31530237  0.68798256\n",
      "   -0.9229078   0.0861187   0.6351516   0.          1.\n",
      "    0.          0.          0.          0.          0.\n",
      "    1.          0.          1.          0.          1.\n",
      "    0.          0.          0.          0.          0.\n",
      "    0.          0.          0.         32.         19.5\n",
      "    7.        ]\n",
      "  [-0.7564547  -0.8110666   0.71070194  0.54040265  0.23297834\n",
      "   -0.6433182  -0.25504756  0.20908785  0.          0.\n",
      "    0.          0.          0.          0.          0.\n",
      "    0.          0.          0.          0.          0.\n",
      "    0.          0.          0.          0.          0.\n",
      "    0.          0.          0.          0.          0.\n",
      "    0.        ]\n",
      "  [-0.7564547  -0.8110666   0.71070194  0.54040265  0.23297834\n",
      "   -0.6433182  -0.25504756  0.20908785  0.          0.\n",
      "    0.          0.          0.          0.          0.\n",
      "    0.          0.          0.          0.          0.\n",
      "    0.          0.          0.          0.          0.\n",
      "    0.          0.          0.          0.          0.\n",
      "    0.        ]]\n",
      "\n",
      " [[ 0.9291465   0.47468472  0.15764785 -0.0419147  -0.6017256\n",
      "   -0.47550154 -0.5961838  -0.84414077  0.          1.\n",
      "    0.          0.          0.          0.          0.\n",
      "    1.          0.          1.          0.          1.\n",
      "    0.          0.          0.          0.          0.\n",
      "    0.          0.          0.         32.         19.5\n",
      "    7.        ]\n",
      "  [ 0.16315103 -0.5933628   0.772671    0.31530237  0.68798256\n",
      "   -0.9229078   0.0861187   0.6351516   0.          1.\n",
      "    0.          0.          0.          0.          0.\n",
      "    1.          0.          1.          0.          1.\n",
      "    0.          0.          0.          0.          0.\n",
      "    0.          0.          0.         32.         19.5\n",
      "    7.        ]\n",
      "  [-0.7564547  -0.8110666   0.71070194  0.54040265  0.23297834\n",
      "   -0.6433182  -0.25504756  0.20908785  0.          0.\n",
      "    0.          0.          0.          0.          0.\n",
      "    0.          0.          0.          0.          0.\n",
      "    0.          0.          0.          0.          0.\n",
      "    0.          0.          0.          0.          0.\n",
      "    0.        ]\n",
      "  [-0.7564547  -0.8110666   0.71070194  0.54040265  0.23297834\n",
      "   -0.6433182  -0.25504756  0.20908785  0.          0.\n",
      "    0.          0.          0.          0.          0.\n",
      "    0.          0.          0.          0.          0.\n",
      "    0.          0.          0.          0.          0.\n",
      "    0.          0.          0.          0.          0.\n",
      "    0.        ]]\n",
      "\n",
      " [[ 0.9291465   0.47468472  0.15764785 -0.0419147  -0.6017256\n",
      "   -0.47550154 -0.5961838  -0.84414077  0.          1.\n",
      "    0.          0.          0.          0.          0.\n",
      "    0.          1.          1.          0.          0.\n",
      "    0.          0.          1.          0.          0.\n",
      "    0.          0.          0.         32.         19.5\n",
      "    7.        ]\n",
      "  [ 0.16315103 -0.5933628   0.772671    0.31530237  0.68798256\n",
      "   -0.9229078   0.0861187   0.6351516   0.          1.\n",
      "    0.          0.          0.          0.          0.\n",
      "    0.          1.          1.          0.          0.\n",
      "    0.          0.          1.          0.          0.\n",
      "    0.          0.          0.         32.         19.5\n",
      "    7.        ]\n",
      "  [ 0.7615566   0.65396404 -0.28796387  0.6787355  -0.29472756\n",
      "   -0.314317    0.26738906 -0.19833302  0.          1.\n",
      "    0.          0.          0.          0.          0.\n",
      "    0.          1.          1.          0.          0.\n",
      "    0.          0.          1.          0.          0.\n",
      "    0.          0.          0.         32.         19.5\n",
      "    7.        ]\n",
      "  [ 0.1338687  -0.81982684  0.9213798  -0.9709532  -0.53457236\n",
      "   -0.90581775  0.88344765  0.3083954   0.          1.\n",
      "    0.          0.          0.          0.          0.\n",
      "    0.          1.          1.          0.          0.\n",
      "    0.          0.          1.          0.          0.\n",
      "    0.          0.          0.         32.         19.5\n",
      "    7.        ]]]\n",
      ">>> inp2emb:(3, 4, 128)\n",
      "[[[ 0.831114    7.960811   -4.397321   ...  0.01421908 -2.748563\n",
      "    2.4690137 ]\n",
      "  [ 0.38526008  7.37501    -4.945638   ...  0.18105727 -2.7949758\n",
      "    2.5445213 ]\n",
      "  [-0.08284762 -0.27221727 -0.01876148 ...  0.01654921  0.15973215\n",
      "    0.02009044]\n",
      "  [-0.08284762 -0.27221727 -0.01876148 ...  0.01654921  0.15973215\n",
      "    0.02009044]]\n",
      "\n",
      " [[ 0.831114    7.960811   -4.397321   ...  0.01421908 -2.748563\n",
      "    2.4690137 ]\n",
      "  [ 0.38526008  7.37501    -4.945638   ...  0.18105727 -2.7949758\n",
      "    2.5445213 ]\n",
      "  [-0.08284762 -0.27221727 -0.01876148 ...  0.01654921  0.15973215\n",
      "    0.02009044]\n",
      "  [-0.08284762 -0.27221727 -0.01876148 ...  0.01654921  0.15973215\n",
      "    0.02009044]]\n",
      "\n",
      " [[ 0.8340234   8.013685   -4.350351   ... -0.36945638 -2.5131507\n",
      "    2.711156  ]\n",
      "  [ 0.3881695   7.4278846  -4.898668   ... -0.20261821 -2.5595634\n",
      "    2.7866635 ]\n",
      "  [ 0.675512    7.9197373  -4.7212787  ... -0.41954955 -2.777935\n",
      "    2.7383828 ]\n",
      "  [ 0.7047717   7.43862    -4.7474666  ...  0.06370869 -2.3908405\n",
      "    2.8009143 ]]]\n",
      ">>> uid_emb:(3, 4, 8)\n",
      "[[[ 0.9291465   0.47468472  0.15764785 -0.0419147  -0.6017256\n",
      "   -0.47550154 -0.5961838  -0.84414077]\n",
      "  [ 0.16315103 -0.5933628   0.772671    0.31530237  0.68798256\n",
      "   -0.9229078   0.0861187   0.6351516 ]\n",
      "  [-0.7564547  -0.8110666   0.71070194  0.54040265  0.23297834\n",
      "   -0.6433182  -0.25504756  0.20908785]\n",
      "  [-0.7564547  -0.8110666   0.71070194  0.54040265  0.23297834\n",
      "   -0.6433182  -0.25504756  0.20908785]]\n",
      "\n",
      " [[ 0.9291465   0.47468472  0.15764785 -0.0419147  -0.6017256\n",
      "   -0.47550154 -0.5961838  -0.84414077]\n",
      "  [ 0.16315103 -0.5933628   0.772671    0.31530237  0.68798256\n",
      "   -0.9229078   0.0861187   0.6351516 ]\n",
      "  [-0.7564547  -0.8110666   0.71070194  0.54040265  0.23297834\n",
      "   -0.6433182  -0.25504756  0.20908785]\n",
      "  [-0.7564547  -0.8110666   0.71070194  0.54040265  0.23297834\n",
      "   -0.6433182  -0.25504756  0.20908785]]\n",
      "\n",
      " [[ 0.9291465   0.47468472  0.15764785 -0.0419147  -0.6017256\n",
      "   -0.47550154 -0.5961838  -0.84414077]\n",
      "  [ 0.16315103 -0.5933628   0.772671    0.31530237  0.68798256\n",
      "   -0.9229078   0.0861187   0.6351516 ]\n",
      "  [ 0.7615566   0.65396404 -0.28796387  0.6787355  -0.29472756\n",
      "   -0.314317    0.26738906 -0.19833302]\n",
      "  [ 0.1338687  -0.81982684  0.9213798  -0.9709532  -0.53457236\n",
      "   -0.90581775  0.88344765  0.3083954 ]]]\n",
      ">>> uid_feats:(3, 4)\n",
      "[[2 3 0 0]\n",
      " [2 3 0 0]\n",
      " [2 3 4 5]]\n"
     ]
    }
   ],
   "source": [
    "catFeats_fc = fc.sequence_categorical_column_with_identity(\"cat_feats\", num_buckets=20)\n",
    "catFeats_fc = fc.indicator_column(catFeats_fc)\n",
    "cat_inp,cat_len=tf.contrib.feature_column.sequence_input_layer(features,[catFeats_fc])\n",
    "\n",
    "\n",
    "# uid_emb = tf.keras.layers.Embedding(25000,8,name=\"uid_emb\")(features['uid_feats']+1)\n",
    "from tensorflow.python.ops import embedding_ops\n",
    "with tf.variable_scope(\"uid_emb\", reuse=tf.AUTO_REUSE) as emb_scope:\n",
    "    emb_table = tf.get_variable(\"emb\", initializer=tf.random_uniform([25*1000,8],minval=-1,maxval=1))\n",
    "    uid_emb = embedding_ops.embedding_lookup(emb_table, features['uid_feats']+1)\n",
    "    \n",
    "\n",
    "\n",
    "num_inp=features['num_feats']\n",
    "inp=tf.concat([uid_emb,cat_inp,num_inp], axis=2)\n",
    "inp2emb=tf.layers.dense(inp,128)\n",
    "\n",
    "\n",
    "\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "with tf.Session(config=config) as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    cat_inp,num_inp,inp,inp2emb,uid_emb,uid_feats=sess.run([cat_inp,num_inp,inp,inp2emb,uid_emb,features['uid_feats']+1])\n",
    "    sess.run(features['uid_feats'])\n",
    "    print(\">>> num_inp:%s\\n%s\" %(str(num_inp.shape), str(num_inp)))\n",
    "    print(\">>> inp:%s\\n%s\" %(str(inp.shape), str(inp)))\n",
    "    print(\">>> inp2emb:%s\\n%s\" %(str(inp2emb.shape), str(inp2emb)))\n",
    "    print(\">>> uid_emb:%s\\n%s\" %(str(uid_emb.shape), str(uid_emb)))\n",
    "    print(\">>> uid_feats:%s\\n%s\" %(str(uid_feats.shape), str(uid_feats)))\n",
    "    \n",
    "\n",
    "#     inp.shape\n",
    "#     inpb\n",
    "#     emb.shape\n",
    "#     emb\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-26T12:34:23.103318Z",
     "start_time": "2021-05-26T12:34:23.098941Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['uid_emb/emb:0',\n",
       " 'dense/kernel:0',\n",
       " 'dense/bias:0',\n",
       " 'dense_1/kernel:0',\n",
       " 'dense_1/bias:0',\n",
       " 'dense_2/kernel:0',\n",
       " 'dense_2/bias:0']"
      ]
     },
     "execution_count": 365,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[i.name for i in tf.trainable_variables()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### onehot-idx to sparseTens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-08T12:57:29.667151Z",
     "start_time": "2021-06-08T12:57:29.585968Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0],\n",
       "       [0, 1],\n",
       "       [1, 0],\n",
       "       [1, 2],\n",
       "       [2, 1],\n",
       "       [2, 2],\n",
       "       [2, 3],\n",
       "       [2, 4]])"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "array([1., 1., 1., 1., 1., 1., 1., 1.], dtype=float32)"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "array([[1., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [1., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 1., 1., 1., 1., 0., 0., 0., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# help(tf.sparse_matmul)\n",
    "# help(tf.sparse.SparseTensor) | indices 2-D, values 1-D, dense_shape\n",
    "\n",
    "sess = tf.InteractiveSession()\n",
    "# features['uid_feats'].eval()\n",
    "\n",
    "a=np.array([[ 0,  1, -1, -1],\n",
    "            [ 0,  2, -1, -1],\n",
    "            [ 1,  2,  3,  4]])\n",
    "features={\"uid_feats\": tf.convert_to_tensor(a)}\n",
    "\n",
    "\n",
    "nonzero_idx = tf.where(features['uid_feats'] >= 0)\n",
    "row_idx = tf.cast(tf.gather(nonzero_idx, 0, axis=1), tf.int64)\n",
    "col_idx = tf.gather_nd(features['uid_feats'], nonzero_idx)\n",
    "indices = tf.cast(tf.transpose(tf.stack([row_idx,col_idx])), tf.int64)\n",
    "indices.eval()\n",
    "values = tf.ones(tf.shape(indices)[0])\n",
    "values.eval()\n",
    "sp = tf.sparse.SparseTensor(indices, values, dense_shape=(tf.shape(features['uid_feats'])[0], 10))\n",
    "tf.sparse_to_dense(sp.indices, sp.dense_shape, sp.values).eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### try RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-27T11:11:31.533722Z",
     "start_time": "2021-05-27T11:11:31.236068Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "##############\n",
    "# MLP for emb\n",
    "##############\n",
    "catFeats_fc = fc.sequence_categorical_column_with_identity(\"cat_feats\", num_buckets=20)\n",
    "catFeats_fc = fc.indicator_column(catFeats_fc)\n",
    "cat_inp,cat_len=tf.contrib.feature_column.sequence_input_layer(features,[catFeats_fc])\n",
    "num_inp=features['num_feats']\n",
    "\n",
    "inp=tf.concat([cat_inp,num_inp], axis=2)\n",
    "emb=tf.layers.dense(inp,128)\n",
    "\n",
    "#######\n",
    "# LSTM\n",
    "#######\n",
    "batch_size=batch_size\n",
    "hidden_size = [16]\n",
    "def make_cell(hidden_size:int,is_training:bool=True, keep_prob:float=0.8):\n",
    "    cell = tf.contrib.rnn.LSTMBlockCell(hidden_size, forget_bias=0.0)\n",
    "    if is_training and keep_prob < 1:\n",
    "        cell = tf.contrib.rnn.DropoutWrapper(cell, output_keep_prob=keep_prob)\n",
    "    return cell\n",
    "\n",
    "with tf.variable_scope(\"rnn\", reuse=tf.AUTO_REUSE):\n",
    "    mlstm_cell = tf.contrib.rnn.MultiRNNCell([make_cell(i) for i in hidden_size], state_is_tuple=True)\n",
    "    init_state = mlstm_cell.zero_state(tf.shape(emb)[0], dtype=tf.float32)\n",
    "    outputs, final_state = tf.nn.dynamic_rnn(mlstm_cell, inputs=emb, initial_state=init_state, time_major=False)\n",
    "    h_state=final_state[0].h\n",
    "    preds = tf.layers.dense(outputs,1)\n",
    "loss = tf.reduce_sum(tf.square(tf.subtract(preds, labels)))\n",
    "loss = tf.pow(loss, 0.5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-20T09:26:01.981528Z",
     "start_time": "2021-05-20T09:26:01.915716Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([Dimension(None), Dimension(4), Dimension(16)])"
      ]
     },
     "execution_count": 258,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'dense_17/BiasAdd:0' shape=(?, 4, 1) dtype=float32>"
      ]
     },
     "execution_count": 258,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'SequenceMask_14/Cast_1:0' shape=(?, ?) dtype=float32>"
      ]
     },
     "execution_count": 258,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'Mul_20:0' shape=<unknown> dtype=float32>"
      ]
     },
     "execution_count": 258,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'IteratorGetNext:3' shape=(?, 4) dtype=float32>"
      ]
     },
     "execution_count": 258,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'Mean_7:0' shape=<unknown> dtype=float32>"
      ]
     },
     "execution_count": 258,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs.shape\n",
    "pred_seq = tf.layers.dense(outputs,1)\n",
    "seq_mask = tf.sequence_mask(features['seq_len'], maxlen=tf.shape(features['num_feats'])[1], dtype=tf.float32)\n",
    "pred_seq_masked = tf.math.multiply(tf.squeeze(pred_seq), seq_mask)\n",
    "seq_weights = tf.ones_like(features[\"leader_atas\"])\n",
    "loss_each = tf.abs(pred_seq_masked - features[\"leader_atas\"])\n",
    "loss_each = tf.math.multiply(seq_weights, loss_each)\n",
    "loss_seq = tf.reduce_mean(loss_each,axis=1)\n",
    "pred_seq\n",
    "seq_mask\n",
    "pred_seq_masked\n",
    "features[\"leader_atas\"]\n",
    "loss_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-27T11:19:23.030497Z",
     "start_time": "2021-05-27T11:19:23.022747Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-402-690013122085>:1: calling reverse_sequence (from tensorflow.python.ops.array_ops) with seq_dim is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "seq_dim is deprecated, use seq_axis instead\n",
      "WARNING:tensorflow:From /home/luban/miniconda3/envs/zt/lib/python3.7/site-packages/tensorflow_core/python/util/deprecation.py:507: calling reverse_sequence (from tensorflow.python.ops.array_ops) with batch_dim is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "batch_dim is deprecated, use batch_axis instead\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'ReverseSequence:0' shape=(?, 4, 1) dtype=float32>"
      ]
     },
     "execution_count": 402,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-27T11:32:49.146206Z",
     "start_time": "2021-05-27T11:32:49.080943Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([ 2, 13]), array([ 2, 15]), array([0.        , 0.13333333])]"
      ]
     },
     "execution_count": 409,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a=[[1,2,3,4],[11,12,13,14]]\n",
    "b=[[1,2,0,0],[11,12,15,0]]\n",
    "a=np.array(a)\n",
    "b=np.array(b)\n",
    "\n",
    "the_seq_len = [2,3]\n",
    "def get_last_by_seqLen(inp_seq, seq_len):\n",
    "    res = tf.reverse_sequence(inp_seq, seq_len, seq_dim=1, batch_dim=0)\n",
    "    res = tf.squeeze(tf.gather(res, [0], axis=1))\n",
    "    return res\n",
    "\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "with tf.Session(config=config) as sess:\n",
    "    last_a = get_last_by_seqLen(a,the_seq_len)\n",
    "    last_b = get_last_by_seqLen(b,the_seq_len)\n",
    "    sess.run([last_a,last_b, tf.abs(last_a-last_b)/last_b])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-20T09:26:03.391830Z",
     "start_time": "2021-05-20T09:26:02.915472Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[-0.21913388],\n",
       "        [-0.59902596],\n",
       "        [-0.42556602],\n",
       "        [-0.19575995]],\n",
       "\n",
       "       [[-0.25491977],\n",
       "        [-0.09046508],\n",
       "        [-0.47043914],\n",
       "        [-0.25141543]],\n",
       "\n",
       "       [[-0.31801993],\n",
       "        [-0.83017725],\n",
       "        [-0.44796383],\n",
       "        [-0.83715534]]], dtype=float32)"
      ]
     },
     "execution_count": 259,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "array([[-0.21913388, -0.59902596, -0.        , -0.        ],\n",
       "       [-0.25491977, -0.09046508, -0.        , -0.        ],\n",
       "       [-0.31801993, -0.83017725, -0.44796383, -0.83715534]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 259,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "array([[1200., 1400.,    0.,    0.],\n",
       "       [1200., 1400.,    0.,    0.],\n",
       "       [1600., 1900., 2100., 2800.]], dtype=float32)"
      ]
     },
     "execution_count": 259,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "array([[1200.2191, 1400.599 ,    0.    ,    0.    ],\n",
       "       [1200.2549, 1400.0905,    0.    ,    0.    ],\n",
       "       [1600.318 , 1900.8302, 2100.448 , 2800.8372]], dtype=float32)"
      ]
     },
     "execution_count": 259,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "array([ 650.2045,  650.0863, 2100.6084], dtype=float32)"
      ]
     },
     "execution_count": 259,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "with tf.Session(config=config) as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    res_pred_seq, res_pred_seq_masked,ata,res_loss_each,res_loss_seq=sess.run([pred_seq,pred_seq_masked, features[\"leader_atas\"], loss_each, loss_seq])\n",
    "    res_pred_seq\n",
    "    res_pred_seq_masked\n",
    "    ata\n",
    "    res_loss_each\n",
    "    res_loss_seq\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-20T09:34:42.085636Z",
     "start_time": "2021-05-20T09:34:42.079042Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1,  3,  6, 10],\n",
       "       [ 1,  2,  3,  3]])"
      ]
     },
     "execution_count": 273,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "array([[1, 2, 3, 4],\n",
       "       [2, 3, 4, 4]])"
      ]
     },
     "execution_count": 273,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k=[[1,2,3,4],\n",
    "   [1,1,1,0]]\n",
    "np.array(k).cumsum(axis=1)\n",
    "np.array(k).cumsum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-20T09:33:49.233382Z",
     "start_time": "2021-05-20T09:33:49.229138Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 3])"
      ]
     },
     "execution_count": 270,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(ata!=0).cumsum(axis=1).argmax(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-20T09:39:21.920626Z",
     "start_time": "2021-05-20T09:39:21.909135Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1200., 1400.,    0.,    0.],\n",
       "       [1200., 1400.,    0.,    0.],\n",
       "       [1600., 1900., 2100., 2800.]], dtype=float32)"
      ]
     },
     "execution_count": 287,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "array([1400., 1400., 2800.], dtype=float32)"
      ]
     },
     "execution_count": 287,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "array([[0, 1],\n",
       "       [1, 1],\n",
       "       [2, 3]])"
      ]
     },
     "execution_count": 287,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "array([1400., 1400., 2800.], dtype=float32)"
      ]
     },
     "execution_count": 287,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ata\n",
    "\n",
    "nonzero_idx=np.vstack([np.arange(ata.shape[0]),(ata!=0).cumsum(1).argmax(1)]).T\n",
    "nonzero_idx\n",
    "ata[nonzero_idx[:,0], nonzero_idx[:,1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-20T06:20:17.427411Z",
     "start_time": "2021-05-20T06:20:16.365787Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'Sum_37:0' shape=() dtype=float32>"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'Max_16:0' shape=() dtype=float32>"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'Sub_75:0' shape=() dtype=float32>"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "22591242.0"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "3610507.0"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "26201748.0"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "9.000516"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "-235.57588"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "22591242.0"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.reduce_sum(tf.square(tf.subtract(preds, labels)))\n",
    "# tf.math.minimum(tf.subtract(preds,labels))\n",
    "tf.reduce_max(tf.square(tf.subtract(preds, labels)))\n",
    "def reduce_var(x, axis=None, keepdims=False):\n",
    "    m = tf.reduce_mean(x, axis=axis, keep_dims=True)\n",
    "    devs_squared = tf.square(x - m)\n",
    "    return tf.reduce_mean(devs_squared, axis=axis, keep_dims=keepdims)\n",
    "def reduce_std(x, axis=None, keepdims=False):\n",
    "    return tf.sqrt(reduce_var(x, axis=axis, keepdims=keepdims))\n",
    "tf.subtract(reduce_std(preds),reduce_std(labels))\n",
    "\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "with tf.Session(config=config) as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    sess.run(tf.reduce_sum(tf.square(tf.subtract(preds, labels))))\n",
    "    sess.run(tf.reduce_max(tf.square(tf.subtract(preds, labels))))\n",
    "    sess.run(tf.reduce_sum(tf.square(tf.subtract(preds, labels))) + tf.reduce_max(tf.square(tf.subtract(preds, labels))))\n",
    "    sess.run(tf.reduce_sum(tf.square(tf.subtract(preds, labels)/labels)))\n",
    "    sess.run(tf.subtract(reduce_std(preds),reduce_std(labels)))\n",
    "    sess.run(tf.reduce_sum(tf.square(tf.subtract(preds, tf.squeeze(labels)) * tf.squeeze(w))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-19T13:35:25.635004Z",
     "start_time": "2021-05-19T13:35:25.630047Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'IteratorGetNext:6' shape=(?,) dtype=int32>"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(3, 4, 16)"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features['seq_len']\n",
    "outputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-19T13:33:42.217384Z",
     "start_time": "2021-05-19T13:33:41.813588Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 4, 20)"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(3, 4, 3)"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "array([[[32. , 19.5,  7. ],\n",
       "        [32. , 19.5,  7. ],\n",
       "        [ 0. ,  0. ,  0. ],\n",
       "        [ 0. ,  0. ,  0. ]],\n",
       "\n",
       "       [[32. , 19.5,  7. ],\n",
       "        [32. , 19.5,  7. ],\n",
       "        [ 0. ,  0. ,  0. ],\n",
       "        [ 0. ,  0. ,  0. ]],\n",
       "\n",
       "       [[32. , 19.5,  7. ],\n",
       "        [32. , 19.5,  7. ],\n",
       "        [32. , 19.5,  7. ],\n",
       "        [32. , 19.5,  7. ]]], dtype=float32)"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "array([[[ 0. ,  1. ,  0. ,  0. ,  0. ,  0. ,  0. ,  1. ,  0. ,  1. ,\n",
       "          0. ,  1. ,  0. ,  0. ,  0. ,  0. ,  0. ,  0. ,  0. ,  0. ,\n",
       "         32. , 19.5,  7. ],\n",
       "        [ 0. ,  1. ,  0. ,  0. ,  0. ,  0. ,  0. ,  1. ,  0. ,  1. ,\n",
       "          0. ,  1. ,  0. ,  0. ,  0. ,  0. ,  0. ,  0. ,  0. ,  0. ,\n",
       "         32. , 19.5,  7. ],\n",
       "        [ 0. ,  0. ,  0. ,  0. ,  0. ,  0. ,  0. ,  0. ,  0. ,  0. ,\n",
       "          0. ,  0. ,  0. ,  0. ,  0. ,  0. ,  0. ,  0. ,  0. ,  0. ,\n",
       "          0. ,  0. ,  0. ],\n",
       "        [ 0. ,  0. ,  0. ,  0. ,  0. ,  0. ,  0. ,  0. ,  0. ,  0. ,\n",
       "          0. ,  0. ,  0. ,  0. ,  0. ,  0. ,  0. ,  0. ,  0. ,  0. ,\n",
       "          0. ,  0. ,  0. ]],\n",
       "\n",
       "       [[ 0. ,  1. ,  0. ,  0. ,  0. ,  0. ,  0. ,  1. ,  0. ,  1. ,\n",
       "          0. ,  1. ,  0. ,  0. ,  0. ,  0. ,  0. ,  0. ,  0. ,  0. ,\n",
       "         32. , 19.5,  7. ],\n",
       "        [ 0. ,  1. ,  0. ,  0. ,  0. ,  0. ,  0. ,  1. ,  0. ,  1. ,\n",
       "          0. ,  1. ,  0. ,  0. ,  0. ,  0. ,  0. ,  0. ,  0. ,  0. ,\n",
       "         32. , 19.5,  7. ],\n",
       "        [ 0. ,  0. ,  0. ,  0. ,  0. ,  0. ,  0. ,  0. ,  0. ,  0. ,\n",
       "          0. ,  0. ,  0. ,  0. ,  0. ,  0. ,  0. ,  0. ,  0. ,  0. ,\n",
       "          0. ,  0. ,  0. ],\n",
       "        [ 0. ,  0. ,  0. ,  0. ,  0. ,  0. ,  0. ,  0. ,  0. ,  0. ,\n",
       "          0. ,  0. ,  0. ,  0. ,  0. ,  0. ,  0. ,  0. ,  0. ,  0. ,\n",
       "          0. ,  0. ,  0. ]],\n",
       "\n",
       "       [[ 0. ,  1. ,  0. ,  0. ,  0. ,  0. ,  0. ,  0. ,  1. ,  1. ,\n",
       "          0. ,  0. ,  0. ,  0. ,  1. ,  0. ,  0. ,  0. ,  0. ,  0. ,\n",
       "         32. , 19.5,  7. ],\n",
       "        [ 0. ,  1. ,  0. ,  0. ,  0. ,  0. ,  0. ,  0. ,  1. ,  1. ,\n",
       "          0. ,  0. ,  0. ,  0. ,  1. ,  0. ,  0. ,  0. ,  0. ,  0. ,\n",
       "         32. , 19.5,  7. ],\n",
       "        [ 0. ,  1. ,  0. ,  0. ,  0. ,  0. ,  0. ,  0. ,  1. ,  1. ,\n",
       "          0. ,  0. ,  0. ,  0. ,  1. ,  0. ,  0. ,  0. ,  0. ,  0. ,\n",
       "         32. , 19.5,  7. ],\n",
       "        [ 0. ,  1. ,  0. ,  0. ,  0. ,  0. ,  0. ,  0. ,  1. ,  1. ,\n",
       "          0. ,  0. ,  0. ,  0. ,  1. ,  0. ,  0. ,  0. ,  0. ,  0. ,\n",
       "         32. , 19.5,  7. ]]], dtype=float32)"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(3, 4, 16)"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "array([[[-8.8086128e-01,  2.4321668e-02, -1.7695712e-04, -0.0000000e+00,\n",
       "          4.7564495e-01,  6.2118530e-01, -7.4857384e-01,  0.0000000e+00,\n",
       "         -3.5250071e-01, -8.6271411e-01, -9.4471562e-01, -1.8042687e-01,\n",
       "         -3.9749655e-05,  8.5611254e-01, -0.0000000e+00,  8.6530536e-01],\n",
       "        [-1.1002945e+00,  4.2692982e-02, -1.8482472e-04, -0.0000000e+00,\n",
       "          5.7674092e-01,  1.0423709e+00, -1.0624982e+00,  6.6699028e-02,\n",
       "         -2.8473222e-01, -1.1007298e+00, -9.5236158e-01, -3.6931729e-01,\n",
       "         -4.3908203e-05,  1.0580661e+00, -3.8704211e-06,  1.1199365e+00],\n",
       "        [-4.7927666e-01, -0.0000000e+00, -4.0772974e-01, -2.0113529e-01,\n",
       "          4.1639525e-01,  3.3404648e-01, -3.9974684e-01,  1.2654787e-01,\n",
       "         -4.0667661e-02, -0.0000000e+00, -0.0000000e+00, -2.9938039e-01,\n",
       "         -7.8346021e-02,  4.3937773e-01,  6.8528228e-02,  0.0000000e+00],\n",
       "        [-2.8489169e-01, -1.2479872e-02, -3.0037537e-01, -0.0000000e+00,\n",
       "          2.5573882e-01,  2.6183191e-01, -0.0000000e+00,  7.8825995e-02,\n",
       "         -2.9612076e-03, -3.5956368e-01, -1.9949822e-01, -2.1989247e-01,\n",
       "         -5.8521036e-02,  0.0000000e+00,  6.0281340e-02,  1.9068709e-01]],\n",
       "\n",
       "       [[-8.8086128e-01,  2.4321668e-02, -1.7695712e-04, -3.9966559e-04,\n",
       "          4.7564495e-01,  6.2118530e-01, -7.4857384e-01,  5.3244088e-02,\n",
       "         -3.5250071e-01, -8.6271411e-01, -9.4471562e-01, -1.8042687e-01,\n",
       "         -3.9749655e-05,  8.5611254e-01, -3.0214835e-06,  8.6530536e-01],\n",
       "        [-1.1002945e+00,  4.2692982e-02, -0.0000000e+00, -8.1723474e-04,\n",
       "          5.7674092e-01,  1.0423709e+00, -1.0624982e+00,  6.6699028e-02,\n",
       "         -2.8473222e-01, -1.1007298e+00, -9.5236158e-01, -3.6931729e-01,\n",
       "         -4.3908203e-05,  1.0580661e+00, -0.0000000e+00,  0.0000000e+00],\n",
       "        [-0.0000000e+00, -9.0119065e-05, -4.0772974e-01, -2.0113529e-01,\n",
       "          0.0000000e+00,  0.0000000e+00, -3.9974684e-01,  1.2654787e-01,\n",
       "         -4.0667661e-02, -5.0322956e-01, -3.0795896e-01, -2.9938039e-01,\n",
       "         -7.8346021e-02,  4.3937773e-01,  6.8528228e-02,  3.0648836e-01],\n",
       "        [-2.8489169e-01, -0.0000000e+00, -3.0037537e-01, -1.2860718e-01,\n",
       "          2.5573882e-01,  2.6183191e-01, -2.1347173e-01,  7.8825995e-02,\n",
       "         -2.9612076e-03, -3.5956368e-01, -1.9949822e-01, -0.0000000e+00,\n",
       "         -5.8521036e-02,  3.3642414e-01,  6.0281340e-02,  1.9068709e-01]],\n",
       "\n",
       "       [[-8.9288640e-01,  3.1506680e-02, -1.8437540e-04, -0.0000000e+00,\n",
       "          3.8145941e-01,  7.7513254e-01, -8.0280375e-01,  4.6084654e-02,\n",
       "         -4.7248524e-01, -8.8139379e-01, -9.4523966e-01, -2.6698190e-01,\n",
       "         -2.9346318e-05,  0.0000000e+00, -3.1746372e-06,  8.6783636e-01],\n",
       "        [-1.1186693e+00,  5.8720637e-02, -0.0000000e+00, -9.7326271e-04,\n",
       "          4.5533973e-01,  0.0000000e+00, -1.0972521e+00,  5.8646001e-02,\n",
       "         -0.0000000e+00, -1.1204194e+00, -9.5466548e-01, -4.4874656e-01,\n",
       "         -3.3474967e-05,  0.0000000e+00, -4.0323057e-06,  1.1242675e+00],\n",
       "        [-1.1579852e+00,  7.5366005e-02, -1.8358184e-04, -0.0000000e+00,\n",
       "          4.7660258e-01,  1.2003208e+00, -1.1786603e+00,  5.9613414e-02,\n",
       "         -3.8327497e-01, -0.0000000e+00, -0.0000000e+00, -4.7674620e-01,\n",
       "         -3.4109464e-05,  1.0721787e+00, -4.8415095e-06,  0.0000000e+00],\n",
       "        [-0.0000000e+00,  8.4926002e-02, -1.8189257e-04, -1.2286466e-03,\n",
       "          4.8126453e-01,  0.0000000e+00, -0.0000000e+00,  5.9534177e-02,\n",
       "         -3.7990752e-01, -1.1598526e+00, -9.5445305e-01, -0.0000000e+00,\n",
       "         -0.0000000e+00,  0.0000000e+00, -5.4377733e-06,  1.2175546e+00]]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(LSTMStateTuple(c=array([[-5.01877308e-01, -1.82277318e-02, -5.69868267e-01,\n",
       "         -1.98345169e-01,  4.71534669e-01,  4.93054748e-01,\n",
       "         -3.68193686e-01,  1.25247285e-01, -4.75209579e-03,\n",
       "         -6.57937884e-01, -3.40801328e-01, -3.73004377e-01,\n",
       "         -9.82199311e-02,  6.49188161e-01,  1.01826645e-01,\n",
       "          3.35281491e-01],\n",
       "        [-5.01877308e-01, -1.82277318e-02, -5.69868267e-01,\n",
       "         -1.98345169e-01,  4.71534669e-01,  4.93054748e-01,\n",
       "         -3.68193686e-01,  1.25247285e-01, -4.75209579e-03,\n",
       "         -6.57937884e-01, -3.40801328e-01, -3.73004377e-01,\n",
       "         -9.82199311e-02,  6.49188161e-01,  1.01826645e-01,\n",
       "          3.35281491e-01],\n",
       "        [-3.27183819e+00,  8.65990222e-02, -3.75423741e+00,\n",
       "         -1.20902419e+00,  3.40672493e+00,  3.18611193e+00,\n",
       "         -2.65387249e+00,  3.51135880e-01, -3.19729805e-01,\n",
       "         -3.90026903e+00, -1.01505065e+00, -7.21302450e-01,\n",
       "         -1.14005692e-02,  3.97746134e+00, -1.57211944e-05,\n",
       "          2.34122705e+00]], dtype=float32), h=array([[-2.27913350e-01, -9.98389814e-03, -2.40300298e-01,\n",
       "         -1.02885745e-01,  2.04591066e-01,  2.09465519e-01,\n",
       "         -1.70777380e-01,  6.30607978e-02, -2.36896612e-03,\n",
       "         -2.87650943e-01, -1.59598574e-01, -1.75913975e-01,\n",
       "         -4.68168296e-02,  2.69139320e-01,  4.82250713e-02,\n",
       "          1.52549669e-01],\n",
       "        [-2.27913350e-01, -9.98389814e-03, -2.40300298e-01,\n",
       "         -1.02885745e-01,  2.04591066e-01,  2.09465519e-01,\n",
       "         -1.70777380e-01,  6.30607978e-02, -2.36896612e-03,\n",
       "         -2.87650943e-01, -1.59598574e-01, -1.75913975e-01,\n",
       "         -4.68168296e-02,  2.69139320e-01,  4.82250713e-02,\n",
       "          1.52549669e-01],\n",
       "        [-9.33131576e-01,  6.79408014e-02, -1.45514059e-04,\n",
       "         -9.82917263e-04,  3.85011613e-01,  9.73769486e-01,\n",
       "         -9.61454570e-01,  4.76273410e-02, -3.03926021e-01,\n",
       "         -9.27882135e-01, -7.63562441e-01, -3.84626538e-01,\n",
       "         -2.73674377e-05,  8.60138655e-01, -4.35021866e-06,\n",
       "          9.74043608e-01]], dtype=float32)),)"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(3, 1, 1)"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "array([[[1400.]],\n",
       "\n",
       "       [[1400.]],\n",
       "\n",
       "       [[1900.]]], dtype=float32)"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "()"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "4752.7837"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "with tf.Session(config=config) as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    cat_inp,num_inp,inp,emb,outputs,final_state,_labels,loss=sess.run([cat_inp,num_inp,inp,emb,outputs,final_state,labels,loss])\n",
    "    cat_inp.shape\n",
    "    num_inp.shape\n",
    "    num_inp\n",
    "    inp\n",
    "    outputs.shape\n",
    "    outputs\n",
    "    final_state\n",
    "    _labels.shape\n",
    "    _labels\n",
    "    loss.shape\n",
    "    loss\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Estimator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 封装一个工具 绕过计算图检查vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-03T09:22:35.548613Z",
     "start_time": "2020-12-03T09:22:31.318612Z"
    },
    "code_folding": [
     4
    ],
    "run_control": {
     "marked": true
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.contrib.framework.python.framework import checkpoint_utils\n",
    "\n",
    "\n",
    "class CheckPointInspector:\n",
    "    def __init__(self, ckpt_dir, check_list=None):\n",
    "        self.ckpt_dir = ckpt_dir\n",
    "        self.name2shape_dict = CheckPointInspector.load_name_shape(self.ckpt_dir)\n",
    "        self.vars = sorted(self.name2shape_dict.keys())\n",
    "        self.check_list = check_list if check_list is not None else []\n",
    "        self.name2value_dict = {}\n",
    "        # workaround: _saver raise ValueError: No variables to save from\n",
    "        # self.__tmpVar = tf.Variable([0],name=\"tmpVarOfInspectUtils\")\n",
    "\n",
    "    def append(self, name):\n",
    "        if name in self.check_list:\n",
    "            print(\"%s exist\" % name)\n",
    "        else:\n",
    "            self.check_list.append(name)\n",
    "\n",
    "    def brief(self):\n",
    "        for i in self.vars:\n",
    "            print(i, self.name2shape_dict[i])\n",
    "\n",
    "    def restore(self):\n",
    "        check_list = self.check_list if len(self.check_list) > 0 else self.vars\n",
    "        print(\"restore with: %s\" % \",\".join(check_list))\n",
    "        # init variable\n",
    "        for name in check_list:\n",
    "            self.name2value_dict[name] = tf.Variable(tf.zeros(self.name2shape_dict[name]), name=name, dtype=tf.float32)\n",
    "        _saver = tf.train.Saver(self.name2value_dict.values(), max_to_keep=1)\n",
    "        with tf.Session() as sess:\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "            # restore\n",
    "            _saver.restore(sess, tf.train.latest_checkpoint(self.ckpt_dir))\n",
    "            # eval variable\n",
    "            for name in check_list:\n",
    "                self.name2value_dict[name] = self.name2value_dict[name].eval()\n",
    "\n",
    "    def show(self, name):\n",
    "        return self.name2value_dict[name]\n",
    "\n",
    "    @staticmethod\n",
    "    def load_name_shape(ckpt_dir):\n",
    "        return dict(checkpoint_utils.list_variables(ckpt_dir))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-04T04:56:02.110326Z",
     "start_time": "2020-12-04T04:56:02.100024Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(64, 71)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "a=\"dnn/moe/expert_2/repr_2/kernel\"\n",
    "b=\"dnn/emb_mlp_kernel_1/input_layer/pickupHash_embedding/embedding_weights\"\n",
    "re.search(\"emb\",a)\n",
    "re.search(\"emb\",b).groups()\n",
    "re.search(\"weights$\",b).span()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-04T03:31:24.433134Z",
     "start_time": "2020-12-04T03:31:24.421398Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"-1\"\n",
    "\n",
    "ckpt_dir=\"/tmp-data/zhoutongzt/DD/Data/BR_useReqTime_MMOE_CAN\"\n",
    "print(\">>> checking ckpt: %s\" % ckpt_dir)\n",
    "ins = CheckPointInspector(ckpt_dir)\n",
    "\n",
    "print(\">>> all var:\")\n",
    "for i in ins.vars:\n",
    "    print(i)\n",
    "\n",
    "# append var to check-value\n",
    "for i in ins.vars:\n",
    "    # if \"embedding_weights/Adagrad\" in i:\n",
    "    #     ins.append(i)\n",
    "    if any(j in i for j in [\"kernel\", \"kernel/Adagrad\", \"bias/Adagrad\"]):\n",
    "        if \"embedding\" not in i:\n",
    "            ins.append(i)\n",
    "ins.restore()\n",
    "\n",
    "print(\">>> inspecting values:\")\n",
    "for name in ins.check_list:\n",
    "    var = ins.show(name)\n",
    "    print(\"\\n===%s %s\" % (name, var.shape))\n",
    "    print(\"min:%s max:%s avg:%s\" % (np.min(var), np.max(var), np.mean(var)))\n",
    "    # leads to err in prefix of 155.1 55.1 5.1\n",
    "    # with np.printoptions(suppress=True, formatter={'float': '{:0.4f}'.format}):\n",
    "    with np.printoptions(suppress=True, precision=4, threshold=50):\n",
    "        print(var)\n",
    "        if len(var.shape) > 1:\n",
    "            max_idx = np.argmax(np.mean(var, axis=1))\n",
    "            min_idx = np.argmin(np.mean(var, axis=1))\n",
    "            print(\"max by mean_axis=1:%s\" % var[max_idx])\n",
    "            print(\"min by mean_axis=1:%s\" % var[min_idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## check model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-24T11:57:02.377916Z",
     "start_time": "2020-11-24T11:57:02.368744Z"
    },
    "run_control": {
     "marked": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/tmp-data/zhoutongzt/DD/Data/BR_addHeat_PE_timIdxByFreqIdx_WDModel_drop_feat'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ckpt_dir = os.path.join(DATA_PATH,\"BR_addHeat_PE_timIdxByFreqIdx_WDModel_drop_feat\")\n",
    "\n",
    "\n",
    "train_spec = dnn_linear_combined_model_fn(\n",
    "                    features=tr_data[0],\n",
    "                    labels=tr_data[1],\n",
    "                    fc_wide=fc_wide,\n",
    "                    fc_numeric=fc_numeric,\n",
    "                    n_classes=hparams['n_classes'],\n",
    "                    hparams=hparams,\n",
    "                    mode=tf.estimator.ModeKeys.TRAIN,\n",
    "                    fc_emb_dict=fc_emb_dict,\n",
    "                    emb_to_add=emb_to_add, emb_to_innerProduct=emb_to_innerProduct)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### restore ckpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-03T02:43:37.143426Z",
     "start_time": "2020-12-03T02:43:37.125924Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.Variable 'input_layer/featA_featB_hour_idx_link_pickupHash_time_idx_shared_embedding/embedding_weights:0' shape=(20, 4) dtype=float32_ref>\n",
      "<tf.Variable 'input_layer_1/pickupHash_embedding/embedding_weights:0' shape=(100, 32) dtype=float32_ref>\n",
      "<tf.Variable 'input_layer_2/pickupHash_embedding/embedding_weights:0' shape=(100, 8) dtype=float32_ref>\n",
      "<tf.Variable 'input_layer_4/pickupHash_embedding/embedding_weights:0' shape=(100, 32) dtype=float32_ref>\n",
      "<tf.Variable 'input_layer_5/pickupHash_embedding/embedding_weights:0' shape=(100, 8) dtype=float32_ref>\n",
      "<tf.Variable 'input_layer_8/pickupHash_embedding/embedding_weights:0' shape=(100, 32) dtype=float32_ref>\n",
      "<tf.Variable 'input_layer_9/pickupHash_embedding/embedding_weights:0' shape=(100, 64) dtype=float32_ref>\n",
      "<tf.Variable 'input_layer_10/pickupHash_embedding/embedding_weights:0' shape=(100, 64) dtype=float32_ref>\n",
      "<tf.Variable 'input_layer_12/pickupHash_embedding/embedding_weights:0' shape=(100, 32) dtype=float32_ref>\n",
      "<tf.Variable 'input_layer_13/pickupHash_embedding/embedding_weights:0' shape=(100, 8) dtype=float32_ref>\n",
      "<tf.Variable 'input_layer_14/pickupHash_embedding/embedding_weights:0' shape=(100, 64) dtype=float32_ref>\n",
      "<tf.Variable 'input_layer_15/pickupHash_embedding/embedding_weights:0' shape=(100, 8) dtype=float32_ref>\n",
      "<tf.Variable 'input_layer_16/pickupHash_embedding/embedding_weights:0' shape=(100, 64) dtype=float32_ref>\n",
      "<tf.Variable 'input_layer_17/pickupHash_embedding/embedding_weights:0' shape=(100, 8) dtype=float32_ref>\n",
      "<tf.Variable 'input_layer_19/pickupHash_embedding/embedding_weights:0' shape=(100, 32) dtype=float32_ref>\n",
      "<tf.Variable 'input_layer_20/pickupHash_embedding/embedding_weights:0' shape=(100, 8) dtype=float32_ref>\n",
      "<tf.Variable 'input_layer_21/pickupHash_embedding/embedding_weights:0' shape=(100, 64) dtype=float32_ref>\n",
      "<tf.Variable 'input_layer_22/pickupHash_embedding/embedding_weights:0' shape=(100, 8) dtype=float32_ref>\n",
      "<tf.Variable 'input_layer_23/pickupHash_embedding/embedding_weights:0' shape=(100, 64) dtype=float32_ref>\n",
      "<tf.Variable 'input_layer_24/pickupHash_embedding/embedding_weights:0' shape=(100, 8) dtype=float32_ref>\n"
     ]
    }
   ],
   "source": [
    "# after restore ckpt\n",
    "for variable_name in tf.global_variables():\n",
    "        print(variable_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 直接检查ckpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-03T08:27:22.624047Z",
     "start_time": "2020-12-03T08:27:05.089652Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.94125503, 0.8952562 , 0.27667555, 0.8631715 ],\n",
       "       [0.58970124, 0.34520158, 0.6283586 , 0.7666799 ],\n",
       "       [0.52678007, 0.38669515, 0.9040632 , 0.8088709 ],\n",
       "       [0.00688401, 0.6805194 , 0.725349  , 0.80912757],\n",
       "       [0.32290053, 0.00987846, 0.66399413, 0.27213082],\n",
       "       [0.77246773, 0.24089827, 0.89595705, 0.9883507 ],\n",
       "       [0.22828436, 0.37639886, 0.9784473 , 0.9773074 ],\n",
       "       [0.49756825, 0.8321498 , 0.7441464 , 0.4640103 ],\n",
       "       [0.89436436, 0.7959515 , 0.5971731 , 0.5020732 ],\n",
       "       [0.9249599 , 0.14479335, 0.32194713, 0.5205036 ]], dtype=float32)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /tmp-data/zhoutongzt/DD/Data/BR_useReqTime_MMOE_CAN/model-100000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 0.09968653,  0.6192992 ,  0.08434715,  0.1492789 ],\n",
       "       [-0.18485333,  0.16393347, -0.12167899,  0.2712814 ],\n",
       "       [-0.04972822,  0.02612558, -0.01568268,  0.28784993],\n",
       "       [-0.7943455 , -0.17553413,  0.7769771 ,  0.3203123 ],\n",
       "       [ 0.7965788 ,  0.30415446, -0.09494041,  0.0148005 ],\n",
       "       [ 0.26068452, -0.14760247,  0.1682965 ,  0.35040897],\n",
       "       [ 0.5200607 ,  0.52716976,  0.38815165, -0.35098234],\n",
       "       [ 0.5012951 , -0.00572503,  0.6568418 ,  0.3084871 ],\n",
       "       [-0.59321654, -0.5363189 , -0.5635058 , -0.41347796],\n",
       "       [-0.03972653,  0.35015455,  0.28889334, -0.24242574]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "m = tf.Variable(np.random.random([12000000, 4]),dtype=tf.float32,name='dnn/emb_mlp_bias_0/input_layer/pickupHash_embedding/embedding_weights')\n",
    "\n",
    "ckpt_dir=\"/tmp-data/zhoutongzt/DD/Data/BR_useReqTime_MMOE_CAN\"\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    saver = tf.train.Saver(max_to_keep=1)\n",
    "    sess.run(m)[:10]\n",
    "    saver.restore(sess, tf.train.latest_checkpoint(ckpt_dir))\n",
    "    sess.run(m)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.contrib.framework.python.framework import checkpoint_utils\n",
    "\n",
    "class checkUtils:\n",
    "    def __init__(self, ckpt_dir, check_list=None):\n",
    "        self.ckpt_dir = ckpt_dir\n",
    "        self.check_list = check_list if check_list is not None else []\n",
    "    \n",
    "    @static\n",
    "    def load_name_shape()\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 梯度测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-15T04:54:37.474459Z",
     "start_time": "2020-12-15T04:54:37.426610Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(None,\n",
       "  <tf.Variable 'input_layer_4/featA_X_featB_embedding/embedding_weights:0' shape=(10, 4) dtype=float32_ref>),\n",
       " (None,\n",
       "  <tf.Variable 'input_layer_6/featA_X_featB_X_featC_embedding/embedding_weights:0' shape=(10, 4) dtype=float32_ref>),\n",
       " (None,\n",
       "  <tf.Variable 'input_layer_8/featA_X_featB_featA_X_featB_X_featC_shared_embedding/embedding_weights:0' shape=(10, 4) dtype=float32_ref>),\n",
       " (None, <tf.Variable 'Variable:0' shape=(4,) dtype=float32_ref>),\n",
       " (None, <tf.Variable 'Variable_1:0' shape=(4,) dtype=float32_ref>),\n",
       " (None, <tf.Variable 'Variable_2:0' shape=(4,) dtype=float32_ref>),\n",
       " (None, <tf.Variable 'Variable_3:0' shape=(3,) dtype=float32_ref>),\n",
       " (<tf.Tensor 'gradients_1/mul_1_grad/tuple/control_dependency:0' shape=(4,) dtype=float32>,\n",
       "  <tf.Variable 'Variable_4:0' shape=(4,) dtype=float32_ref>),\n",
       " (None, <tf.Variable 'Variable_5:0' shape=(3,) dtype=float32_ref>)]"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<bound method GradientDescentOptimizer.apply_gradients of <tensorflow.python.training.gradient_descent.GradientDescentOptimizer object at 0x7fed7372e2d0>>"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = tf.Variable([1,2,3,4], dtype=tf.float32)\n",
    "b = a*2\n",
    "c = tf.Variable([1,2,3], dtype=tf.float32)\n",
    "opt = tf.train.GradientDescentOptimizer(0.1)\n",
    "opt.compute_gradients(b)\n",
    "opt.apply_gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-27T11:32:00.011686Z",
     "start_time": "2020-11-27T11:32:00.001416Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['sigmoid_cross_entropy_with_logits',\n",
       " 'softmax_cross_entropy_with_logits',\n",
       " 'softmax_cross_entropy_with_logits_v2',\n",
       " 'sparse_softmax_cross_entropy_with_logits',\n",
       " 'weighted_cross_entropy_with_logits']"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[i for i in dir(tf.nn) if \"entropy\" in i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-27T12:07:13.292832Z",
     "start_time": "2020-11-27T12:07:12.854315Z"
    },
    "run_control": {
     "marked": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'logits'"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "array([[0.64769123, 0.99691358, 0.51880326, 0.65811273],\n",
       "       [0.59906347, 0.75306733, 0.13624713, 0.00411712],\n",
       "       [0.14950888, 0.698439  , 0.59335256, 0.89991535],\n",
       "       [0.44445739, 0.316785  , 0.92308176, 0.46586186]])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "'labels'"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "[1, 1, 2, 3]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "'softmax(logits)'"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "array([[0.23215253, 0.32918403, 0.20407889, 0.23458456],\n",
       "       [0.29872185, 0.34845766, 0.18804786, 0.16477263],\n",
       "       [0.15605622, 0.27019568, 0.24324277, 0.33050533],\n",
       "       [0.22145306, 0.19491006, 0.35739264, 0.22624424]])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "'target-one of softmax(logits)'"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "[0.32918402769283184,\n",
       " 0.34845765974154597,\n",
       " 0.24324277039282377,\n",
       " 0.22624423686097198]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "[-1.111138329907829,\n",
       " -1.05423854901307,\n",
       " -1.413695279250508,\n",
       " -1.4861401691642224]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "1.2663030624389648"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(1024)\n",
    "logits = np.random.random([4,4])\n",
    "labels=[1,1,2,3]\n",
    "\"logits\"\n",
    "logits\n",
    "\"labels\"\n",
    "labels\n",
    "with tf.Session() as sess:\n",
    "    \"softmax(logits)\"\n",
    "    sess.run(tf.nn.softmax(logits))\n",
    "    \"target-one of softmax(logits)\"\n",
    "    to = [sess.run(tf.nn.softmax(logits))[idx,i] for idx,i in enumerate(labels)]\n",
    "    to\n",
    "    [np.log(i) for i in to]\n",
    "    sess.run(tf.losses.sparse_softmax_cross_entropy(logits=logits,labels=labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Py3.7(zt)",
   "language": "python",
   "name": "zt"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "310px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
