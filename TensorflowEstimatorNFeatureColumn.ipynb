{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-02T11:06:22.352063Z",
     "start_time": "2020-12-02T11:06:21.711810Z"
    }
   },
   "outputs": [],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "%matplotlib inline\n",
    "from tqdm.auto import tqdm\n",
    "import concurrent.futures\n",
    "from multiprocessing import Pool\n",
    "import copy,os,sys\n",
    "from collections import Counter,deque\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "import functools, itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-02T11:06:25.360598Z",
     "start_time": "2020-12-02T11:06:22.450761Z"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.feature_column as fc\n",
    "import numpy as np\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pyhocon import ConfigFactory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-02T11:06:25.475345Z",
     "start_time": "2020-12-02T11:06:25.469671Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"-1\"  # 禁用GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-02T11:06:25.590410Z",
     "start_time": "2020-12-02T11:06:25.585583Z"
    }
   },
   "outputs": [],
   "source": [
    "DATA_PATH=\"/tmp-data/zhoutongzt/DD/Data\"\n",
    "NFS_DATA_PATH=\"/nfs/map-tmp-vol0/zhoutong/Data\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 处理TFRecord"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 检查schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-02T07:07:51.160284Z",
     "start_time": "2020-12-02T07:07:50.716972Z"
    },
    "run_control": {
     "marked": true
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[featName]:dnnCrsCurRoad [type]:int64_list [value_example]:[8593260L]\n",
      "[featName]:dnnCrsDragRoad [type]:int64_list [value_example]:[-1L]\n",
      "[featName]:dnnCrsPoiRoad [type]:int64_list [value_example]:[-1L]\n",
      "[featName]:dnnDGlobalTime [type]:int64_list [value_example]:[42553L]\n",
      "[featName]:dnnNpoiDis [type]:float_list [value_example]:[0.5899999737739563]\n",
      "[featName]:dnnPGlobalTime [type]:int64_list [value_example]:[8655390L]\n",
      "[featName]:dnncCurrentDis [type]:float_list [value_example]:[14.9399995803833]\n",
      "[featName]:dnncPoiDis [type]:float_list [value_example]:[12.050000190734863]\n",
      "[featName]:dnncRgeoDis [type]:float_list [value_example]:[0.0]\n",
      "[featName]:dnnpidDropoffHeat [type]:float_list [value_example]:[0.0]\n",
      "[featName]:dnnpidDropoffRatio [type]:float_list [value_example]:[0.0]\n",
      "[featName]:dnnpidLBChargeTime [type]:float_list [value_example]:[0.0]\n",
      "[featName]:dnnpidLFChargeTime [type]:float_list [value_example]:[0.0]\n",
      "[featName]:dnnpidPickupHeat [type]:float_list [value_example]:[0.0]\n",
      "[featName]:dnnpidRatio [type]:float_list [value_example]:[0.0]\n",
      "[featName]:dnnpidTrajNum [type]:float_list [value_example]:[0.0]\n",
      "[featName]:dnnpidTrajRatio [type]:float_list [value_example]:[0.0]\n",
      "[featName]:dnnpoiDropoffRatio [type]:float_list [value_example]:[0.0]\n",
      "[featName]:dnnpoiRatio [type]:float_list [value_example]:[8.100000381469727]\n",
      "[featName]:dnnpsw [type]:float_list [value_example]:[10.520000457763672]\n",
      "[featName]:hourIdx [type]:int64_list [value_example]:[8593273L]\n",
      "[featName]:label [type]:int64_list [value_example]:[3L, 0L]\n",
      "[featName]:link [type]:int64_list [value_example]:[12939794L]\n",
      "[featName]:locHash [type]:int64_list [value_example]:[-1L]\n",
      "[featName]:pickupHash [type]:int64_list [value_example]:[10519901L]\n",
      "[featName]:pid [type]:int64_list [value_example]:[14160771L]\n",
      "[featName]:poi [type]:int64_list [value_example]:[-1L]\n",
      "[featName]:rgeoHash [type]:int64_list [value_example]:[-1L]\n",
      "[featName]:timeIdx [type]:int64_list [value_example]:[12273636L]\n",
      "[featName]:weekDay [type]:int64_list [value_example]:[8593268L]\n",
      "[featName]:wideFeatures [type]:int64_list [value_example]:[64722170L, -1L, -1L, -1L, -1L, 55171488L, -1L, 55171501L, 55171508L, 55171518L, -1L, -1L, -1L, -1L, 55171554L, -1L, -1L, -1L, -1L, -1L, 56183148L, 493793L, 55171597L, -1L, -1L, 55171605L, 55171610L, 75141947L, 56183451L, 56203337L, 81605532L, -1L, 147462951L, -1L, -1L, -1L, -1L, -1L, -1L, -1L, -1L, -1L]\n",
      "[featName]:wifi [type]:int64_list [value_example]:[-1L, -1L, -1L, -1L, -1L, -1L, -1L, -1L]\n"
     ]
    }
   ],
   "source": [
    "# trd_dir=\"/tmp-data/zhoutongzt/DD/Data/BR_useReqTime_addCross_test\"\n",
    "trd_dir = NFS_DATA_PATH+\"/BR_useReqTime_expTryMMOE_train\"\n",
    "trd_fp = [os.path.join(trd_dir,i) for i in os.listdir(trd_dir)]\n",
    "trd=tf.data.TFRecordDataset(trd_fp,compression_type='GZIP')\n",
    "iterator=trd.make_one_shot_iterator()\n",
    "one=iterator.get_next()\n",
    "res = []\n",
    "with tf.Session() as sess:\n",
    "    one_ = sess.run(one)\n",
    "    featInfo = tf.train.Example.FromString(one_).features.feature # not dict\n",
    "    for key in featInfo:\n",
    "        info_list=[(key, attr, featInfo[key].__getattribute__(attr).value) for attr in [\"bytes_list\",\"float_list\",\"int64_list\"]]\n",
    "        info = [i for i in info_list if i[-1] != []][0] # 只应该有一个\n",
    "        res.append(\"[featName]:{} [type]:{} [value_example]:{}\".format(*info))\n",
    "\n",
    "print(\"\\n\".join(sorted(res)))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 解析"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-18T07:48:55.485524Z",
     "start_time": "2020-11-18T07:48:55.025899Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[features.keys]: dnnCrsCurRoad, dnnCrsDragRoad, dnnCrsPoiRoad, dnnDGlobalTime, dnnNpoiDis, dnnPGlobalTime, dnncCurrentDis, dnncPoiDis, dnncRgeoDis, dnnpidDropoffHeat, dnnpidDropoffRatio, dnnpidLBChargeTime, dnnpidLFChargeTime, dnnpidPickupHeat, dnnpidRatio, dnnpidTrajNum, dnnpidTrajRatio, dnnpoiDropoffRatio, dnnpoiRatio, dnnpsw, link, locHash, pickupHash, pid, poi, rgeoHash, wideFeatures, wifi\n",
      ">>> all keys: rgeoHash,pid,dnnNpoiDis,dnncRgeoDis,poi,dnncPoiDis,dnnpidDropoffRatio,wideFeatures,dnncCurrentDis,dnnpidDropoffHeat,locHash,dnnPGlobalTime,dnnDGlobalTime,dnnpoiDropoffRatio,pickupHash,dnnpidLFChargeTime,dnnpidPickupHeat,link,dnnpidTrajRatio,dnnCrsDragRoad,dnnCrsPoiRoad,dnnpidTrajNum,dnnpidRatio,wifi,dnnpsw,dnnCrsCurRoad,dnnpidLBChargeTime,dnnpoiRatio\n",
      "\n",
      ">>> dnnCrsCurRoad:\n",
      "[9414388 9414388 9414388 9414388 9414389 9414388 9414388 9414388 9414388\n",
      " 9414388 9414388 9414389 9414388 9414388 9414389 9414388 9414389 9414388\n",
      " 9414389 9414388 9414388 9414389 9414389 9414389 9414389 9414388 9414388\n",
      " 9414388 9414389 9414389 9414389 9414388 9414389 9414389 9414388 9414388\n",
      " 9414388 9414388 9414389 9414388 9414388 9414389 9414388 9414388 9414388\n",
      " 9414388 9414388 9414388 9414388 9414388 9414388 9414389 9414388 9414388\n",
      " 9414388 9414389 9414389 9414389 9414388 9414388 9414388 9414388 9414389\n",
      " 9414388]\n"
     ]
    }
   ],
   "source": [
    "trd_dir = \"/nfs/map-tmp-vol0/weizhipeng/tensorflow_lr/trainDatas/BR_pidtwo_train\"\n",
    "trd_fp = [os.path.join(trd_dir,i) for i in os.listdir(trd_dir)]\n",
    "wide_size = 35\n",
    "config_path = open('/nfs/map-tmp-vol0/weizhipeng/tensorflow_lr/src/main/resources/features.json','r')\n",
    "# config_path = open('/tmp-data/zhoutongzt/DD/tensorflow_lr/src/main/resources/features_oriFeat.json','r')\n",
    "conf = json.load(config_path)\n",
    "features={}\n",
    "features['wideFeatures'] = tf.io.FixedLenFeature([wide_size], tf.int64)\n",
    "# features['label'] = tf.io.FixedLenFeature([1], tf.int64)\n",
    "for deepName in conf[\"deepFeat\"]:\n",
    "    fshape = conf[\"varlensFeat_length\"].get(deepName, None)\n",
    "    fshape = [] if fshape is None else [fshape]\n",
    "    fdtype = tf.float32 if deepName in conf[\"floatFeat\"] else tf.int64\n",
    "    features[deepName] = tf.io.FixedLenFeature(fshape, fdtype)\n",
    "\n",
    "print(\"[features.keys]: \"+\", \".join(sorted(features.keys())))\n",
    "def _parse_func(inp):\n",
    "    parsed_features = tf.io.parse_single_example(inp,features)\n",
    "    return parsed_features\n",
    "\n",
    "trd=tf.data.TFRecordDataset(trd_fp,compression_type='GZIP').map(_parse_func)\n",
    "\n",
    "####################\n",
    "# tf1.x specific\n",
    "####################\n",
    "dataset = trd.batch(64,drop_remainder=True)\n",
    "line = dataset.make_one_shot_iterator().get_next()\n",
    "to_print = [\"dnnCrsCurRoad\",]\n",
    "with tf.Session() as sess:\n",
    "    i = sess.run(line)\n",
    "    print(\">>> all keys: {}\\n\".format(\",\".join(i.keys())))\n",
    "    for key in to_print:\n",
    "        print(\">>> {}:\".format(key))\n",
    "        print(i[key])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 处理TFRecord | tf2.x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 检查schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trd_dir = \"/nfs/map-tmp-vol0/zhoutong/Data/BR_useReqTime_addCross_addTimeByFreq_train\"\n",
    "trd_fp = [os.path.join(trd_dir,i) for i in os.listdir(trd_dir)]\n",
    "trd_unparsed=tf.data.TFRecordDataset(trd_fp,compression_type='GZIP')\n",
    "\n",
    "for raw_record in trd_unparsed.take(1):\n",
    "    example = tf.train.Example()\n",
    "    _ = example.ParseFromString(raw_record.numpy())\n",
    "    \n",
    "def format_exampleInfo(exampleInfo):\n",
    "    res = []\n",
    "    for name,feat in exampleInfo.features.feature.items():\n",
    "        info_list=[(name, attr, feat.__getattribute__(attr).value) for attr in [\"bytes_list\",\"float_list\",\"int64_list\"]]\n",
    "        info = [(name,attr,value) for name,attr,value in info_list if value != []][0]\n",
    "        res.append(\"[featName]:{} [type]:{} [value_example]:{}\".format(*info))\n",
    "    print(\"\\n\".join(sorted(map(str,res))))\n",
    "    \n",
    "format_exampleInfo(example)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 解析"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trd_dir = \"/nfs/map-tmp-vol0/zhoutong/Data/BR_useReqTime_addCross_addTimeByFreq_addTimeHeatCross_test\"\n",
    "trd_fp = [os.path.join(trd_dir,i) for i in os.listdir(trd_dir)]\n",
    "wide_size = 48\n",
    "config_path = open('/tmp-data/zhoutongzt/DD/tensorflow_lr/src/main/resources/features_useReqTime_addCross.json','r')\n",
    "# config_path = open('/tmp-data/zhoutongzt/DD/tensorflow_lr/src/main/resources/features_oriFeat.json','r')\n",
    "conf = json.load(config_path)\n",
    "features={}\n",
    "features['wideFeatures'] = tf.io.FixedLenFeature([wide_size], tf.int64)\n",
    "features['label'] = tf.io.FixedLenFeature([1], tf.int64)\n",
    "for deepName in conf[\"deepFeat\"]:\n",
    "    fshape = conf[\"varlensFeat_length\"].get(deepName, None)\n",
    "    fshape = [] if fshape is None else [fshape]\n",
    "    fdtype = tf.float32 if deepName in conf[\"floatFeat\"] else tf.int64\n",
    "    features[deepName] = tf.io.FixedLenFeature(fshape, fdtype)\n",
    "\n",
    "print(\"[features.keys]: \"+\", \".join(sorted(features.keys())))\n",
    "def _parse_func(inp):\n",
    "    parsed_features = tf.io.parse_single_example(inp,features)\n",
    "    return parsed_features\n",
    "\n",
    "trd=tf.data.TFRecordDataset(trd_fp,compression_type='GZIP').map(_parse_func)\n",
    "\n",
    "####################\n",
    "# tf2.x specific\n",
    "####################\n",
    "for i in trd.take(1):\n",
    "    print(i)\n",
    "    print(\"\\n[keys]:\" + str(i.keys()))\n",
    "    print(\"\\n[heat_time_idx_poi_id]: \" + str(i['heat_time_idx_poi_id']))\n",
    "    print(\"[heat_time_idx_pickup_geoHash]: \" + str(i['heat_time_idx_pickup_geoHash']))\n",
    "    print(\"[heat_time_idx_bind_line_id_single]: \" + str(i['heat_time_idx_bind_line_id_single']))\n",
    "#     print(\"[weights]:\" + str(i['weights']))\n",
    "#     print(\"[order_info]:\" + str(i['order_info']))\n",
    "    print(i['wideFeatures'].shape)\n",
    "    \n",
    "# 跑不出来377w数太多了？\n",
    "# set([i['wideFeatures'].shape for i in trd.as_numpy_iterator()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# feature_column TEST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## categorical_column_with_hash_bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-02T06:56:06.895537Z",
     "start_time": "2020-12-02T06:56:06.859909Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'featA': array([[2],\n",
       "        [6]]), 'featB': array([[4],\n",
       "        [2]]), 'featC': array([[5],\n",
       "        [6]]), 'featD': array([[0],\n",
       "        [4]]), 'featE': array([[4],\n",
       "        [6]]), 'pickupHash': array([[3],\n",
       "        [5]])}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_buckets_deep=7\n",
    "dims_embs_deep=4\n",
    "_batch_size=2\n",
    "\n",
    "# features\n",
    "features = {}\n",
    "features[\"featA\"] = np.random.randint(num_buckets_deep, size=(_batch_size, 1))\n",
    "features[\"featB\"] = np.random.randint(num_buckets_deep, size=(_batch_size, 1))\n",
    "features[\"featC\"] = np.random.randint(num_buckets_deep, size=(_batch_size, 1))\n",
    "features[\"featD\"] = np.random.randint(num_buckets_deep, size=(_batch_size, 1))\n",
    "features[\"featE\"] = np.random.randint(num_buckets_deep, size=(_batch_size, 1))\n",
    "features[\"pickupHash\"] = np.random.randint(num_buckets_deep, size=(_batch_size, 1))\n",
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-02T07:03:39.745875Z",
     "start_time": "2020-12-02T07:03:39.403303Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'input_layer/concat:0' shape=(2, 7) dtype=float32>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'input_layer_1/concat:0' shape=(2, 10) dtype=float32>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 1., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 1.]], dtype=float32)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 1., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "# codec column\n",
    "\n",
    "featA_cat_id_column = fc.categorical_column_with_identity(\"featA\", num_buckets_deep, 0)\n",
    "featA_cat_hash_column = fc.categorical_column_with_hash_bucket(\"featA\", hash_bucket_size=10, dtype=tf.int64)\n",
    "\n",
    "with tf.Session():\n",
    "    fc.input_layer(features, fc.indicator_column(featA_cat_id_column))\n",
    "    fc.input_layer(features, fc.indicator_column(featA_cat_hash_column))\n",
    "    fc.input_layer(features, fc.indicator_column(featA_cat_id_column)).eval()\n",
    "    fc.input_layer(features, fc.indicator_column(featA_cat_hash_column)).eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## cross_column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### basic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-10T13:32:19.156285Z",
     "start_time": "2020-11-10T13:32:19.135683Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'featA': array([[0],\n",
       "        [4]]), 'featB': array([[0],\n",
       "        [5]]), 'featC': array([[0],\n",
       "        [5]]), 'featD': array([[4],\n",
       "        [2]]), 'featE': array([[4],\n",
       "        [5]])}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_buckets_deep=7\n",
    "dims_embs_deep=4\n",
    "_batch_size=2\n",
    "\n",
    "# features\n",
    "features = {}\n",
    "features[\"featA\"] = np.random.randint(num_buckets_deep, size=(_batch_size, 1))\n",
    "features[\"featB\"] = np.random.randint(num_buckets_deep, size=(_batch_size, 1))\n",
    "features[\"featC\"] = np.random.randint(num_buckets_deep, size=(_batch_size, 1))\n",
    "features[\"featD\"] = np.random.randint(num_buckets_deep, size=(_batch_size, 1))\n",
    "features[\"featE\"] = np.random.randint(num_buckets_deep, size=(_batch_size, 1))\n",
    "features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-10T13:33:44.595215Z",
     "start_time": "2020-11-10T13:33:43.943343Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> featA_indicator:\n",
      "[[1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0.]]\n",
      ">>> featB_indicator:\n",
      "[[1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0.]]\n",
      ">>> featC_indicator:\n",
      "[[1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0.]]\n",
      ">>> featA_X_featB_indicator:\n",
      "[[0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]]\n",
      ">>> featA_X_featB_embedding:\n",
      "[[-0.263117   -0.3976633   0.331601   -0.29276696]\n",
      " [-0.1234642  -0.6340207   0.10317452  0.33643612]]\n",
      ">>> featA_X_featB_X_featC_indicator:\n",
      "[[0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]]\n",
      ">>> featA_X_featB_X_featC_embedding:\n",
      "[[-0.27891752  0.22432825  0.6575673  -0.6044132 ]\n",
      " [-0.27891752  0.22432825  0.6575673  -0.6044132 ]]\n",
      ">>> featA_X_featB_shared_embedding:\n",
      "[[-0.2952471   0.674401   -0.17939782  0.19393241]\n",
      " [ 0.00449466 -0.19109586 -0.43362102 -0.10811146]]\n",
      ">>> featA_X_featB_X_featC_shared_embedding:\n",
      "[[ 0.33795786  0.8862156  -0.27822748 -0.13059047]\n",
      " [ 0.33795786  0.8862156  -0.27822748 -0.13059047]]\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "# codec column\n",
    "featA_cat_column = fc.categorical_column_with_identity(\"featA\", num_buckets_deep, 0)\n",
    "featA_idx_column = fc.indicator_column(featA_cat_column)\n",
    "\n",
    "featB_cat_column = fc.categorical_column_with_identity(\"featB\", num_buckets_deep, 0)\n",
    "featB_idx_column = fc.indicator_column(featB_cat_column)\n",
    "\n",
    "featC_cat_column = fc.categorical_column_with_identity(\"featC\", num_buckets_deep, 0)\n",
    "featC_idx_column = fc.indicator_column(featC_cat_column)\n",
    "\n",
    "featAxB_cat_column = fc.crossed_column([\"featA\",\"featB\"], hash_bucket_size=10)\n",
    "featAxB_idx_column = fc.indicator_column(featAxB_cat_column)\n",
    "featAxB_emb_column = fc.embedding_column(featAxB_cat_column, dimension=dims_embs_deep)\n",
    "\n",
    "featAxBxC_cat_column = fc.crossed_column([\"featA\",\"featB\",\"featC\"], hash_bucket_size=10)\n",
    "featAxBxC_idx_column = fc.indicator_column(featAxBxC_cat_column)\n",
    "featAxBxC_emb_column = fc.embedding_column(featAxBxC_cat_column, dimension=dims_embs_deep)\n",
    "\n",
    "shaerd_emb_columns = fc.shared_embedding_columns([featAxB_cat_column, featAxBxC_cat_column], dimension=dims_embs_deep)\n",
    "\n",
    "to_be_input = [featA_idx_column,\n",
    "               featB_idx_column,\n",
    "               featC_idx_column,\n",
    "               featAxB_idx_column,\n",
    "               featAxB_emb_column,\n",
    "               featAxBxC_idx_column,\n",
    "               featAxBxC_emb_column]+shaerd_emb_columns\n",
    "inp_layer_list = [fc.input_layer(features,i) for i in to_be_input]\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer()) \n",
    "    inp_list = sess.run(inp_layer_list)\n",
    "    for k,v in zip([i.name for i in to_be_input], inp_list):\n",
    "        print(\">>> {}:\\n{}\".format(k,v))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interact emb by Add | fake Positional-Emb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### basic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-18T07:27:44.217697Z",
     "start_time": "2020-11-18T07:27:44.151838Z"
    },
    "run_control": {
     "marked": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'featA': array([[19],\n",
      "       [19]]), 'pickupHash': array([[15],\n",
      "       [ 8]]), 'time_idx': array([[9],\n",
      "       [1]]), 'numericB': [[0.5], [0.2]], 'numericA': [[0.5], [1.2]], 'link': [[-1], [-1]], 'hour_idx': array([[5],\n",
      "       [2]]), 'whateverB': array([[15],\n",
      "       [ 0]]), 'whateverA': array([[17],\n",
      "       [16]]), 'featB': array([[1],\n",
      "       [2]])}\n"
     ]
    }
   ],
   "source": [
    "##############\n",
    "# h-params\n",
    "##############\n",
    "num_buckets_deep=20\n",
    "dims_embs_deep=4\n",
    "_batch_size=2\n",
    "\n",
    "##############\n",
    "# features\n",
    "##############\n",
    "features = {}\n",
    "features[\"featA\"] = np.random.randint(num_buckets_deep, size=(_batch_size, 1))\n",
    "features[\"featB\"] = np.random.randint(num_buckets_deep, size=(_batch_size, 1))\n",
    "features[\"link\"] = [[-1],[-1]]\n",
    "features[\"pickupHash\"] = np.random.randint(num_buckets_deep, size=(_batch_size, 1))\n",
    "features[\"hour_idx\"] = np.random.randint(num_buckets_deep, size=(_batch_size, 1))\n",
    "features[\"time_idx\"] = np.random.randint(num_buckets_deep, size=(_batch_size, 1))\n",
    "features[\"whateverA\"] = np.random.randint(num_buckets_deep, size=(_batch_size, 1))\n",
    "features[\"whateverB\"] = np.random.randint(num_buckets_deep, size=(_batch_size, 1))\n",
    "features[\"numericA\"] = [[0.5],[1.2]]\n",
    "features[\"numericB\"] = [[0.5],[0.2]]\n",
    "print(features)\n",
    "\n",
    "################\n",
    "# build columns\n",
    "################\n",
    "tf.reset_default_graph()\n",
    "# numeric column\n",
    "fc_numeric = list()\n",
    "fc_numeric.append(fc.numeric_column(\"numericA\", default_value=0))\n",
    "fc_numeric.append(fc.numeric_column(\"numericB\", default_value=0))\n",
    "\n",
    "# codec column\n",
    "emb_feat_list = list()\n",
    "for i in [\"featA\",\"featB\",\"link\",\"pickupHash\",\"hour_idx\",\"time_idx\"]:\n",
    "    emb_feat_list.append(fc.categorical_column_with_identity(i, num_buckets_deep, default_value=0))\n",
    "\n",
    "# emb column\n",
    "fc_shared_emb = fc.shared_embedding_columns(\n",
    "                    emb_feat_list,\n",
    "                    dimension=dims_embs_deep, max_norm=np.sqrt(dims_embs_deep),\n",
    "                    trainable=True)\n",
    "\n",
    "# shared_emb 转成KV结构方便后面操作\n",
    "fc_emb_dict = {seCol.categorical_column.key: seCol for seCol in fc_shared_emb}\n",
    "\n",
    "# 指出将要相加的emb\n",
    "emb_to_add=[['link','hour_idx'],\n",
    "            ['pickupHash','time_idx']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### input_layer搞事情"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-18T07:27:51.079465Z",
     "start_time": "2020-11-18T07:27:48.288221Z"
    }
   },
   "outputs": [],
   "source": [
    "emb_added = []\n",
    "for f1,f2 in emb_to_add:\n",
    "    net1 = fc.input_layer(features, fc_emb_dict[f1])\n",
    "    net2 = fc.input_layer(features, fc_emb_dict[f2])\n",
    "    emb_added.append(net1+net2)\n",
    "\n",
    "emb_to_add_flatten = [i for sub in emb_to_add for i in sub]\n",
    "net_ori = fc.input_layer(features, [v for k,v in fc_emb_dict.items() if k not in emb_to_add_flatten])\n",
    "\n",
    "net = tf.concat([net_ori]+emb_added,axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### run起来看看结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-18T07:28:29.698347Z",
     "start_time": "2020-11-18T07:28:27.741264Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('net_ori',\n",
       " array([[-0.03376518, -0.75637287,  0.01980731,  0.57804877, -0.41253054,\n",
       "          0.76133245,  0.21623057,  0.34377784],\n",
       "        [-0.03376518, -0.75637287,  0.01980731,  0.57804877,  0.5149282 ,\n",
       "         -0.06459129, -0.2458921 , -0.7198298 ]], dtype=float32))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "('emb_added', [array([[-0.2679743 , -0.51504064,  0.3350453 ,  0.47310346],\n",
       "         [ 0.5149282 , -0.06459129, -0.2458921 , -0.7198298 ]],\n",
       "        dtype=float32),\n",
       "  array([[-0.02274224,  1.3028573 , -0.6851692 ,  0.33623815],\n",
       "         [ 0.28992552,  1.1738838 ,  0.27457008,  0.5552162 ]],\n",
       "        dtype=float32)])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "('net',\n",
       " array([[-0.03376518, -0.75637287,  0.01980731,  0.57804877, -0.41253054,\n",
       "          0.76133245,  0.21623057,  0.34377784, -0.2679743 , -0.51504064,\n",
       "          0.3350453 ,  0.47310346, -0.02274224,  1.3028573 , -0.6851692 ,\n",
       "          0.33623815],\n",
       "        [-0.03376518, -0.75637287,  0.01980731,  0.57804877,  0.5149282 ,\n",
       "         -0.06459129, -0.2458921 , -0.7198298 ,  0.5149282 , -0.06459129,\n",
       "         -0.2458921 , -0.7198298 ,  0.28992552,  1.1738838 ,  0.27457008,\n",
       "          0.5552162 ]], dtype=float32))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> details of emb_added\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('link', array([[0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.]], dtype=float32))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "('hour_idx', array([[-0.2679743 , -0.51504064,  0.3350453 ,  0.47310346],\n",
       "        [ 0.5149282 , -0.06459129, -0.2458921 , -0.7198298 ]],\n",
       "       dtype=float32))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "('pickupHash', array([[-0.5041411 ,  0.9702361 , -0.7937973 , -0.33923268],\n",
       "        [ 0.70245606,  0.41255134,  0.05833951,  0.21143836]],\n",
       "       dtype=float32))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "('time_idx', array([[ 0.48139885,  0.33262116,  0.10862812,  0.6754708 ],\n",
       "        [-0.41253054,  0.76133245,  0.21623057,  0.34377784]],\n",
       "       dtype=float32))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \"net_ori\",sess.run(net_ori)\n",
    "    \"emb_added\",sess.run(emb_added)\n",
    "    \"net\",sess.run(net)\n",
    "    print(\">>> details of emb_added\")\n",
    "    for f1,f2 in emb_to_add:\n",
    "        net1 = fc.input_layer(features, fc_emb_dict[f1])\n",
    "        net2 = fc.input_layer(features, fc_emb_dict[f2])\n",
    "        f1,sess.run(net1)\n",
    "        f2,sess.run(net2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interact emb by Inner-product"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### basic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-19T07:51:57.751767Z",
     "start_time": "2020-11-19T07:51:57.715181Z"
    },
    "run_control": {
     "marked": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "featA\n",
      "[[1]\n",
      " [6]]\n",
      "pickupHash\n",
      "[[6]\n",
      " [2]]\n",
      "featB\n",
      "[[5]\n",
      " [5]]\n",
      "link\n",
      "[[-1], [2]]\n",
      "featC\n",
      "[[3]\n",
      " [2]]\n",
      "hour_idx\n",
      "[[0]\n",
      " [5]]\n",
      "time_idx\n",
      "[[4]\n",
      " [5]]\n"
     ]
    }
   ],
   "source": [
    "num_buckets_deep=7\n",
    "dims_embs_deep=3\n",
    "_batch_size=2\n",
    "\n",
    "# features\n",
    "features = {}\n",
    "features[\"featA\"] = np.random.randint(num_buckets_deep, size=(_batch_size, 1))\n",
    "features[\"featB\"] = np.random.randint(num_buckets_deep, size=(_batch_size, 1))\n",
    "features[\"featC\"] = np.random.randint(num_buckets_deep, size=(_batch_size, 1))\n",
    "features[\"link\"] = [[-1],[2]]\n",
    "assert(len(features[\"link\"]) == _batch_size)\n",
    "features[\"pickupHash\"] = np.random.randint(num_buckets_deep, size=(_batch_size, 1))\n",
    "features[\"hour_idx\"] = np.random.randint(num_buckets_deep, size=(_batch_size, 1))\n",
    "features[\"time_idx\"] = np.random.randint(num_buckets_deep, size=(_batch_size, 1))\n",
    "\n",
    "for k,v in features.items():\n",
    "    print(k)\n",
    "    print(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-19T07:51:58.887681Z",
     "start_time": "2020-11-19T07:51:58.859034Z"
    },
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "# numeric column\n",
    "fc_numeric = list()\n",
    "fc_numeric.append(fc.numeric_column(\"numericA\", default_value=0))\n",
    "fc_numeric.append(fc.numeric_column(\"numericB\", default_value=0))\n",
    "\n",
    "# codec column\n",
    "emb_feat_list = list()\n",
    "for i in [\"featA\",\"featB\",\"featC\",\"link\",\"pickupHash\",\"hour_idx\",\"time_idx\"]:\n",
    "    emb_feat_list.append(fc.categorical_column_with_identity(i, num_buckets_deep, default_value=0))\n",
    "\n",
    "# emb column\n",
    "fc_shared_emb = fc.shared_embedding_columns(\n",
    "                    emb_feat_list,\n",
    "                    dimension=dims_embs_deep, max_norm=np.sqrt(dims_embs_deep),\n",
    "                    trainable=True)\n",
    "\n",
    "# shared_emb 转成KV结构方便后面操作\n",
    "fc_emb_dict = {seCol.categorical_column.key: seCol for seCol in fc_shared_emb}\n",
    "\n",
    "# 指出将要相加的emb\n",
    "emb_to_add=[['link','hour_idx'],\n",
    "            ['pickupHash','time_idx']]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### input_layer操作起来\n",
    "\n",
    "format by: [[\"hour_idx\",\"featA\"], [\"hour_idx\",\"featB\"], [\"hour_idx\", \"featC\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-19T07:47:04.744558Z",
     "start_time": "2020-11-19T07:47:03.363075Z"
    },
    "run_control": {
     "marked": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> last-one as demo\n",
      "pickupHash\n",
      "[[-0.06088226 -0.696076   -0.45172015]\n",
      " [-0.915245   -0.16775647  0.3668979 ]]\n",
      "hour_idx\n",
      "[[-0.5740352   0.24671505  0.4600736 ]\n",
      " [ 0.27878988 -0.06976111 -0.5711166 ]]\n",
      "_res | f1 f2做内积得到\n",
      "[[ 0.03494856 -0.17173243 -0.20782451]\n",
      " [-0.25516105  0.01170288 -0.2095415 ]]\n",
      "res | _res做reduce_sum得到\n",
      "[-0.34460837 -0.45299965]\n",
      ">>> emb_dot\n",
      "[array([0.        , 0.28957057], dtype=float32), array([-0.34460837, -0.45299965], dtype=float32)]\n",
      ">>> fm_2nd_res\n",
      "[-0.34460837 -0.16342908]\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "emb_to_innerProduct = [\"link\",\"pickupHash\"]\n",
    "emb_to_innerProduct = [[i,\"hour_idx\"] for i in emb_to_innerProduct]\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    emb_dot = []\n",
    "    for f1,f2 in emb_to_innerProduct:\n",
    "        net1 = fc.input_layer(features, fc_emb_dict[f1])\n",
    "        net2 = fc.input_layer(features, fc_emb_dict[f2])\n",
    "        _res = net1 * net2\n",
    "        res = tf.reduce_sum(net1*net2, axis=1)\n",
    "        emb_dot.append(res)\n",
    "\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    print(\">>> last-one as demo\")\n",
    "    print(f1)\n",
    "    print(sess.run(net1))\n",
    "    print(f2)\n",
    "    print(sess.run(net2))\n",
    "    print(\"_res | f1 f2做内积得到\")\n",
    "    print(sess.run(_res))\n",
    "    print(\"res | _res做reduce_sum得到\")\n",
    "    print(sess.run(res))\n",
    "    \n",
    "    \n",
    "    print(\">>> emb_dot\")\n",
    "    print(sess.run(emb_dot))\n",
    "    fm_2nd_res = tf.reduce_sum(emb_dot, axis=0)\n",
    "    print(\">>> fm_2nd_res\")\n",
    "    print(sess.run(fm_2nd_res))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### input_layer操作起来 | 字典形式\n",
    "\n",
    "format by: {\"hour_idx\": [\"featA\", \"featB\", \"featC\"], \"time_idx\": [\"featA\", \"featB\", \"featC\"]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-19T08:28:52.679488Z",
     "start_time": "2020-11-19T08:28:52.668988Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function batch_norm in module tensorflow.contrib.layers.python.layers.layers:\n",
      "\n",
      "batch_norm(*args, **kwargs)\n",
      "    Adds a Batch Normalization layer from http://arxiv.org/abs/1502.03167.\n",
      "    \n",
      "      \"Batch Normalization: Accelerating Deep Network Training by Reducing\n",
      "      Internal Covariate Shift\"\n",
      "    \n",
      "      Sergey Ioffe, Christian Szegedy\n",
      "    \n",
      "    Can be used as a normalizer function for conv2d and fully_connected. The\n",
      "    normalization is over all but the last dimension if `data_format` is `NHWC`\n",
      "    and all but the second dimension if `data_format` is `NCHW`.  In case of a 2D\n",
      "    tensor this corresponds to the batch dimension, while in case of a 4D tensor\n",
      "    this\n",
      "    corresponds to the batch and space dimensions.\n",
      "    \n",
      "    Note: when training, the moving_mean and moving_variance need to be updated.\n",
      "    By default the update ops are placed in `tf.GraphKeys.UPDATE_OPS`, so they\n",
      "    need to be added as a dependency to the `train_op`. For example:\n",
      "    \n",
      "    ```python\n",
      "      update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
      "      with tf.control_dependencies(update_ops):\n",
      "        train_op = optimizer.minimize(loss)\n",
      "    ```\n",
      "    \n",
      "    One can set updates_collections=None to force the updates in place, but that\n",
      "    can have a speed penalty, especially in distributed settings.\n",
      "    \n",
      "    Args:\n",
      "      inputs: A tensor with 2 or more dimensions, where the first dimension has\n",
      "        `batch_size`. The normalization is over all but the last dimension if\n",
      "        `data_format` is `NHWC` and the second dimension if `data_format` is\n",
      "        `NCHW`.\n",
      "      decay: Decay for the moving average. Reasonable values for `decay` are close\n",
      "        to 1.0, typically in the multiple-nines range: 0.999, 0.99, 0.9, etc.\n",
      "        Lower `decay` value (recommend trying `decay`=0.9) if model experiences\n",
      "        reasonably good training performance but poor validation and/or test\n",
      "        performance. Try zero_debias_moving_mean=True for improved stability.\n",
      "      center: If True, add offset of `beta` to normalized tensor. If False, `beta`\n",
      "        is ignored.\n",
      "      scale: If True, multiply by `gamma`. If False, `gamma` is\n",
      "        not used. When the next layer is linear (also e.g. `nn.relu`), this can be\n",
      "        disabled since the scaling can be done by the next layer.\n",
      "      epsilon: Small float added to variance to avoid dividing by zero.\n",
      "      activation_fn: Activation function, default set to None to skip it and\n",
      "        maintain a linear activation.\n",
      "      param_initializers: Optional initializers for beta, gamma, moving mean and\n",
      "        moving variance.\n",
      "      param_regularizers: Optional regularizer for beta and gamma.\n",
      "      updates_collections: Collections to collect the update ops for computation.\n",
      "        The updates_ops need to be executed with the train_op.\n",
      "        If None, a control dependency would be added to make sure the updates are\n",
      "        computed in place.\n",
      "      is_training: Whether or not the layer is in training mode. In training mode\n",
      "        it would accumulate the statistics of the moments into `moving_mean` and\n",
      "        `moving_variance` using an exponential moving average with the given\n",
      "        `decay`. When it is not in training mode then it would use the values of\n",
      "        the `moving_mean` and the `moving_variance`.\n",
      "      reuse: Whether or not the layer and its variables should be reused. To be\n",
      "        able to reuse the layer scope must be given.\n",
      "      variables_collections: Optional collections for the variables.\n",
      "      outputs_collections: Collections to add the outputs.\n",
      "      trainable: If `True` also add variables to the graph collection\n",
      "        `GraphKeys.TRAINABLE_VARIABLES` (see `tf.Variable`).\n",
      "      batch_weights: An optional tensor of shape `[batch_size]`,\n",
      "        containing a frequency weight for each batch item. If present,\n",
      "        then the batch normalization uses weighted mean and\n",
      "        variance. (This can be used to correct for bias in training\n",
      "        example selection.)\n",
      "      fused: if `None` or `True`, use a faster, fused implementation if possible.\n",
      "        If `False`, use the system recommended implementation.\n",
      "      data_format: A string. `NHWC` (default) and `NCHW` are supported.\n",
      "      zero_debias_moving_mean: Use zero_debias for moving_mean. It creates a new\n",
      "        pair of variables 'moving_mean/biased' and 'moving_mean/local_step'.\n",
      "      scope: Optional scope for `variable_scope`.\n",
      "      renorm: Whether to use Batch Renormalization\n",
      "        (https://arxiv.org/abs/1702.03275). This adds extra variables during\n",
      "        training. The inference is the same for either value of this parameter.\n",
      "      renorm_clipping: A dictionary that may map keys 'rmax', 'rmin', 'dmax' to\n",
      "        scalar `Tensors` used to clip the renorm correction. The correction\n",
      "        `(r, d)` is used as `corrected_value = normalized_value * r + d`, with\n",
      "        `r` clipped to [rmin, rmax], and `d` to [-dmax, dmax]. Missing rmax, rmin,\n",
      "        dmax are set to inf, 0, inf, respectively.\n",
      "      renorm_decay: Momentum used to update the moving means and standard\n",
      "        deviations with renorm. Unlike `momentum`, this affects training\n",
      "        and should be neither too small (which would add noise) nor too large\n",
      "        (which would give stale estimates). Note that `decay` is still applied\n",
      "        to get the means and variances for inference.\n",
      "      adjustment: A function taking the `Tensor` containing the (dynamic) shape of\n",
      "        the input tensor and returning a pair (scale, bias) to apply to the\n",
      "        normalized values (before gamma and beta), only during training. For\n",
      "        example,\n",
      "          `adjustment = lambda shape: (\n",
      "            tf.random_uniform(shape[-1:], 0.93, 1.07),\n",
      "            tf.random_uniform(shape[-1:], -0.1, 0.1))`\n",
      "        will scale the normalized value by up to 7% up or down, then shift the\n",
      "        result by up to 0.1 (with independent scaling and bias for each feature\n",
      "        but shared across all examples), and finally apply gamma and/or beta. If\n",
      "        `None`, no adjustment is applied.\n",
      "    \n",
      "    Returns:\n",
      "      A `Tensor` representing the output of the operation.\n",
      "    \n",
      "    Raises:\n",
      "      ValueError: If `data_format` is neither `NHWC` nor `NCHW`.\n",
      "      ValueError: If the rank of `inputs` is undefined.\n",
      "      ValueError: If rank or channels dimension of `inputs` is undefined.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(tf.contrib.layers.batch_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-20T04:46:43.914529Z",
     "start_time": "2020-11-20T04:46:42.347242Z"
    }
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "from tensorflow.python.keras import regularizers\n",
    "from tensorflow.python.ops import init_ops\n",
    "emb_to_innerProduct = {\"hour_idx\":[\"featA\",\"featB\",\"featC\",\"link\",\"pickupHash\"],\n",
    "                       \"time_idx\":[\"featA\",\"featB\",\"featC\",\"link\",\"pickupHash\"],}\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    emb_dot = []\n",
    "    for k, v in emb_to_innerProduct.items():\n",
    "        # k: f1  v: [featA,featB,featC]\n",
    "        # cross: f1*featA + f1*featB + f1*featC \n",
    "        emb1 = fc.input_layer(features, fc_emb_dict[k])\n",
    "        emb2 = fc.input_layer(features, [fc_emb_dict[i] for i in v])\n",
    "        emb1_r = tf.expand_dims(emb1, -1)\n",
    "        emb2_r = tf.reshape(emb2,[-1, len(v), emb1.shape[-1]])\n",
    "        res = tf.squeeze(tf.matmul(emb2_r, emb1_r))\n",
    "        emb_dot.append(res)\n",
    "    all_emb_dot = tf.concat(emb_dot, axis=1)\n",
    "#     fm_2nd_res = tf.reduce_sum(tf.concat(emb_dot, axis=1), axis=1)\n",
    "#     fm_2nd_res = tf.reshape(fm_2nd_res, [-1,1])\n",
    "    \n",
    "    logits = tf.layers.dense(all_emb_dot, 4, kernel_initializer=init_ops.glorot_uniform_initializer(),\n",
    "                        activation=None, kernel_regularizer=regularizers.l2(0.01),\n",
    "                        name=\"logits\")\n",
    "\n",
    "    all_emb_dot_bn = tf.contrib.layers.batch_norm(all_emb_dot,is_training=False,updates_collections=None)\n",
    "    logits_bn = tf.layers.dense(all_emb_dot_bn, 4, kernel_initializer=init_ops.glorot_uniform_initializer(),\n",
    "                        activation=None, kernel_regularizer=regularizers.l2(0.01),\n",
    "                        name=\"logits_bn\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-20T04:48:00.380741Z",
     "start_time": "2020-11-20T04:47:59.599829Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'MatMul_5:0' shape=(2, 5, 1) dtype=float32>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'Squeeze_1:0' shape=(2, 5) dtype=float32>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "array([[[-0.4614984 ],\n",
       "        [-0.13459314],\n",
       "        [ 0.8183997 ],\n",
       "        [ 0.        ],\n",
       "        [-0.1535201 ]],\n",
       "\n",
       "       [[-0.03328903],\n",
       "        [ 0.82868665],\n",
       "        [ 0.55066764],\n",
       "        [ 0.55066764],\n",
       "        [ 0.55066764]]], dtype=float32)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "array([[-0.4614984 , -0.13459314,  0.8183997 ,  0.        , -0.1535201 ],\n",
       "       [-0.03328903,  0.82868665,  0.55066764,  0.55066764,  0.55066764]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    tf.matmul(emb2_r, emb1_r)\n",
    "    res\n",
    "    sess.run(tf.matmul(emb2_r, emb1_r))\n",
    "    sess.run(tf.reshape(tf.matmul(emb2_r, emb1_r),[-1, tf.matmul(emb2_r, emb1_r).shape[1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-19T08:34:04.898297Z",
     "start_time": "2020-11-19T08:34:00.537240Z"
    },
    "run_control": {
     "marked": true
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time_idx\n",
      "['featA', 'featB', 'featC', 'link', 'pickupHash']\n",
      "emb1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 0.41871086,  0.52999526,  0.4945093 ],\n",
       "       [-0.629783  , -0.06730537, -0.3083893 ]], dtype=float32)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "emb2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 0.00486276,  0.46456635,  0.63068444, -0.629783  , -0.06730537,\n",
       "        -0.3083893 ,  0.79900736, -0.19154112, -0.7691954 ,  0.        ,\n",
       "         0.        ,  0.        ,  0.26811442, -0.47049198, -0.89255047],\n",
       "       [ 0.26811442, -0.47049198, -0.89255047, -0.629783  , -0.06730537,\n",
       "        -0.3083893 , -0.27455962,  0.29733664, -0.7572018 , -0.27455962,\n",
       "         0.29733664, -0.7572018 , -0.27455962,  0.29733664, -0.7572018 ]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "emb1_r\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[[ 0.41871086],\n",
       "        [ 0.52999526],\n",
       "        [ 0.4945093 ]],\n",
       "\n",
       "       [[-0.629783  ],\n",
       "        [-0.06730537],\n",
       "        [-0.3083893 ]]], dtype=float32)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "emb2_r\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[[ 0.00486276,  0.46456635,  0.63068444],\n",
       "        [-0.629783  , -0.06730537, -0.3083893 ],\n",
       "        [ 0.79900736, -0.19154112, -0.7691954 ],\n",
       "        [ 0.        ,  0.        ,  0.        ],\n",
       "        [ 0.26811442, -0.47049198, -0.89255047]],\n",
       "\n",
       "       [[ 0.26811442, -0.47049198, -0.89255047],\n",
       "        [-0.629783  , -0.06730537, -0.3083893 ],\n",
       "        [-0.27455962,  0.29733664, -0.7572018 ],\n",
       "        [-0.27455962,  0.29733664, -0.7572018 ],\n",
       "        [-0.27455962,  0.29733664, -0.7572018 ]]], dtype=float32)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "res\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 0.5601334 , -0.45186988, -0.14733711,  0.        , -0.57847065],\n",
       "       [ 0.13806576,  0.49626055,  0.38641354,  0.38641354,  0.38641354]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> 只看batch里的第一个\n",
      "emb1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.41871086, 0.52999526, 0.4945093 ], dtype=float32)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "emb2_r\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 0.00486276,  0.46456635,  0.63068444],\n",
       "       [-0.629783  , -0.06730537, -0.3083893 ],\n",
       "       [ 0.79900736, -0.19154112, -0.7691954 ],\n",
       "       [ 0.        ,  0.        ,  0.        ],\n",
       "       [ 0.26811442, -0.47049198, -0.89255047]], dtype=float32)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "res\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 0.5601334 , -0.45186988, -0.14733711,  0.        , -0.57847065],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "emb_dot\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[array([[ 0.12446526, -0.48983523,  0.30339354,  0.        , -0.03028464],\n",
       "        [ 0.13806576,  0.49626055,  0.38641354,  0.38641354,  0.38641354]],\n",
       "       dtype=float32),\n",
       " array([[ 0.5601334 , -0.45186988, -0.14733711,  0.        , -0.57847065],\n",
       "        [ 0.13806576,  0.49626055,  0.38641354,  0.38641354,  0.38641354]],\n",
       "       dtype=float32)]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all_emb_dot | concat emb_dot\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 0.12446526, -0.48983523,  0.30339354,  0.        , -0.03028464,\n",
       "         0.5601334 , -0.45186988, -0.14733711,  0.        , -0.57847065],\n",
       "       [ 0.13806576,  0.49626055,  0.38641354,  0.38641354,  0.38641354,\n",
       "         0.13806576,  0.49626055,  0.38641354,  0.38641354,  0.38641354]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logits | tf.concat(all_emb_dot, axis=1) & 4分类\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[-0.03423132,  0.16598332,  0.17956713, -0.2545311 ],\n",
       "       [ 0.16397662, -0.02424657, -1.1411203 ,  0.46004915]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all_emb_dot_bn | concat emb_dot\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 0.12440307, -0.4895905 ,  0.30324197,  0.        , -0.03026951,\n",
       "         0.55985355, -0.45164412, -0.1472635 ,  0.        , -0.5781816 ],\n",
       "       [ 0.13799678,  0.49601263,  0.38622048,  0.38622048,  0.38622048,\n",
       "         0.13799678,  0.49601263,  0.38622048,  0.38622048,  0.38622048]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logits_bn | tf.concat(all_emb_dot_bn, axis=1) & 4分类\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[-0.04481216,  0.5127205 , -0.04209074, -0.35768437],\n",
       "       [-0.33998445, -0.2264207 , -0.0203252 , -0.3708216 ]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(k)\n",
    "print(v)\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    print(\"emb1\")\n",
    "    sess.run(emb1)\n",
    "    print(\"emb2\")\n",
    "    sess.run(emb2)\n",
    "    print(\"emb1_r\")\n",
    "    sess.run(emb1_r)\n",
    "    print(\"emb2_r\")\n",
    "    sess.run(emb2_r)\n",
    "    print(\"res\")\n",
    "    sess.run(res)\n",
    "    \n",
    "    print(\">>> 只看batch里的第一个\")\n",
    "    print(\"emb1\")\n",
    "    sess.run(emb1)[0]\n",
    "#     print(\"emb2\")\n",
    "#     sess.run(emb2)[0]\n",
    "#     print(\"emb1_r\")\n",
    "#     sess.run(emb1_r)[0]\n",
    "    print(\"emb2_r\")\n",
    "    sess.run(emb2_r)[0]\n",
    "    print(\"res\")\n",
    "    sess.run(res)[0]\n",
    "    \n",
    "    print(\"emb_dot\")\n",
    "    sess.run(emb_dot)\n",
    "    print(\"all_emb_dot | concat emb_dot\")\n",
    "    sess.run(all_emb_dot)\n",
    "#     print(\"tf.concat(emb_dot, axis=0)\")\n",
    "#     sess.run(tf.concat(emb_dot, axis=1))\n",
    "#     print(\"fm_2nd_res | tf.reduce_sum(tf.concat(emb_dot, axis=1), axis=1)\")\n",
    "#     sess.run(fm_2nd_res)\n",
    "    print(\"logits | tf.concat(all_emb_dot, axis=1) & 4分类\")\n",
    "    sess.run(logits)\n",
    "\n",
    "    print(\"all_emb_dot_bn | concat emb_dot\")\n",
    "    sess.run(all_emb_dot_bn)\n",
    "    print(\"logits_bn | tf.concat(all_emb_dot_bn, axis=1) & 4分类\")\n",
    "    sess.run(logits_bn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-19T08:30:45.214216Z",
     "start_time": "2020-11-19T08:30:45.203254Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.2338353800000001"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "3.66484891"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum([ 0.12115489,  0.39662713, -0.45657066,  0.        , -0.11924169,\n",
    "        -0.06230509, -0.43095323,  0.20267147,  0.        ,  0.1147818 ])\n",
    "sum([ 0.98669964, -0.9935233 , -0.96973526,  0.9557099 ,  0.80604684,\n",
    "         0.95365196, -0.9989523 ,  0.9882004 ,  0.9557099 ,  0.98104113])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-19T07:48:16.846625Z",
     "start_time": "2020-11-19T07:48:16.827614Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.5709219853572799, -0.013578032623862401, -0.093756372889564]"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "0.4635875798438535"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# copy 出来手工计算\n",
    "emb1 = [ 0.91038424, -0.03854756, -0.2341526 ]\n",
    "emb2 = [ 0.627122  ,  0.35224104,  0.40040714]\n",
    "\n",
    "[a*b for a,b in zip(emb1,emb2)]\n",
    "sum([a*b for a,b in zip(emb1,emb2)])\n",
    "\n",
    "# sum([ 0.3972234 , -0.308128  ,  0.        , -0.07037204, -0.308128  ,\n",
    "#          1.4594309 ,  0.        , -0.29277474])\n",
    "# sum([ 0.5564581 ,  0.41053557,  0.        ,  0.5564581 ,  0.4649981 , 0.17164305,  0.        ,  0.4649981 ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \"net\",sess.run(net)\n",
    "    \"emb_added\",sess.run(emb_added)\n",
    "    \"net_final\",sess.run(net_final)\n",
    "    print(\">>> details of emb_added\")\n",
    "    for f1,f2 in emb_to_add:\n",
    "        net1 = fc.input_layer(features, fc_emb_dict[f1])\n",
    "        net2 = fc.input_layer(features, fc_emb_dict[f2])\n",
    "        f1,sess.run(net1)\n",
    "        f2,sess.run(net2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def fm(inputs):\n",
    "#     if K.ndim(inputs) != 3:\n",
    "#         raise ValueError(\n",
    "#             \"Unexpected inputs dimensions %d, expect to be 3 dimensions\"\n",
    "#             % (K.ndim(inputs)))\n",
    "\n",
    "#     concated_embeds_value = inputs\n",
    "\n",
    "#     square_of_sum = tf.square(reduce_sum(\n",
    "#         concated_embeds_value, axis=1, keep_dims=True))\n",
    "#     sum_of_square = reduce_sum(\n",
    "#         concated_embeds_value * concated_embeds_value, axis=1, keep_dims=True)\n",
    "#     cross_term = square_of_sum - sum_of_square\n",
    "#     cross_term = 0.5 * reduce_sum(cross_term, axis=2, keep_dims=False)\n",
    "\n",
    "#     return cross_term\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interact emb by Co-Action Unit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### basic | imitation hparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-02T11:06:25.665748Z",
     "start_time": "2020-12-02T11:06:25.650280Z"
    },
    "run_control": {
     "marked": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'batch_size': 2,\n",
       " 'emb_mlp_feature_size': 100,\n",
       " 'emb_mlp_units': [8, 8, 8],\n",
       " 'embedding_feature_size': 20,\n",
       " 'embedding_size': 4,\n",
       " 'n_classes': [4, 4]}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hparams = {}\n",
    "hparams['batch_size'] = 2\n",
    "hparams['n_classes'] = [4,4]  # idx=1是跨路label\n",
    "hparams['embedding_size'] = 4\n",
    "hparams[\"embedding_feature_size\"]=20\n",
    "hparams['emb_mlp_units'] = [8,8,8]\n",
    "hparams['emb_mlp_feature_size'] = 100\n",
    "\n",
    "hparams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### basic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-02T11:06:25.791993Z",
     "start_time": "2020-12-02T11:06:25.733144Z"
    },
    "run_control": {
     "marked": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'featA': array([[1],\n",
      "       [6]]), 'pickupHash': array([[18],\n",
      "       [ 5]]), 'time_idx': array([[16],\n",
      "       [17]]), 'numericB': [[0.5], [0.2]], 'numericA': [[0.5], [1.2]], 'link': [[-1], [-1]], 'hour_idx': array([[15],\n",
      "       [ 3]]), 'whateverB': array([[14],\n",
      "       [14]]), 'whateverA': array([[5],\n",
      "       [6]]), 'featB': array([[ 5],\n",
      "       [17]])}\n"
     ]
    }
   ],
   "source": [
    "##############\n",
    "# h-params\n",
    "##############\n",
    "num_buckets_deep=hparams[\"embedding_feature_size\"]\n",
    "dims_embs_deep=hparams['embedding_size']\n",
    "_batch_size=hparams['batch_size']\n",
    "\n",
    "##############\n",
    "# features\n",
    "##############\n",
    "features = {}\n",
    "features[\"featA\"] = np.random.randint(num_buckets_deep, size=(_batch_size, 1))\n",
    "features[\"featB\"] = np.random.randint(num_buckets_deep, size=(_batch_size, 1))\n",
    "features[\"link\"] = [[-1],[-1]]\n",
    "features[\"pickupHash\"] = np.random.randint(num_buckets_deep, size=(_batch_size, 1))\n",
    "features[\"hour_idx\"] = np.random.randint(num_buckets_deep, size=(_batch_size, 1))\n",
    "features[\"time_idx\"] = np.random.randint(num_buckets_deep, size=(_batch_size, 1))\n",
    "features[\"whateverA\"] = np.random.randint(num_buckets_deep, size=(_batch_size, 1))\n",
    "features[\"whateverB\"] = np.random.randint(num_buckets_deep, size=(_batch_size, 1))\n",
    "features[\"numericA\"] = [[0.5],[1.2]]\n",
    "features[\"numericB\"] = [[0.5],[0.2]]\n",
    "print(features)\n",
    "\n",
    "################\n",
    "# build columns\n",
    "################\n",
    "tf.reset_default_graph()\n",
    "# numeric column\n",
    "fc_numeric = list()\n",
    "fc_numeric.append(fc.numeric_column(\"numericA\", default_value=0))\n",
    "fc_numeric.append(fc.numeric_column(\"numericB\", default_value=0))\n",
    "\n",
    "# codec column\n",
    "emb_feat_list = list()\n",
    "for i in [\"featA\",\"featB\",\"link\",\"pickupHash\",\"hour_idx\",\"time_idx\"]:\n",
    "    emb_feat_list.append(fc.categorical_column_with_identity(i, num_buckets_deep, default_value=0))\n",
    "\n",
    "# emb column\n",
    "fc_shared_emb = fc.shared_embedding_columns(\n",
    "                    emb_feat_list,\n",
    "                    dimension=dims_embs_deep, max_norm=np.sqrt(dims_embs_deep),\n",
    "                    trainable=True)\n",
    "\n",
    "# shared_emb 转成KV结构方便后面操作\n",
    "fc_emb_dict = {seCol.categorical_column.key: seCol for seCol in fc_shared_emb}\n",
    "\n",
    "# 指出将要操作的emb\n",
    "emb_to_add=[['link','hour_idx'],\n",
    "            ['pickupHash','time_idx']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-02T11:06:27.391034Z",
     "start_time": "2020-12-02T11:06:27.356136Z"
    },
    "code_folding": [
     0,
     33
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[_EmbeddingColumn(categorical_column=_HashedCategoricalColumn(key='pickupHash', hash_bucket_size=100, dtype=tf.int64), dimension=32, combiner='mean', layer_creator=<function _creator at 0x7f94cdf64b18>, ckpt_to_load_from=None, tensor_name_in_ckpt=None, max_norm=5.656854249492381, trainable=True),\n",
       " _EmbeddingColumn(categorical_column=_HashedCategoricalColumn(key='pickupHash', hash_bucket_size=100, dtype=tf.int64), dimension=64, combiner='mean', layer_creator=<function _creator at 0x7f94cdf64c08>, ckpt_to_load_from=None, tensor_name_in_ckpt=None, max_norm=8.0, trainable=True),\n",
       " _EmbeddingColumn(categorical_column=_HashedCategoricalColumn(key='pickupHash', hash_bucket_size=100, dtype=tf.int64), dimension=64, combiner='mean', layer_creator=<function _creator at 0x7f94cdf64938>, ckpt_to_load_from=None, tensor_name_in_ckpt=None, max_norm=8.0, trainable=True)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "[_EmbeddingColumn(categorical_column=_HashedCategoricalColumn(key='pickupHash', hash_bucket_size=100, dtype=tf.int64), dimension=8, combiner='mean', layer_creator=<function _creator at 0x7f94cdf64ed8>, ckpt_to_load_from=None, tensor_name_in_ckpt=None, max_norm=2.8284271247461903, trainable=True),\n",
       " _EmbeddingColumn(categorical_column=_HashedCategoricalColumn(key='pickupHash', hash_bucket_size=100, dtype=tf.int64), dimension=8, combiner='mean', layer_creator=<function _creator at 0x7f94cdf64e60>, ckpt_to_load_from=None, tensor_name_in_ckpt=None, max_norm=2.8284271247461903, trainable=True),\n",
       " _EmbeddingColumn(categorical_column=_HashedCategoricalColumn(key='pickupHash', hash_bucket_size=100, dtype=tf.int64), dimension=8, combiner='mean', layer_creator=<function _creator at 0x7f94cdf64d70>, ckpt_to_load_from=None, tensor_name_in_ckpt=None, max_norm=2.8284271247461903, trainable=True)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def co_action_unit_fc(hparams):\n",
    "    # fc_target_item = fc.categorical_column_with_identity(hparams[\"emb_mlp_feat\"], hparams['mlp_embedding_feature_size'], default_value=0)\n",
    "    fc_target_item = fc.categorical_column_with_hash_bucket(\"pickupHash\", hash_bucket_size=hparams['emb_mlp_feature_size'], dtype=tf.int64)\n",
    "\n",
    "    fc_emb_mlp_kernel_list = []\n",
    "    fc_emb_mlp_bias_list = []\n",
    "    inp_size = hparams['embedding_size']\n",
    "    for idx, i in enumerate(hparams['emb_mlp_units']):\n",
    "        kernel_size = inp_size*i\n",
    "        bias_size = i\n",
    "        _fc_emb_kernel = fc.embedding_column(fc_target_item,\n",
    "                                             dimension=kernel_size, max_norm=np.sqrt(kernel_size),\n",
    "                                             trainable=True)\n",
    "        fc_emb_mlp_kernel_list.append(_fc_emb_kernel)\n",
    "        _fc_emb_bias = fc.embedding_column(fc_target_item,\n",
    "                                           dimension=bias_size, max_norm=np.sqrt(bias_size),\n",
    "                                           trainable=True)\n",
    "        fc_emb_mlp_bias_list.append(_fc_emb_bias)\n",
    "\n",
    "        # # >>>deubg\n",
    "        # kernel = np.random.random([inp_size,i])\n",
    "        # bias = np.random.random(bias_size)\n",
    "        # print(\"kernel shape:%s size:%s\" % (kernel.shape, kernel_size))\n",
    "        # print(\"bias shape:%s size:%s\" % (bias.shape, bias_size))\n",
    "        # # <<<deubg\n",
    "\n",
    "        inp_size = i  # IMPORTANT\n",
    "\n",
    "    # [fc_emb.dimension for fc_emb in fc_emb_mlp_kernel_list]\n",
    "    # [fc_emb.dimension for fc_emb in fc_emb_mlp_bias_list]\n",
    "    return fc_emb_mlp_kernel_list, fc_emb_mlp_bias_list\n",
    "fc_emb_mlp_kernel_list, fc_emb_mlp_bias_list = co_action_unit_fc(hparams)\n",
    "fc_emb_mlp_kernel_list\n",
    "fc_emb_mlp_bias_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-02T11:07:36.930994Z",
     "start_time": "2020-12-02T11:07:36.519843Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'Reshape_2:0' shape=(2, 4, 8) dtype=float32>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'Reshape_3:0' shape=(2, 8, 8) dtype=float32>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'Reshape_4:0' shape=(2, 8, 8) dtype=float32>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'ExpandDims_5:0' shape=(2, 1, 8) dtype=float32>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fc_emb_inp_list = [fc_emb_dict[i] for i in ['featA','featB']]\n",
    "\n",
    "emb_inp = fc.input_layer(features, fc_emb_inp)  # (batch_size, D)\n",
    "emb_inp = tf.math.pow(emb_inp, 1) + tf.math.pow(emb_inp, 2) + tf.math.pow(emb_inp, 3)\n",
    "emb_inp = tf.expand_dims(emb_inp, 1)\n",
    "\n",
    "net = emb_inp\n",
    "for idx, fc_emb in enumerate(fc_emb_mlp_kernel_list):\n",
    "    emb_mlp_kernel = fc.input_layer(features, fc_emb)\n",
    "    cur_output = hparams['emb_mlp_units'][idx]\n",
    "    emb_mlp_kernel = tf.reshape(emb_mlp_kernel, [-1, fc_emb.dimension/cur_output, cur_output])\n",
    "    emb_mlp_kernel\n",
    "#     emb_mlp_bias = fc.input_layer(features, fc_emb_mlp_bias_list[idx])\n",
    "#     emb_mlp_bias = tf.expand_dims(emb_mlp_bias, 1)\n",
    "#     # wx+b\n",
    "#     res = tf.einsum(\"bik,bkj->bij\", net, emb_mlp_kernel) + emb_mlp_bias\n",
    "#     # activation\n",
    "#     res = tf.nn.relu(res)\n",
    "#     net = res\n",
    "# net = tf.squeeze(net)\n",
    "net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-02T11:09:21.319787Z",
     "start_time": "2020-12-02T11:09:19.985205Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Squeeze:0\", shape=(2, 8), dtype=float32)\n",
      "Tensor(\"Squeeze_1:0\", shape=(2, 8), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "def co_action_unit(features, fc_emb_inp, fc_emb_mlp_kernel_list, fc_emb_mlp_bias_list, hparams, mode):\n",
    "    \"\"\"\n",
    "\n",
    "    :param features:\n",
    "    :param fc_emb_inp: emb_feature_column\n",
    "    :param fc_emb_mlp_kernel_list: list[emb_feature_column]\n",
    "    :param fc_emb_mlp_bias_list: list[emb_feature_column]\n",
    "    :param hparams: params\n",
    "    :param mode: TRAIN EVALUATION .etc\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    emb_inp = fc.input_layer(features, fc_emb_inp)  # (batch_size, D)\n",
    "    emb_inp = tf.math.pow(emb_inp, 1) + tf.math.pow(emb_inp, 2) + tf.math.pow(emb_inp, 3)\n",
    "    emb_inp = tf.expand_dims(emb_inp, 1)\n",
    "\n",
    "    net = emb_inp\n",
    "    for idx, fc_emb in enumerate(fc_emb_mlp_kernel_list):\n",
    "        emb_mlp_kernel = fc.input_layer(features, fc_emb)\n",
    "        cur_output = hparams['emb_mlp_units'][idx]\n",
    "        emb_mlp_kernel = tf.reshape(emb_mlp_kernel, [-1, fc_emb.dimension/cur_output, cur_output])\n",
    "        emb_mlp_bias = fc.input_layer(features, fc_emb_mlp_bias_list[idx])\n",
    "        emb_mlp_bias = tf.expand_dims(emb_mlp_bias, 1)\n",
    "        # wx+b\n",
    "        res = tf.einsum(\"bik,bkj->bij\", net, emb_mlp_kernel) + emb_mlp_bias\n",
    "        # activation\n",
    "        res = tf.nn.relu(res)\n",
    "        net = res\n",
    "    net = tf.squeeze(net)\n",
    "    return net\n",
    "\n",
    "fc_emb_inp_list = [fc_emb_dict[i] for i in ['featA','featB']]\n",
    "emb_can_list = []\n",
    "\n",
    "for fc_emb_inp in fc_emb_inp_list:\n",
    "        emb_can = co_action_unit(features, fc_emb_inp, fc_emb_mlp_kernel_list, fc_emb_mlp_bias_list, hparams, 'TRAIN')\n",
    "        print(emb_can)\n",
    "        emb_can_list.append(emb_can)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-02T11:10:16.056334Z",
     "start_time": "2020-12-02T11:10:15.543578Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'input_layer_28/concat:0' shape=(2, 24) dtype=float32>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor 'Squeeze:0' shape=(2, 8) dtype=float32>,\n",
       " <tf.Tensor 'Squeeze_1:0' shape=(2, 8) dtype=float32>]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'concat:0' shape=(2, 40) dtype=float32>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = fc.input_layer(features, fc_emb_dict.values())\n",
    "net\n",
    "emb_can_list\n",
    "tf.concat([net] + emb_can_list ,axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### basic | emb_mlp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "kernel一张表，bias一张表"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-02T06:34:58.124973Z",
     "start_time": "2020-12-02T06:34:58.069293Z"
    },
    "code_folding": [
     0
    ],
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true,
     "marked": true
    }
   },
   "outputs": [],
   "source": [
    "# emb_mlp size的计算，输入是普通的emb_size输出是n_class，中间是N个全连接\n",
    "hparams['emb_mlp_units'] = [8]*3  # 先用桶状结构吧，有利于后面reshape然后用矩阵乘法计算更高效（不然要用slice）\n",
    "hparams['emb_mlp_bias'] = [8]*3\n",
    "\n",
    "emb_mlp_kernel_size_1st = 0\n",
    "emb_mlp_kernel_size = 0\n",
    "emb_mlp_bias_size = 0\n",
    "inp_size = hparams['embedding_size']\n",
    "for idx,i in enumerate(hparams['emb_mlp_units']):\n",
    "    kernel_size = inp_size*i\n",
    "    bias_size = i\n",
    "    kernel = np.random.random([inp_size,i])\n",
    "    bias = np.random.random(bias_size)\n",
    "    print(\"shape:%s size:%s\" % (kernel.shape, kernel_size))\n",
    "    print(\"shape:%s size:%s\" % (bias.shape, bias_size))\n",
    "    if idx == 0:\n",
    "        emb_mlp_kernel_size_1st = kernel_size\n",
    "    \n",
    "    total_emb_mlp_kernel_size += kernel_size\n",
    "    total_emb_mlp_bias_size += bias_size\n",
    "    inp_size = i\n",
    "\n",
    "hparams['emb_mlp_kernel_size'] = total_emb_mlp_kernel_size\n",
    "hparams['emb_mlp_bias_size'] = total_emb_mlp_bias_size\n",
    "total_emb_mlp_kernel_size\n",
    "total_emb_mlp_bias_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "每个kernel各自一张表，每个bias各自一张表"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-02T07:05:43.154016Z",
     "start_time": "2020-12-02T07:05:43.146280Z"
    }
   },
   "outputs": [],
   "source": [
    "# emb_mlp size的计算，输入是普通的emb_size输出是n_class，中间是N个全连接\n",
    "hparams['emb_mlp_units'] = [8,16,8]\n",
    "# 应该是独立去重计算，但是这里不像重新建索引，就还是偷个懒用之前的size\n",
    "hparams[\"mlp_embedding_feature_size\"] = hparams[\"embedding_feature_size\"]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-02T07:05:45.178269Z",
     "start_time": "2020-12-02T07:05:43.851577Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kernel shape:(4, 8) size:32\n",
      "bias shape:(8,) size:8\n",
      "kernel shape:(8, 16) size:128\n",
      "bias shape:(16,) size:16\n",
      "kernel shape:(16, 8) size:128\n",
      "bias shape:(8,) size:8\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[32, 128, 128]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "[8, 16, 8]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'ExpandDims:0' shape=(2, 1, 4) dtype=float32>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> at idx: 0 \n",
      ">>> at idx: 1 \n",
      ">>> at idx: 2 \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'Relu_2:0' shape=(2, 1, 8) dtype=float32>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "targetItemCol = fc.categorical_column_with_hash_bucket(\"pickupHash\", hash_bucket_size=hparams['embedding_feature_size'], dtype=tf.int64)\n",
    "fc_emb_mlp_kernel = []\n",
    "fc_emb_mlp_bias = []\n",
    "inp_size = hparams['embedding_size']\n",
    "for idx,i in enumerate(hparams['emb_mlp_units']):\n",
    "    kernel_size = inp_size*i\n",
    "    bias_size = i\n",
    "    _fc_emb_kernel = fc.embedding_column(targetItemCol,\n",
    "                                         dimension=kernel_size, max_norm=np.sqrt(kernel_size),\n",
    "                                         trainable=True)\n",
    "    fc_emb_mlp_kernel.append(_fc_emb_kernel)\n",
    "    _fc_emb_bias = fc.embedding_column(targetItemCol,\n",
    "                                       dimension=bias_size, max_norm=np.sqrt(bias_size),\n",
    "                                       trainable=True)\n",
    "    fc_emb_mlp_bias.append(_fc_emb_bias)\n",
    "    \n",
    "    # >>>deubg\n",
    "    kernel = np.random.random([inp_size,i])\n",
    "    bias = np.random.random(bias_size)\n",
    "    print(\"kernel shape:%s size:%s\" % (kernel.shape, kernel_size))\n",
    "    print(\"bias shape:%s size:%s\" % (bias.shape, bias_size))\n",
    "    # <<<deubg\n",
    "    \n",
    "    inp_size = i  # IMPORTANT\n",
    "\n",
    "[fc_emb.dimension for fc_emb in fc_emb_mlp_kernel]\n",
    "[fc_emb.dimension for fc_emb in fc_emb_mlp_bias]\n",
    "\n",
    "\n",
    "# 输入侧的emb column\n",
    "# fc_emb_inp = [fc_emb_dict[i] for i in ['link','hour_idx']]\n",
    "fc_emb_inp = fc_emb_dict['hour_idx']  # 一个个来吧，考虑到还要做1~3次幂\n",
    "emb_inp = fc.input_layer(features, fc_emb_inp) # (batch_size, D)\n",
    "emb_inp = tf.math.pow(emb_inp,1) + tf.math.pow(emb_inp,2) + tf.math.pow(emb_inp,3)\n",
    "emb_inp = tf.expand_dims(emb_inp,1)\n",
    "emb_inp\n",
    "\n",
    "inp = emb_inp\n",
    "for idx,fc_emb in enumerate(fc_emb_mlp_kernel):\n",
    "    print(\">>> at idx: %s \" % idx)\n",
    "    emb_mlp_kernel = fc.input_layer(features, fc_emb)\n",
    "    cur_output = hparams['emb_mlp_units'][idx]\n",
    "    emb_mlp_kernel = tf.reshape(emb_mlp_kernel,[-1, fc_emb.dimension/cur_output, cur_output])\n",
    "    emb_mlp_bias = fc.input_layer(features, fc_emb_mlp_bias[idx])\n",
    "    emb_mlp_bias = tf.expand_dims(emb_mlp_bias,1)\n",
    "    res = tf.einsum(\"bik,bkj->bij\", inp, emb_mlp_kernel) + emb_mlp_bias\n",
    "    res = tf.nn.relu(res)\n",
    "    inp = res\n",
    "\n",
    "res\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Estimator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## check model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-24T11:57:02.377916Z",
     "start_time": "2020-11-24T11:57:02.368744Z"
    },
    "run_control": {
     "marked": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/tmp-data/zhoutongzt/DD/Data/BR_addHeat_PE_timIdxByFreqIdx_WDModel_drop_feat'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ckpt_dir = os.path.join(DATA_PATH,\"BR_addHeat_PE_timIdxByFreqIdx_WDModel_drop_feat\")\n",
    "\n",
    "\n",
    "train_spec = dnn_linear_combined_model_fn(\n",
    "                    features=tr_data[0],\n",
    "                    labels=tr_data[1],\n",
    "                    fc_wide=fc_wide,\n",
    "                    fc_numeric=fc_numeric,\n",
    "                    n_classes=hparams['n_classes'],\n",
    "                    hparams=hparams,\n",
    "                    mode=tf.estimator.ModeKeys.TRAIN,\n",
    "                    fc_emb_dict=fc_emb_dict,\n",
    "                    emb_to_add=emb_to_add, emb_to_innerProduct=emb_to_innerProduct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-27T11:32:00.011686Z",
     "start_time": "2020-11-27T11:32:00.001416Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['sigmoid_cross_entropy_with_logits',\n",
       " 'softmax_cross_entropy_with_logits',\n",
       " 'softmax_cross_entropy_with_logits_v2',\n",
       " 'sparse_softmax_cross_entropy_with_logits',\n",
       " 'weighted_cross_entropy_with_logits']"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[i for i in dir(tf.nn) if \"entropy\" in i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-27T12:07:13.292832Z",
     "start_time": "2020-11-27T12:07:12.854315Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'logits'"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "array([[0.64769123, 0.99691358, 0.51880326, 0.65811273],\n",
       "       [0.59906347, 0.75306733, 0.13624713, 0.00411712],\n",
       "       [0.14950888, 0.698439  , 0.59335256, 0.89991535],\n",
       "       [0.44445739, 0.316785  , 0.92308176, 0.46586186]])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "'labels'"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "[1, 1, 2, 3]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "'softmax(logits)'"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "array([[0.23215253, 0.32918403, 0.20407889, 0.23458456],\n",
       "       [0.29872185, 0.34845766, 0.18804786, 0.16477263],\n",
       "       [0.15605622, 0.27019568, 0.24324277, 0.33050533],\n",
       "       [0.22145306, 0.19491006, 0.35739264, 0.22624424]])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "'target-one of softmax(logits)'"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "[0.32918402769283184,\n",
       " 0.34845765974154597,\n",
       " 0.24324277039282377,\n",
       " 0.22624423686097198]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "[-1.111138329907829,\n",
       " -1.05423854901307,\n",
       " -1.413695279250508,\n",
       " -1.4861401691642224]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "1.2663030624389648"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(1024)\n",
    "logits = np.random.random([4,4])\n",
    "labels=[1,1,2,3]\n",
    "\"logits\"\n",
    "logits\n",
    "\"labels\"\n",
    "labels\n",
    "with tf.Session() as sess:\n",
    "    \"softmax(logits)\"\n",
    "    sess.run(tf.nn.softmax(logits))\n",
    "    \"target-one of softmax(logits)\"\n",
    "    to = [sess.run(tf.nn.softmax(logits))[idx,i] for idx,i in enumerate(labels)]\n",
    "    to\n",
    "    [np.log(i) for i in to]\n",
    "    sess.run(tf.losses.sparse_softmax_cross_entropy(logits=logits,labels=labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python2.7(tf1.12)",
   "language": "python",
   "name": "tf1.12"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "310px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
