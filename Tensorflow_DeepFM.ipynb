{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensorflow feature_column API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-31T13:02:13.436264Z",
     "start_time": "2018-10-31T13:02:09.075540Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from sklearn.metrics import roc_auc_score,log_loss\n",
    "from DeepFM_use_generator_gpu_fieldMerge import DeepFM as DeepFM_fieldMerge\n",
    "from DeepFM_use_generator_gpu import DeepFM\n",
    "import time\n",
    "import sys\n",
    "import itertools\n",
    "from progressbar import ProgressBar\n",
    "import numpy as np\n",
    "from functools import wraps\n",
    "import os\n",
    "import shutil\n",
    "from collections import deque\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "from functools import reduce\n",
    "from tensorflow.contrib.layers.python.layers import batch_norm as batch_norm\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "# 禁用GPU\n",
    "# os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\" "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 获取feature_map、info.json 以及一个简易的generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-16T01:29:58.232918Z",
     "start_time": "2018-10-16T01:29:58.169049Z"
    },
    "hide_input": false
   },
   "outputs": [],
   "source": [
    "def gen(p):\n",
    "    with open(p,\"r+\") as f:\n",
    "        for l in f:\n",
    "            yield l.strip(\"[]\\n\")\n",
    "        \n",
    "data_g = gen(\"/home/zhoutong/data/apus_ad/data_2018-09-21_to_2018-09-26/2018-09-21_to_2018-09-25_shuffled\")\n",
    "\n",
    "base_p = \"/home/zhoutong/data/apus_ad/data_2018-09-21_to_2018-09-26\"\n",
    "with open(base_p + \"/feature_map_2018-09-21_to_2018-09-25\",\"r+\") as f: \n",
    "    feature_map = {x.strip().split(\"\\t\")[1] : x.strip().split(\"\\t\")[0] for x in f.readlines()}\n",
    "with open(base_p + \"/info.json\", \"r+\") as f:\n",
    "    info = eval(f.readlines()[0].strip())\n",
    "all_fields = set(map(lambda x:x.split(\"=\")[0], feature_map.values()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TFRecord | 完整Demo（pure python)\n",
    "   - 构造TFRecord\n",
    "   - 持久化存储\n",
    "   - 读取解析"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     52
    ],
    "deletable": false,
    "editable": false,
    "hide_input": false
   },
   "outputs": [],
   "source": [
    "sample_label = int([i.split(\" \") for i in next(data_g).split(\"\\t\")][0][0])\n",
    "sample_features_raw = [i.split(\" \") for i in next(data_g).split(\"\\t\")][1:]\n",
    "###########\n",
    "# 事前准备： 把libsvm格式换成 [feature_name, 1.0]\n",
    "sample_features = []\n",
    "for features in sample_features_raw:\n",
    "    print(\"*\" * 10)\n",
    "    for pair in features:\n",
    "        k,v = pair.split(\":\")\n",
    "        sample_features.append([feature_map[k],float(v)])\n",
    "    print(features)\n",
    "\n",
    "numeric_fieldList = [\"ad_info__budget_unit\",\"apolo\",\"gama\"]\n",
    "multi_hot_fieldList = [\"user_profile_app__install_app_list\",\"user_profile_app__tag_unit\"]\n",
    "\n",
    "\n",
    "# 主要目的还是把分散的multi-hot特征 k=v1, k=v2, k=v3 合并成 k=[v1,v2,v3]\n",
    "data_dict = {}\n",
    "for feature_name, value in sample_features:\n",
    "    if any([field in feature_name for field in numeric_fieldList]):\n",
    "        # numeric_feature\n",
    "        data_dict[feature_name] = value\n",
    "    elif any([field in feature_name for field in multi_hot_fieldList]):\n",
    "        # multi_hot_feature\n",
    "        field,value_ = feature_name.split(\"=\")\n",
    "        if field not in data_dict.keys():\n",
    "            data_dict[field] = [value_]\n",
    "        else:\n",
    "            data_dict[field] = data_dict[field] + [value_]\n",
    "    else:\n",
    "        field,value_ = feature_name.split(\"=\")\n",
    "        data_dict[field] = value_\n",
    "\n",
    "##############\n",
    "# 构造TFRecord\n",
    "feature_dict = {}\n",
    "for k,v in data_dict.items():\n",
    "    if type(v)==float:\n",
    "        feature_dict[k] = tf.train.Feature(float_list = tf.train.FloatList(value=[v]))\n",
    "    elif type(v)==list:\n",
    "        feature_dict[k] = tf.train.Feature(bytes_list = tf.train.BytesList(value=[i.encode('utf-8') for i in v]))\n",
    "    else:\n",
    "        feature_dict[k] = tf.train.Feature(bytes_list = tf.train.BytesList(value=[str(v).encode('utf-8')]))\n",
    "\n",
    "features = tf.train.Features(feature=feature_dict)\n",
    "example = tf.train.Example(features=features)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "##############\n",
    "# 持久化TFRecord\n",
    "with tf.python_io.TFRecordWriter('/home/zhoutong/sample1.tfrecord') as writer:\n",
    "    writer.write(example.SerializeToString())\n",
    "\n",
    "    \n",
    "    \n",
    "##############\n",
    "# 读取TFRecord\n",
    "reader = tf.TFRecordReader()\n",
    "filename_queue = tf.train.string_input_producer(['/home/zhoutong/sample1.tfrecord'])\n",
    "_, serialized_example = reader.read(filename_queue)\n",
    "\n",
    "read_feature = {}\n",
    "for field in all_fields:\n",
    "    if any([num_f in field for num_f in numeric_fieldList]):\n",
    "        read_feature[field] = tf.FixedLenFeature([], dtype=tf.float32)\n",
    "    elif any([multi_f in field for multi_f in multi_hot_fieldList]):\n",
    "        read_feature[field] = tf.VarLenFeature(dtype=tf.string)\n",
    "    else:\n",
    "        read_feature[field] = tf.FixedLenFeature([], dtype=tf.string)\n",
    "\n",
    "tf.train.start_queue_runners(sess)\n",
    "read_data = tf.parse_single_example(serialized=serialized_example, features=read_feature)\n",
    "for name, tensor in read_data.items():\n",
    "    print(\"%s : %s\" % (name, tensor.eval()))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TFReocrd | Spark+TensorFlow\n",
    "- Spark 直接生成 TFRecord 形式的文件\n",
    "- TF三种方式解析TFRecord\n",
    "    - DataSet\n",
    "    - TFRecordReader  (<font style=\"color:rgb(199,21,133)\">deprecated</font>)\n",
    "    - tf_record_iterator\n",
    "\n",
    "注意如果Spark没有处理null值问题，这里使用的时候，需要给 ```tf.FixedLenFeature```(或```tf.VarLenFeature```) 设置默认值参数 ```default_value=```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-15T05:53:57.653030Z",
     "start_time": "2018-10-15T05:53:57.649212Z"
    },
    "deletable": false,
    "editable": false,
    "hide_input": true
   },
   "outputs": [],
   "source": [
    "# 获取 tfrecords 文件\n",
    "# tfrecordFiles=[\"/home/zhoutong/data/part-total\"]\n",
    "tfrecordFiles=[\"/home/zhoutong/data/part-r-00000\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ### TFRecord | DataSet解析"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-15T05:56:32.270260Z",
     "start_time": "2018-10-15T05:56:32.102838Z"
    },
    "deletable": false,
    "editable": false,
    "hide_input": true
   },
   "outputs": [],
   "source": [
    "##################\n",
    "# success\n",
    "# 方案一： 使用 TF DataSet\n",
    "def _parse_function(example_proto):\n",
    "    all_fields=[\"cpu_cores\", \"client_id\", \"manufacturer\", \"network_type\", \"city\", \"memory_total\", \"country\", \"model\", \"sdk_level\", \"storage_total\", \"is_limit_ad_tracking\", \"height\", \"locale\", \"os_version\", \"width\", \"is_root\", \"dpi\"]\n",
    "    feature_structure={i:tf.FixedLenFeature([], dtype=tf.string, default_value=\"N/A\") for i in all_fields}\n",
    "    parsed_features = tf.parse_single_example(example_proto, feature_structure)\n",
    "    return list(parsed_features.items())\n",
    "dataset = tf.data.TFRecordDataset(tfrecordFiles)\n",
    "dataset = dataset.map(_parse_function)\n",
    "\n",
    "# iterator = dataset.make_one_shot_iterator()\n",
    "iterator = dataset.make_initializable_iterator()\n",
    "next_element = iterator.get_next()\n",
    "#############\n",
    "# sess 运行\n",
    "with tf.Session() as sess:\n",
    "    # 除了one_shot_iterator其他类型的iterator需要初始化\n",
    "    sess.run(iterator.initializer)\n",
    "    for _ in range(3):\n",
    "        print(type(sess.run(next_element)),\":\")\n",
    "        print(sess.run(next_element),\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ### TFRecord | TFRecordReader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-10-15T05:56:25.609Z"
    },
    "deletable": false,
    "editable": false,
    "hide_input": true,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "########################\n",
    "# fail\n",
    "# 方案二： 使用以前的TFRecordReader\n",
    "filename_queue = tf.train.string_input_producer(tfrecordFiles)\n",
    "reader = tf.TFRecordReader()\n",
    "_, serialized_example = reader.read(filename_queue)\n",
    "batch = tf.train.batch([serialized_example],batch_size=5)\n",
    "# 给出特征结构（这里全都是定长特征）\n",
    "all_fields=[\"cpu_cores\", \"client_id\", \"manufacturer\", \"network_type\", \"city\", \"memory_total\", \"country\", \"model\", \"sdk_level\", \"storage_total\", \"is_limit_ad_tracking\", \"height\", \"locale\", \"os_version\", \"width\", \"is_root\", \"dpi\"]\n",
    "read_feature={i:tf.FixedLenFeature([], dtype=tf.string, default_value=\"N/A\") for i in all_fields}\n",
    "# 解析\n",
    "# read_data = tf.parse_single_example(serialized=batch, features=read_feature)\n",
    "read_data = tf.parse_example(serialized=batch, features=read_feature)\n",
    "len(read_data.items())\n",
    "\n",
    "#############\n",
    "# sess 运行\n",
    "# 未知错误，在sess.run(tensor)时会卡主\n",
    "# with tf.Session() as sess:\n",
    "#     for name, tensor in read_data.items():\n",
    "#         print(\"%s : %s\" % (name, tensor))\n",
    "#         print(\"%s : %s\" % (name, sess.run(tensor)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-23T02:03:03.308690Z",
     "start_time": "2018-10-23T02:02:15.086Z"
    }
   },
   "source": [
    "- ### TFRecord | tf_record_iterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": true
   },
   "outputs": [],
   "source": [
    "########################\n",
    "# success\n",
    "# 方案三： 使用 tf_record_iterator\n",
    "example = tf.train.Example()\n",
    "all_fields=[\"cpu_cores\", \"client_id\", \"manufacturer\", \"network_type\", \"city\", \"memory_total\", \"country\", \"model\", \"sdk_level\", \"storage_total\", \"is_limit_ad_tracking\", \"height\", \"locale\", \"os_version\", \"width\", \"is_root\", \"dpi\"]\n",
    "\n",
    "record_iter = tf.python_io.tf_record_iterator(tfrecordFiles[0])\n",
    "for record in itertools.islice(record_iter,0,5):\n",
    "    example.ParseFromString(record)\n",
    "    f = example.features.feature\n",
    "    feature_list = []\n",
    "    for field in all_fields:\n",
    "        feature_list.append([field, f[field].bytes_list.value])\n",
    "    print(feature_list)\n",
    "\n",
    "########################\n",
    "# 涉及到压缩格式的TFRecord\n",
    "example = tf.train.Example()\n",
    "tfreocrd_option = tf.python_io.TFRecordOptions(tf.python_io.TFRecordCompressionType.GZIP)\n",
    "record_iter = tf.python_io.tf_record_iterator(tfrecord_file,tfreocrd_option)\n",
    "for record in itertools.islice(record_iter,0,2):\n",
    "    example.ParseFromString(record)\n",
    "    f = example.features.feature\n",
    "    feature_list = []\n",
    "    for field in global_all_fields:\n",
    "        feature_list.append([field, f[field].int64_list.value])\n",
    "    print(feature_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 使用tf.feature_column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.feature_column.categorical_column_with_identity\n",
    "\n",
    "tf.feature_column.categorical_column_with_vocabulary_list(key='ad_info__app_package_name',\n",
    "                                                         vocabulary_list=[])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## apus_ad | 数据准备（通用）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-17T12:29:50.386141Z",
     "start_time": "2018-10-17T12:29:50.367873Z"
    },
    "code_folding": [],
    "deletable": false,
    "editable": false,
    "hide_input": true,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "###########\n",
    "# 迭代生成\n",
    "\n",
    "class DataGenerator(object):\n",
    "    def __init__(self, train_path, valid_path, max_numeric):\n",
    "        self.train_path = train_path\n",
    "        self.valid_path = valid_path\n",
    "        self.max_numeric = max_numeric\n",
    "    @staticmethod\n",
    "    def print_t(param):\n",
    "        sys.stdout.flush()\n",
    "        now = time.strftime(\"|%Y-%m-%d %H:%M:%S| \", time.localtime(time.time()))\n",
    "        new_params = now + \": \" + param\n",
    "        print(new_params)\n",
    "        sys.stdout.flush()\n",
    "    def get_apus_ad_train_generator(self):\n",
    "        return self._yield_apus_ad_generator(self.train_path,self.max_numeric)\n",
    "    def get_apus_ad_valid(self):\n",
    "        return self._yield_apus_ad_generator(self.valid_path,self.max_numeric)\n",
    "    @staticmethod\n",
    "    def _yield_apus_ad_generator(reader_path,max_numeric):\n",
    "        with open(reader_path, \"r\") as f:\n",
    "            for line in f:\n",
    "                info = line.strip(\"[]\\n\").split(\"\\t\")\n",
    "                label = int(info[0])\n",
    "                numeric_f = info[1].split(\" \")\n",
    "                category_f = info[2].split(\" \")\n",
    "                multi_hot_f = [x.split(\" \")for x in info[3:]]\n",
    "\n",
    "                y = [label]\n",
    "                def get_idx_and_value(feature_info):\n",
    "                    index = [int(x.split(\":\")[0]) for x in feature_info]\n",
    "                    value = [float(x.split(\":\")[1]) for x in feature_info]\n",
    "                    return index,value\n",
    "                Xi_numeric, Xv_numeric = get_idx_and_value(numeric_f)\n",
    "                Xi_category, Xv_category = get_idx_and_value(category_f)\n",
    "                multi_hot_f_tmp = [get_idx_and_value(feature) for feature in multi_hot_f]\n",
    "                Xi_multi_hot_list = [feature_idx for feature_idx,value in multi_hot_f_tmp]\n",
    "                Xv_multi_hot_list = [value for feature_idx,value in multi_hot_f_tmp]\n",
    "                # Xv_numeric 归一化\n",
    "                if len(max_numeric)>0:\n",
    "                    # 根据索引是0还是1还是2从 max_nuemeric里面取值\n",
    "                    Xv_numeric = []\n",
    "                    for i in numeric_f:\n",
    "                        idx = int(i.split(\":\")[0])\n",
    "                        v = float(i.split(\":\")[1])\n",
    "                        Xv_numeric.append(v/max_numeric[idx])\n",
    "                    Xv_numeric = list(map(lambda x: x if x<=1 else 1, Xv_numeric))\n",
    "                yield [y, Xi_numeric, Xv_numeric, Xi_category, Xv_category, Xi_multi_hot_list, Xv_multi_hot_list]\n",
    "\n",
    "\n",
    "##########\n",
    "# 数据准备：两个多值离散特征\n",
    "train_double_multi = \"/home/zhoutong/data/apus_ad/data_2018-09-03_to_2018-09-09/data_2018-09-03_to_2018-09-09_train_shuffled\"\n",
    "valid_double_multi = \"/home/zhoutong/data/apus_ad/data_2018-09-03_to_2018-09-09/data_2018-09-03_to_2018-09-09_valid_shuffled\"\n",
    "data_g_double_multi = DataGenerator(train_path=train_double_multi, valid_path=valid_double_multi, max_numeric=[999, 10001, 13])\n",
    "train_generator_d = data_g_double_multi.get_apus_ad_train_generator()\n",
    "\n",
    "##########\n",
    "# 数据准备：LR使用的特征（一个多值离散）\n",
    "train_single_multi = \"/home/zhoutong/data/apus_ad/data_2018-09-21_to_2018-09-26/2018-09-21_to_2018-09-25_shuffled\"\n",
    "valid_single_multi = \"/home/zhoutong/data/apus_ad/data_2018-09-21_to_2018-09-26/2018-09-26_to_2018-09-26_shuffled\"\n",
    "data_g_single_multi = DataGenerator(train_path=train_single_multi, valid_path=valid_single_multi, max_numeric=[2])\n",
    "train_generator_s = data_g_single_multi.get_apus_ad_train_generator()\n",
    "\n",
    "batch_info_d = list(itertools.islice(train_generator_d,0, 10))\n",
    "batch_info_s = list(itertools.islice(train_generator_s,0, 10))\n",
    "\n",
    "##########\n",
    "# 解析函数\n",
    "def get_sparse_idx(input_df):\n",
    "            result = []\n",
    "            for i in range(len(input_df)):\n",
    "                for j in input_df[i]:\n",
    "                    result.append([i, j])\n",
    "            return np.array(result)\n",
    "def get_sparse_tensor_from(input_df, inp_tensor_shape):\n",
    "    \"\"\"\n",
    "    这里构造的稀疏向量,其indices是自增数,没有实际用途\n",
    "    其values是input_df内部的值\n",
    "        - 例如使用 Xi_multi_hot,则生成的稀疏向量values为特征在featureMap中的索引\n",
    "        - 使用 Xv_multi_hot,则生成的稀疏向量values为特征的\"权重\",基本都为 1.0\n",
    "    :param input_df:\n",
    "    :param inp_tensor_shape:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    tensor_values_ = []\n",
    "    tensor_indices_ = []\n",
    "    for idx in range(len(input_df)):\n",
    "        tensor_values_.extend(input_df[idx])\n",
    "        tensor_indices_.extend([[idx,v] for v in range(len(input_df[idx]))])\n",
    "    sp_tensor = tf.SparseTensorValue(indices=tensor_indices_, values=tensor_values_, dense_shape=inp_tensor_shape)\n",
    "    return sp_tensor\n",
    "def get_payload_gpu(gpu_cnt, payload,*allData_inp):\n",
    "    result = []\n",
    "    start = gpu_cnt*payload\n",
    "    end = (gpu_cnt+1)*payload\n",
    "    start\n",
    "    end\n",
    "    for data in allData_inp:\n",
    "        result.append(data[start:end])\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## apus_ad | 使用DataSet输入数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-17T12:32:17.172896Z",
     "start_time": "2018-10-17T12:32:17.115322Z"
    },
    "code_folding": [
     15
    ],
    "hide_input": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "batch_info_zipped = [np.array(x) for x in zip(*batch_info_s)]\n",
    "\n",
    "y_inp = batch_info_zipped[0]\n",
    "payload_per_gpu=len(y_inp)\n",
    "\n",
    "i=0\n",
    "# allData=[y, Xi_numeric, Xv_numeric, Xi_category, Xv_category, Xi_multi_hot_list, Xv_multi_hot_list]\n",
    "allData = get_payload_gpu(i, payload_per_gpu, *batch_info_zipped)\n",
    "[y, Xi_numeric, Xv_numeric, Xi_category, Xv_category, Xi_multi_hot_list, Xv_multi_hot_list] = allData\n",
    "\n",
    "tensor_dense_shape=[len(y_inp),18840]\n",
    "# 整体特征汇总\n",
    "Xi_total = []\n",
    "Xv_total = []\n",
    "for row_idx in range(len(Xi_numeric)):\n",
    "    Xi_total.append(list(Xi_numeric[row_idx])+list(Xi_category[row_idx])+reduce(lambda a,b: a+b, list(Xi_multi_hot_list[row_idx])))\n",
    "    Xv_total.append(list(Xv_numeric[row_idx])+list(Xv_category[row_idx])+reduce(lambda a,b: a+b, list(Xv_multi_hot_list[row_idx])))\n",
    "total_idx_sp = get_sparse_tensor_from(Xi_total,tensor_dense_shape)\n",
    "total_value_sp = get_sparse_tensor_from(Xv_total,tensor_dense_shape)\n",
    "\n",
    "print(\"feat_idx:\\n\")\n",
    "total_idx_sp.indices[:5]\n",
    "total_idx_sp.values[:5]\n",
    "print(\"feat_value:\\n\")\n",
    "total_value_sp.indices[:5]\n",
    "total_value_sp.values[:5]\n",
    "\n",
    "# 连续特征\n",
    "v_numeric_sparse = np.reshape(Xv_numeric, -1)\n",
    "v_category_sparse = np.reshape(Xv_category, -1)\n",
    "# 离散特征\n",
    "idx_numeric_sparse = get_sparse_idx(Xi_numeric)\n",
    "idx_category_sparse = get_sparse_idx(Xi_category)\n",
    "\n",
    "print(\"category_sparse:\")\n",
    "idx_category_sparse\n",
    "v_category_sparse\n",
    "\n",
    "\n",
    "# 多值离散特征 第0个 作示例\n",
    "Xi_multi_hot = [idx_list[0] for idx_list in Xi_multi_hot_list]\n",
    "# Xi_multi_hot_tensor = get_sparse_tensor_from(Xi_multi_hot, inp_tensor_shape=tensor_dense_shape)\n",
    "\n",
    "Xv_multi_hot = [value_list[0] for value_list in Xv_multi_hot_list]\n",
    "Xv_multi_hot_tensor = get_sparse_tensor_from(Xv_multi_hot, inp_tensor_shape=tensor_dense_shape)\n",
    "# Xv_multi_hot_tensor\n",
    "\n",
    "# Xi_numeric,Xv_numeric = allData[1:3]\n",
    "\n",
    "# idx_numeric_sparse = get_sparse_idx(Xi_numeric)\n",
    "# v_numeric_sparse = np.reshape(Xv_numeric,-1)\n",
    "# tensor_dense_shape = [1024, 18850]\n",
    "# input_sp_tensor = tf.SparseTensorValue(indices=idx_numeric_sparse, \n",
    "#                                        values=v_numeric_sparse, \n",
    "#                                        dense_shape=tensor_dense_shape)\n",
    "\n",
    "################\n",
    "# TensorFlow部分\n",
    "# sp_tensor = tf.sparse_placeholder(tf.float32)\n",
    "# dataset = tf.data.Dataset.from_tensor_slices(sp_tensor).batch(tensor_dense_shape[0])\n",
    "# # dataset = tf.data.Dataset.from_tensor_slices(sp_tensor).batch(10)\n",
    "# iterator = dataset.make_initializable_iterator()\n",
    "# next_el = iterator.get_next()\n",
    "# feed_dict = {sp_tensor: input_sp_tensor}\n",
    "# with tf.Session() as sess:\n",
    "#     sess.run(sp_tensor, feed_dict)\n",
    "#     sess.run(iterator.initializer, feed_dict)\n",
    "#     next_el\n",
    "\n",
    "# dataset_idx_of_numeric = tf.data.Dataset.from_tensors(idx_numeric_sparse)\n",
    "# dataset_value_of_numeric = tf.data.Dataset.from_tensors(tf.cast(v_numeric_sparse,'float'))\n",
    "# dataset_idx_of_numeric.output_types\n",
    "# dataset_idx_of_numeric.output_shapes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 使用TFRecord+DataSet作为输入\n",
    "- ## apus_ad | DeepFM | config\n",
    "- ## apus_ad | DeepFM | weights\n",
    "- ## apus_ad | DeepFM | model\n",
    "- ## apus_ad | DeepFM | input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## apus_ad | DeepFM | config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-01T08:39:17.076403Z",
     "start_time": "2018-11-01T08:39:17.045948Z"
    },
    "code_folding": [
     2,
     25,
     26,
     124
    ],
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "#***** 存跃的数据（midas） *****\n",
    "# deprecated\n",
    "class config_midas():\n",
    "    basePath = \"/home/zhoutong/data/apus_ad/midas/tfrecord_2018-09-21_to_2018-10-04_and_2018-10-05_to_2018-10-11_intersectLR\"\n",
    "#     basePath = \"/home/zhoutong/data/apus_ad/midas/tfrecord_2018-10-10_to_2018-10-17_and_2018-10-18_to_2018-10-21_intersectLR\"\n",
    "    train_tfrecord_file = basePath+\"/train.tfrecord.gz\"\n",
    "    valid_tfrecord_file = basePath+\"/valid.tfrecord.gz\"\n",
    "    info_file = basePath+\"/info.json\"\n",
    "    \n",
    "    with open(info_file,\"r+\") as f:\n",
    "        info = \"\".join(f.readlines())\n",
    "        result = json.loads(info)\n",
    "        \n",
    "    # fields\n",
    "    global_all_fields = [\n",
    "    \"label\", \"ad_info__ad_type\", \"ad_info__app_package_name\",\n",
    "    \"ad_info__budget_unit\", \"user_behavior__ad_position_id_s\",\n",
    "    \"user_behavior__app_version_s\", \"user_behavior__client_ip_s\",\n",
    "    \"user_behavior__country_s\", \"user_behavior__hour_s\",\n",
    "    \"user_behavior__language_s\", \"user_behavior__package_name_s\",\n",
    "    \"user_profile_app__install_app_list\"]\n",
    "    \n",
    "    global_numeric_fields = ['ad_info__budget_unit']\n",
    "    global_multi_hot_fields = ['user_profile_app__install_app_list']\n",
    "    global_one_hot_fields = []\n",
    "    for i in global_all_fields:\n",
    "        if i not in global_numeric_fields and i not in global_multi_hot_fields and i != \"label\":\n",
    "            global_one_hot_fields.append(i)\n",
    "    # feature_size\n",
    "    feature_size = 1451685+1\n",
    "    # field_size\n",
    "    multi_hot_field_size = len(global_multi_hot_fields)\n",
    "    numeric_field_size = len(global_numeric_fields)\n",
    "    one_hot_field_size = len(global_one_hot_fields)\n",
    "    \n",
    "    # 连续特征的索引号要单独给出来，方便后续构造idx_sparse_tensor\n",
    "    tmp_map_num_f = {'ad_info__budget_unit':1291744}\n",
    "    max_numeric = {\"ad_info__budget_unit\": 2.0}\n",
    "    # shape\n",
    "    global_dense_shape = [1024,feature_size]\n",
    "    \n",
    "    # deepfm_param\n",
    "    embedding_size = 8\n",
    "    dropout_fm = [1.0, 1.0]\n",
    "    deep_layers = [128, 64, 32, 16]\n",
    "    dropout_deep = [1.0, 0.9, 0.9, 0.9, 0.9]\n",
    "    deep_layers_activation = tf.nn.relu\n",
    "    epoch=10\n",
    "    batch_size= 1024*3\n",
    "    learning_rate= 0.001\n",
    "    optimizer_type=\"adam\"\n",
    "    batch_norm= 1\n",
    "    batch_norm_decay= 0.99\n",
    "    l2_reg=-0.01\n",
    "    verbose= True\n",
    "    eval_metric=roc_auc_score\n",
    "    random_seed=2017\n",
    "    gpu_num=1\n",
    "    is_debug=False\n",
    "    \n",
    "    summary_save_dir = \"/home/zhoutong/midas_summary\"\n",
    "    model_save_dir = \"/home/zhoutong/midas_model\"\n",
    "    \n",
    "    def get_now():\n",
    "        return time.strftime(\"%Y-%m-%d-%H-%M-%S\", time.localtime(time.time()))\n",
    "\n",
    "class config_midas_():\n",
    "    basePath = \"/home/zhoutong/data/apus_ad/midas/tfrecord_2018-09-21_to_2018-10-04_and_2018-10-05_to_2018-10-11_filterRepeatView\"\n",
    "#     basePath = \"/home/zhoutong/data/apus_ad/midas/tfrecord_2018-09-21_to_2018-10-04_and_2018-10-05_to_2018-10-11_intersectLR\"\n",
    "    train_tfrecord_file = basePath+\"/train.tfrecord.gz\"\n",
    "    valid_tfrecord_file = basePath+\"/valid.tfrecord.gz\"\n",
    "    info_file = basePath+\"/info.json\"\n",
    "    \n",
    "    # fields\n",
    "    with open(info_file,\"r+\") as f:\n",
    "        info = \"\".join(f.readlines())\n",
    "        result = json.loads(info)\n",
    "\n",
    "    fieldInfo = result['allField']\n",
    "    global_all_fields = fieldInfo['all_fields'].split(\",\")\n",
    "    global_numeric_fields = fieldInfo['numeric_fields'].split(\",\")\n",
    "    global_multi_hot_fields = fieldInfo['multi_hot_fields'].split(\",\")\n",
    "    global_one_hot_fields = []\n",
    "    for i in global_all_fields:\n",
    "        if i not in global_numeric_fields and i not in global_multi_hot_fields and i != \"label\":\n",
    "            global_one_hot_fields.append(i)\n",
    "    \n",
    "    statisticInfo = result['statistic']\n",
    "    # feature_size\n",
    "    feature_size = statisticInfo['feature_size']+1\n",
    "    # field_size\n",
    "    multi_hot_field_size = len(global_multi_hot_fields)\n",
    "    numeric_field_size = len(global_numeric_fields)\n",
    "    one_hot_field_size = len(global_one_hot_fields)\n",
    "    \n",
    "    # 连续特征的索引号要单独给出来，方便后续构造idx_sparse_tensor\n",
    "    tmp_map_num_f = result['numericFieldMap']#{'ad_info__budget_unit':1291744}\n",
    "    max_numeric = result['numericMax']#{\"ad_info__budget_unit\": 2.0}\n",
    "    \n",
    "    # deepfm_param\n",
    "    embedding_size = 1\n",
    "    dropout_fm = [1.0, 1.0]\n",
    "    deep_layers = [2,1]\n",
    "    dropout_deep = [1.0, 0.9, 0.9, 0.9, 0.8]\n",
    "    deep_layers_activation = tf.nn.relu\n",
    "    epoch=10\n",
    "    batch_size= 1024*3\n",
    "    learning_rate= 0.001\n",
    "    optimizer_type=\"adam\"\n",
    "    batch_norm= 1\n",
    "    batch_norm_decay= 0.9\n",
    "    l2_reg=-0.0001\n",
    "    verbose= True\n",
    "    eval_metric=roc_auc_score\n",
    "    random_seed=2017\n",
    "    gpu_num=1\n",
    "    is_debug=False\n",
    "    \n",
    "    # shape\n",
    "    global_dense_shape = [batch_size,feature_size]\n",
    "    \n",
    "    summary_save_dir = \"/home/zhoutong/midas_summary\"\n",
    "    model_save_dir = \"/home/zhoutong/midas_model\"\n",
    "    \n",
    "    def get_now():\n",
    "        return time.strftime(\"%Y-%m-%d-%H-%M-%S\", time.localtime(time.time()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-06T07:00:56.047755Z",
     "start_time": "2018-11-06T07:00:56.031956Z"
    },
    "code_folding": [],
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "#***** 国盛的数据（starksdk） *****\n",
    "class config_starksdk():\n",
    "    basePath = \"/home/zhoutong/data/apus_ad/starksdk/tfrecord_2018-09-21_to_2018-10-04_and_2018-10-05_to_2018-10-11\"\n",
    "    train_tfrecord_file = basePath+\"/train.tfrecord.gz\"\n",
    "    valid_tfrecord_file = basePath+\"/valid.tfrecord.gz\"\n",
    "    info_file = basePath+\"/info.json\"\n",
    "    \n",
    "    # fields\n",
    "    global_all_fields = [\"label\",\"user_profile_app__install_app_list\",\"ad_info__advertiser\",\"ad_info__advertiser_gp_class\",\n",
    "    \"user_profile_basic__dpi\",\"user_profile_basic__network_type\",\"user_profile_basic__locale\",\n",
    "    \"user_profile_basic__manufacturer\",\"user_profile_basic__model\",\"user_profile_basic__country\"]\n",
    "    global_numeric_fields = []\n",
    "    global_multi_hot_fields = ['user_profile_app__install_app_list']\n",
    "    global_one_hot_fields = []\n",
    "    for i in global_all_fields:\n",
    "        if i not in global_numeric_fields and i not in global_multi_hot_fields and i != \"label\":\n",
    "            global_one_hot_fields.append(i)\n",
    "    feature_size = 1087368+1\n",
    "    # field_size ( 如果没有该类型的特征，后续会默认填充一个为0的tensor，这里需要把size置为1)\n",
    "    multi_hot_field_size = 1 if len(global_multi_hot_fields)==0 else len(global_multi_hot_fields)\n",
    "    numeric_field_size = 1 if len(global_numeric_fields)==0 else len(global_numeric_fields)\n",
    "    one_hot_field_size = 1 if len(global_one_hot_fields)==0 else len(global_one_hot_fields)\n",
    "    \n",
    "    # 连续特征的索引号要单独给出来，方便后续构造idx_sparse_tensor\n",
    "    tmp_map_num_f ={}# {'ad_info__budget_unit':0}\n",
    "    max_numeric = {}#{\"ad_info__budget_unit\": 2.0}\n",
    "    # shape\n",
    "    global_dense_shape = [1024,feature_size]\n",
    "    \n",
    "    # deepfm_param\n",
    "    embedding_size = 8\n",
    "    dropout_fm = [1.0, 1.0]\n",
    "    deep_layers = [128, 64, 32, 16]\n",
    "    dropout_deep = [1.0, 0.9, 0.9, 0.9, 0.9]\n",
    "    deep_layers_activation = tf.nn.relu\n",
    "    epoch=10\n",
    "    batch_size= 1024*6\n",
    "    learning_rate= 0.001\n",
    "    optimizer_type=\"adam\"\n",
    "    batch_norm= 1\n",
    "    batch_norm_decay= 0.99\n",
    "    l2_reg=-0.00001\n",
    "    verbose= True\n",
    "    eval_metric=roc_auc_score\n",
    "    random_seed=2017\n",
    "    gpu_num=1\n",
    "    is_debug=False\n",
    "    \n",
    "    summary_save_dir = \"/home/zhoutong/starksdk_summary\"\n",
    "    model_save_dir = \"/home/zhoutong/starksdk_model\"\n",
    "    \n",
    "    def get_now():\n",
    "        return time.strftime(\"%Y-%m-%d-%H-%M-%S\", time.localtime(time.time()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-06T07:01:06.676690Z",
     "start_time": "2018-11-06T07:01:06.662936Z"
    },
    "code_folding": [
     0,
     11,
     14
    ],
    "deletable": false,
    "editable": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "json's info:\n",
      "statistic --\n",
      "     train_pos = 514076\n",
      "     valid_neg = 110887\n",
      "     train_neg = 2051521\n",
      "     multi_hot_f_size = 1\n",
      "     field_size = 10\n",
      "     valid_pos = 193486\n",
      "     feature_size = 1087368\n",
      "     numeric_f_size = 0\n",
      "numericFieldMap --\n",
      "allField --\n",
      "     all_fields = {'empty': False, 'traversableAgain': True}\n",
      "     numeric_fields = {'empty': True, 'traversableAgain': True}\n",
      "     multi_hot_fields = {'empty': False, 'traversableAgain': True}\n",
      "featureCnt --\n",
      "     user_profile_basic__manufacturer = 3199\n",
      "     ad_info__advertiser = 72980\n",
      "     ad_info__advertiser_gp_class = 35\n",
      "     user_profile_basic__network_type = 9\n",
      "     user_profile_basic__model = 17118\n",
      "     user_profile_basic__locale = 514\n",
      "     user_profile_basic__country = 234\n",
      "     user_profile_basic__dpi = 150\n",
      "     user_profile_app__install_app_list = 993130\n",
      "batch_cnt: 417.57763671875\n"
     ]
    }
   ],
   "source": [
    "def get_dict(instance):\n",
    "    dict_ = instance.__dict__\n",
    "    keys = [i for i in dict_.keys() if \"__\" not in i and i != \"get_dict\" ]   \n",
    "    return {key:dict_[key] for key in keys}\n",
    "\n",
    "config = config_starksdk\n",
    "# config = config_midas_\n",
    "# config = config_midas_noLR\n",
    "\n",
    "print(\"json's info:\")\n",
    "import json\n",
    "with open(config.info_file,\"r+\") as f:\n",
    "    info = f.read()\n",
    "    result = json.loads(info)\n",
    "for key,value in result.items():\n",
    "    print(key,\"--\")\n",
    "    for key_,value_ in value.items():\n",
    "        print(\"    \",key_,\"=\",value_)\n",
    "print(\"batch_cnt:\",(result['statistic']['train_pos']+result['statistic']['train_neg'])/config.batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-18T03:26:20.807199Z",
     "start_time": "2018-10-18T03:26:20.785301Z"
    }
   },
   "source": [
    "## apus_ad | DeepFM | weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-01T08:39:18.313249Z",
     "start_time": "2018-11-01T08:39:18.294681Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def _initialize_weights():\n",
    "        multi_hot_field_size = config.multi_hot_field_size\n",
    "        one_hot_field_size = config.one_hot_field_size\n",
    "        numeric_field_size = config.numeric_field_size\n",
    "        feature_size = config.feature_size\n",
    "        embedding_size = config.embedding_size\n",
    "        deep_layers = config.deep_layers\n",
    "        \n",
    "        weights = dict()\n",
    "        # embeddings\n",
    "        weights[\"feature_embeddings\"] = tf.Variable(\n",
    "            tf.random_normal([feature_size, embedding_size], -0.01, 0.01),\n",
    "            name=\"feature_embeddings\")  # feature_size * K\n",
    "        # FM first-order weights\n",
    "        weights[\"feature_bias\"] = tf.Variable(\n",
    "            tf.random_uniform([feature_size, 1], -0.01, 0.01), name=\"feature_bias\")  # feature_size * 1\n",
    "        # deep layers\n",
    "        # 总输入元个数为 : (涉及emb的特征个数) * embedding_size + 连续特征个数\n",
    "        input_size_emb = (multi_hot_field_size+one_hot_field_size) * embedding_size + numeric_field_size\n",
    "        glorot = np.sqrt(2.0 / (input_size_emb + deep_layers[0]))\n",
    "        weights[\"layer_0\"] = tf.Variable(\n",
    "            initial_value=np.random.normal(loc=0, scale=glorot, size=(input_size_emb, deep_layers[0])), \n",
    "            dtype=np.float32,\n",
    "            name=\"w_layer_0\")\n",
    "        weights[\"bias_0\"] = tf.Variable(np.random.normal(loc=0, scale=glorot, size=(1, deep_layers[0])),\n",
    "                                        dtype=np.float32, name=\"b_layer_0\")  # 1 * layers[0]\n",
    "        for i in range(1, len(deep_layers)):\n",
    "            glorot = np.sqrt(2.0 / (deep_layers[i - 1] + deep_layers[i]))\n",
    "            weights[\"layer_%d\" % i] = tf.Variable(\n",
    "                np.random.normal(loc=0, scale=glorot, size=(deep_layers[i - 1], deep_layers[i])),\n",
    "                dtype=np.float32, name=\"w_layer_%d\" % i)  # layers[i-1] * layers[i]\n",
    "            weights[\"bias_%d\" % i] = tf.Variable(\n",
    "                np.random.normal(loc=0, scale=glorot, size=(1, deep_layers[i])),\n",
    "                dtype=np.float32, name=\"b_layer_%d\" % i)  # 1 * layer[i]\n",
    "        # final concat projection layer\n",
    "        ################\n",
    "        # fm的y_first_order已经被提前求和了，所以只需要给它一个权重\n",
    "        # （因为在weights[\"feature_bias\"]中已经有部分作为“权重”乘上了y_first_order的特征值，然后求和，相当于每个一阶特征都有自己的隐向量x权重(来自w[\"feature_bias\"])\n",
    "        ################\n",
    "        cocnat_input_size_emb = 1 + embedding_size + deep_layers[-1]\n",
    "        glorot = np.sqrt(2.0 / (cocnat_input_size_emb + 1))\n",
    "        weights[\"concat_projection\"] = tf.Variable(\n",
    "            np.random.normal(loc=0, scale=glorot, size=(cocnat_input_size_emb, 1)),\n",
    "            dtype=np.float32, name=\"concat_projection\")  # layers[i-1]*layers[i]\n",
    "        weights[\"concat_bias\"] = tf.Variable(tf.constant(0.01), dtype=np.float32, name=\"concat_bias\")\n",
    "        return weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## apus_ad | DeepFM | model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-01T08:39:19.012480Z",
     "start_time": "2018-11-01T08:39:18.973468Z"
    },
    "code_folding": [
     1,
     15,
     18
    ]
   },
   "outputs": [],
   "source": [
    "# params for deepfm\n",
    "def run_deepfm(self,weights,inp_list,train_phase,dropout_keep_fm,dropout_keep_deep):\n",
    "    with tf.name_scope(\"gen_feat_total\"):\n",
    "        feat_total_idx_sp,feat_total_value_sp = _get_total_feature(inp_list)\n",
    "    with tf.name_scope(\"gen_feat_multi_hot\"):\n",
    "        feat_multi_hot_idx_sp_list = _get_multi_hot_idx_list(inp_list)\n",
    "        feat_multi_hot_value_sp_list = _make_multi_hot_value_list(feat_multi_hot_idx_sp_list)\n",
    "    with tf.name_scope(\"gen_feat_numeric\"):\n",
    "        feat_numeric_sp = _get_numeric_sp(inp_list,config.batch_size)\n",
    "    with tf.name_scope(\"gen_feat_category\"):\n",
    "        feat_category_sp = _get_category_sp(inp_list)\n",
    "    return _deep_fm_graph(self,weights,dropout_keep_fm,dropout_keep_deep,feat_total_idx_sp, feat_total_value_sp,\n",
    "                      feat_multi_hot_idx_sp_list, feat_multi_hot_value_sp_list,\n",
    "                      feat_numeric_sp, feat_category_sp, train_phase)\n",
    "\n",
    "def _deep_fm_graph(self,weights, dropout_keep_fm,dropout_keep_deep,feat_total_idx_sp, feat_total_value_sp,\n",
    "                      feat_multi_hot_idx_sp_list, feat_multi_hot_value_sp_list,\n",
    "                      feat_numeric_sp, feat_category_sp, train_phase):\n",
    "        def batch_norm_layer(x, inp_train_phase, scope_bn,inp_batch_norm_decay):\n",
    "            bn_train = batch_norm(x, decay=inp_batch_norm_decay, center=True, scale=True, updates_collections=None,\n",
    "                                  is_training=True, reuse=None, trainable=True, scope=scope_bn)\n",
    "            bn_inference = batch_norm(x, decay=inp_batch_norm_decay, center=True, scale=True, updates_collections=None,\n",
    "                                      is_training=False, reuse=True, trainable=True, scope=scope_bn)\n",
    "            z = tf.cond(inp_train_phase, lambda: bn_train, lambda: bn_inference)\n",
    "            return z\n",
    "        \n",
    "#         dropout_keep_fm = self.dropout_fm\n",
    "#         dropout_keep_deep = self.dropout_deep\n",
    "        numeric_feature_size = self.numeric_field_size\n",
    "        onehot_field_size = self.one_hot_field_size\n",
    "        multi_hot_field_size = self.multi_hot_field_size\n",
    "        embedding_size = self.embedding_size\n",
    "        deep_layers_activation = self.deep_layers_activation\n",
    "        batch_norm_decay = self.batch_norm_decay\n",
    "        \n",
    "        deep_input_size = multi_hot_field_size + onehot_field_size\n",
    "        \n",
    "        # ---------- FM component ---------\n",
    "        with tf.name_scope(\"FM\"):\n",
    "            # ---------- first order term ----------\n",
    "            with tf.name_scope(\"1st_order\"):\n",
    "                y_first_order = tf.nn.embedding_lookup_sparse(\n",
    "                    weights[\"feature_bias\"],\n",
    "                    sp_ids=feat_total_idx_sp,\n",
    "                    sp_weights=feat_total_value_sp,\n",
    "                    combiner=\"sum\")\n",
    "                y_first_order = tf.nn.dropout(\n",
    "                    y_first_order,\n",
    "                    dropout_keep_fm[0],\n",
    "                    name=\"y_first_order_dropout\")\n",
    "            # ---------- second order term ---------------\n",
    "            with tf.name_scope(\"2nd_order\"):\n",
    "                # sum_square part\n",
    "                summed_features_emb_square = tf.square(\n",
    "                    tf.nn.embedding_lookup_sparse(\n",
    "                        weights[\"feature_embeddings\"],\n",
    "                        sp_ids=feat_total_idx_sp,\n",
    "                        sp_weights=feat_total_value_sp,\n",
    "                        combiner=\"sum\"))\n",
    "                # square_sum part\n",
    "                squared_sum_features_emb = tf.nn.embedding_lookup_sparse(\n",
    "                    tf.square(weights[\"feature_embeddings\"]),\n",
    "                    sp_ids=feat_total_idx_sp,\n",
    "                    sp_weights=tf.square(feat_total_value_sp),\n",
    "                    combiner=\"sum\")\n",
    "                # second order\n",
    "                y_second_order = 0.5 * tf.subtract(\n",
    "                    summed_features_emb_square,\n",
    "                    squared_sum_features_emb)  # None * K\n",
    "                y_second_order = tf.nn.dropout(y_second_order,\n",
    "                                               dropout_keep_fm[1])  # None * K\n",
    "        # ---------- Deep component -------\n",
    "        with tf.name_scope(\"Deep\"):\n",
    "            # total_embedding 均值 用户的multi-hot one-hot特征都取到embedding作为DNN输入\n",
    "            with tf.name_scope(\"total_emb\"):\n",
    "                # feat_one_hot = tf.sparse_add(feat_numeric_sp, feat_category_sp)\n",
    "                feat_one_hot = feat_category_sp\n",
    "                one_hot_embeddings = tf.nn.embedding_lookup(\n",
    "                    weights[\"feature_embeddings\"], feat_one_hot.indices[:, 1])\n",
    "                one_hot_embeddings = tf.reshape(\n",
    "                    one_hot_embeddings,\n",
    "                    shape=(-1, onehot_field_size, embedding_size))\n",
    "                multi_hot_embeddings = []\n",
    "                for feat_idx_sp, feat_value_sp in zip(\n",
    "                        feat_multi_hot_idx_sp_list, feat_multi_hot_value_sp_list):\n",
    "                    emb = tf.nn.embedding_lookup_sparse(\n",
    "                        weights[\"feature_embeddings\"],\n",
    "                        sp_ids=feat_idx_sp,\n",
    "                        sp_weights=feat_value_sp,\n",
    "                        combiner=\"mean\")\n",
    "                    emb = tf.reshape(emb, shape=[-1, 1, embedding_size])\n",
    "                    multi_hot_embeddings.append(emb)\n",
    "                total_embeddings = tf.concat(\n",
    "                    values=[one_hot_embeddings] + multi_hot_embeddings, axis=1)\n",
    "            # input\n",
    "            with tf.name_scope(\"input\"):\n",
    "                # 把连续特征不经过embedding直接输入到NN\n",
    "                feat_numeric_sp_dense = tf.cast(\n",
    "                    tf.reshape(\n",
    "                        feat_numeric_sp.values, shape=(-1, numeric_feature_size)),\n",
    "                    tf.float32)\n",
    "                y_deep_input = tf.reshape(\n",
    "                    total_embeddings,\n",
    "                    shape=[-1, deep_input_size * embedding_size])  # None * (F*K)\n",
    "                y_deep_input = tf.concat([y_deep_input, feat_numeric_sp_dense],\n",
    "                                         axis=1)\n",
    "                y_deep_input = tf.nn.dropout(y_deep_input, dropout_keep_deep[0])\n",
    "            # layer0\n",
    "            with tf.name_scope(\"layer0\"):\n",
    "                y_deep_layer_0 = tf.add(\n",
    "                    tf.matmul(y_deep_input, weights[\"layer_0\"]), weights[\"bias_0\"])\n",
    "                y_deep_layer_0 = batch_norm_layer(\n",
    "                    y_deep_layer_0, inp_train_phase=train_phase, scope_bn=\"bn_0\",inp_batch_norm_decay=batch_norm_decay)\n",
    "                y_deep_layer_0 = deep_layers_activation(y_deep_layer_0)\n",
    "                y_deep_layer_0 = tf.nn.dropout(y_deep_layer_0, dropout_keep_deep[1])\n",
    "            # layer1\n",
    "            with tf.name_scope(\"layer1\"):\n",
    "                y_deep_layer_1 = tf.add(\n",
    "                    tf.matmul(y_deep_layer_0, weights[\"layer_1\"]),\n",
    "                    weights[\"bias_1\"])\n",
    "                y_deep_layer_1 = batch_norm_layer(\n",
    "                    y_deep_layer_1, inp_train_phase=train_phase, scope_bn=\"bn_1\",inp_batch_norm_decay=batch_norm_decay)\n",
    "                y_deep_layer_1 = deep_layers_activation(y_deep_layer_1)\n",
    "                y_deep_layer_1 = tf.nn.dropout(y_deep_layer_1, dropout_keep_deep[2])\n",
    "#             # layer2\n",
    "#             with tf.name_scope(\"layer2\"):\n",
    "#                 y_deep_layer_2 = tf.add(\n",
    "#                     tf.matmul(y_deep_layer_1, weights[\"layer_2\"]),\n",
    "#                     weights[\"bias_2\"])\n",
    "#                 y_deep_layer_2 = batch_norm_layer(\n",
    "#                     y_deep_layer_2, inp_train_phase=train_phase, scope_bn=\"bn_2\",inp_batch_norm_decay=batch_norm_decay)\n",
    "#                 y_deep_layer_2 = deep_layers_activation(y_deep_layer_2)\n",
    "#                 y_deep_layer_2 = tf.nn.dropout(y_deep_layer_2, dropout_keep_deep[3])\n",
    "            # layer3\n",
    "#             with tf.name_scope(\"layer3\"):\n",
    "#                 y_deep_layer_3 = tf.add(\n",
    "#                     tf.matmul(y_deep_layer_0, weights[\"layer_3\"]),\n",
    "#                     weights[\"bias_3\"])\n",
    "#                 y_deep_layer_3 = batch_norm_layer(\n",
    "#                     y_deep_layer_3, inp_train_phase=train_phase, scope_bn=\"bn_3\",inp_batch_norm_decay=batch_norm_decay)\n",
    "#                 y_deep_layer_3 = deep_layers_activation(y_deep_layer_3)\n",
    "#                 y_deep_layer_3 = tf.nn.dropout(y_deep_layer_3, dropout_keep_deep[4])\n",
    "        # ---------- DeepFM ---------------\n",
    "        with tf.name_scope(\"DeepFM\"):\n",
    "            y_deep_out = y_deep_layer_1\n",
    "#             y_deep_out = y_deep_layer_3\n",
    "            concat_input = tf.concat([y_first_order, y_second_order, y_deep_out], axis=1)\n",
    "#             concat_input = tf.concat([y_first_order, y_second_order], axis=1)\n",
    "            out = tf.add(\n",
    "                tf.matmul(concat_input, weights[\"concat_projection\"]),\n",
    "                weights[\"concat_bias\"])\n",
    "    \n",
    "        return tf.nn.sigmoid(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## apus_ad | DeepFM | input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-01T08:39:19.821450Z",
     "start_time": "2018-11-01T08:39:19.783632Z"
    },
    "code_folding": [
     1,
     16,
     29,
     34,
     56,
     71,
     88
    ],
    "deletable": false,
    "editable": false,
    "hide_input": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 解析 TF Example 文件\n",
    "def _decode(serialized_example):\n",
    "    feature_structure = {}\n",
    "    for field in config.global_all_fields:\n",
    "        if field == \"label\":\n",
    "            feature_structure[field]=tf.FixedLenFeature([], dtype=tf.int64)\n",
    "        elif field in config.global_multi_hot_fields:\n",
    "            feature_structure[field] = tf.VarLenFeature(dtype=tf.int64)\n",
    "        elif field in config.global_numeric_fields:\n",
    "            feature_structure[field] = tf.FixedLenFeature([],dtype=tf.float32)\n",
    "        else:\n",
    "            feature_structure[field]=tf.FixedLenFeature([], dtype=tf.int64)\n",
    "    parsed_features = tf.parse_single_example(serialized_example, feature_structure)\n",
    "    return parsed_features\n",
    "\n",
    "# 目前label是string要转成int64，后续会改\n",
    "def _convert_label_toInt64(parsed_features):\n",
    "    label = parsed_features['label']\n",
    "    parsed_features['label'] = tf.string_to_number(label,out_type=tf.int64)\n",
    "    return parsed_features\n",
    "\n",
    "# 连续特征归一化\n",
    "def _normalize(parsed_features):\n",
    "    for num_f in config.global_numeric_fields:\n",
    "        max_v = config.max_numeric[num_f] \n",
    "        parsed_features[num_f] = parsed_features[num_f] / max_v - 0.5\n",
    "    return parsed_features\n",
    "\n",
    "# 把连续特征的idx加进去，跟样本一起出现batch_size次\n",
    "def _add_idx_of_numeric(parsed_features):\n",
    "    for field in config.global_numeric_fields:\n",
    "        parsed_features[field+\"_idx\"] = tf.cast(config.tmp_map_num_f[field],tf.int64)\n",
    "    return parsed_features\n",
    "\n",
    "def __add_idx_to_tensor(inp_tensor):\n",
    "    idx = tf.range(tf.shape(inp_tensor)[0])\n",
    "    idx_2d = tf.reshape(idx,[-1,1])\n",
    "    idx_2d_full = tf.cast(tf.tile(idx_2d,[1,tf.shape(inp_tensor)[1]]),dtype=inp_tensor.dtype)\n",
    "    result = tf.concat([tf.reshape(idx_2d_full,[-1,1]),tf.reshape(inp_tensor,[-1,1])],axis=1)\n",
    "    return result\n",
    "\n",
    "def _get_numeric_sp(inp_dict,batch_size):\n",
    "    if len(config.global_numeric_fields) !=0:\n",
    "        idx_to_stack=[]\n",
    "        value_to_stack=[]\n",
    "        for field in config.global_numeric_fields:\n",
    "            idx_to_stack.append(inp_dict[field+\"_idx\"])\n",
    "            value_to_stack.append(inp_dict[field])\n",
    "        idx_dense = __add_idx_to_tensor(tf.transpose(tf.stack(idx_to_stack)))\n",
    "        value_dense = tf.reshape(tf.transpose(tf.stack(value_to_stack)),[-1])\n",
    "    else:\n",
    "        # 为了保持连贯性，没有连续特征会构造“一个”连续特征，全为0\n",
    "        idx_dense = tf.constant([[i,0] for i in range(batch_size)],dtype=tf.int64)\n",
    "        value_dense = tf.constant([0.0]*batch_size,dtype=tf.float32)\n",
    "    return tf.SparseTensor(indices=idx_dense, values=value_dense, dense_shape=[batch_size,config.numeric_field_size])\n",
    "\n",
    "def _get_category_sp(inp_dict):\n",
    "    if len(config.global_one_hot_fields) != 0:\n",
    "        idx_to_stack=[]\n",
    "        value_to_stack=[]\n",
    "        for field in config.global_one_hot_fields:\n",
    "            idx_to_stack.append(inp_dict[field])\n",
    "            value_to_stack.append(tf.ones_like(inp_dict[field],dtype=tf.float32))\n",
    "            idx_dense = __add_idx_to_tensor(tf.transpose(tf.stack(idx_to_stack)))\n",
    "            value_dense = tf.reshape(tf.transpose(tf.stack(value_to_stack)),[-1])\n",
    "    else:\n",
    "        idx_dense = tf.constant([[0,0]],dtype=tf.int64)\n",
    "        value_dense = tf.constant([0.0],dtype=tf.float32)\n",
    "    return tf.SparseTensor(indices=idx_dense, values=value_dense, dense_shape=config.global_dense_shape)\n",
    "\n",
    "# 这个multi_hot的默认值没有做完，和numeric之前一样的错误，没有构造batch个稀疏特征\n",
    "def _get_multi_hot_idx_list(inp_dict):\n",
    "    multi_hot_idx_list = []\n",
    "    if len(config.global_multi_hot_fields) != 0:\n",
    "        for field in config.global_multi_hot_fields:\n",
    "            multi_hot_idx_list.append(inp_dict[field])\n",
    "    else:\n",
    "        multi_hot_idx_list.append(tf.SparseTensor(indices=[[0,0]], values=[0.0], dense_shape=config.global_dense_shape))\n",
    "    return multi_hot_idx_list\n",
    "\n",
    "def _make_multi_hot_value_list(feat_idx_list):\n",
    "    multi_hot_value_list = []\n",
    "    if len(feat_idx_list) !=0:\n",
    "        multi_hot_value_list=[tf.SparseTensor(indices=sparse.indices,values=tf.ones_like(sparse.values,dtype=tf.float32),dense_shape=sparse.dense_shape) for sparse in feat_idx_list]\n",
    "    else:\n",
    "        multi_hot_value_list.append(tf.SparseTensor(indices=[[0,0]], values=[0.0], dense_shape=config.global_dense_shape))\n",
    "    return multi_hot_value_list\n",
    "\n",
    "def _get_total_feature(inp_dict):\n",
    "    idx_to_stack = []\n",
    "    value_to_stack = []\n",
    "    # sparse_tensor来表示multi_hot\n",
    "    multi_hot_idx_sparse_list = []\n",
    "    for field in config.global_all_fields:\n",
    "        if field in config.global_multi_hot_fields:\n",
    "            multi_hot_idx_sparse_list.append(inp_dict[field])\n",
    "        if field in config.global_numeric_fields:\n",
    "            idx_to_stack.append(inp_dict[field+\"_idx\"])\n",
    "            value_to_stack.append(inp_dict[field])\n",
    "            pass\n",
    "        if field in config.global_one_hot_fields:\n",
    "            idx_to_stack.append(inp_dict[field])\n",
    "            value_to_stack.append(tf.ones_like(inp_dict[field],dtype=tf.float32))\n",
    "            pass\n",
    "    # sparse_tensor的values中原来都是特征索引，替换成1.0\n",
    "    multi_hot_value_sparse_list = [tf.SparseTensor(indices=sparse.indices, values=tf.ones_like(sparse.values,dtype=tf.float32), dense_shape=sparse.dense_shape) for sparse in multi_hot_idx_sparse_list]\n",
    "    # idx sparse of numeric+onehot \n",
    "    idx_dense = tf.transpose(tf.stack(idx_to_stack))\n",
    "    idx_sparse = tf.contrib.layers.dense_to_sparse(tensor=idx_dense,eos_token=-1)\n",
    "    # value sparse of numeric+onehot \n",
    "    value_dense = tf.transpose(tf.stack(value_to_stack))\n",
    "    value_sparse = tf.contrib.layers.dense_to_sparse(tensor=value_dense,eos_token=-1)\n",
    "    \n",
    "    total_idx_sparse = tf.sparse_concat(axis=1,sp_inputs=[idx_sparse]+ multi_hot_idx_sparse_list)\n",
    "    total_value_sparse = tf.sparse_concat(axis=1,sp_inputs=[value_sparse] + multi_hot_value_sparse_list)\n",
    "    return total_idx_sparse, total_value_sparse\n",
    "    \n",
    "def get_input(tfrecord_path):\n",
    "    with tf.name_scope(\"dataset\"):\n",
    "        dataset = tf.data.TFRecordDataset(tfrecord_path,compression_type = compression_type)\n",
    "        dataset = (dataset.map(_decode)\n",
    "                   .map(_normalize)\n",
    "                   .map(_add_idx_of_numeric))\n",
    "        dataset = (dataset.shuffle(5*config.batch_size)\n",
    "                   .batch(config.batch_size,drop_remainder=True))\n",
    "        iterator = dataset.make_initializable_iterator()\n",
    "    return iterator\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## apus_ad | DeepFM | Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-01T08:39:27.809121Z",
     "start_time": "2018-11-01T08:39:20.847364Z"
    },
    "code_folding": [
     8,
     35,
     54,
     78
    ],
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total_parameters cnt : 3104325\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zhoutong/python3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:108: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'total_loss:0' shape=() dtype=string>"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'empirical_risk_logloss:0' shape=() dtype=string>"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 构造图\n",
    "# train_tfrecord_file = \"/home/zhoutong/data/apus_ad/tfrecord_2018-09-21_to_2018-10-04_and_2018-10-05_to_2018-10-11/train.tfrecord.gz\"\n",
    "# valid_tfrecord_file = \"/home/zhoutong/data/apus_ad/tfrecord_2018-09-21_to_2018-10-04_and_2018-10-05_to_2018-10-11/valid.tfrecord.gz\"\n",
    "\n",
    "compression_type = \"GZIP\"\n",
    "tf.set_random_seed(2018)\n",
    "graph = tf.Graph()\n",
    "\n",
    "with graph.as_default():\n",
    "    weights = _initialize_weights()\n",
    "    total_parameters = 0\n",
    "    for variable in weights.values():\n",
    "        shape = variable.get_shape()\n",
    "        variable_parameters = 1\n",
    "        for dim in shape:\n",
    "            variable_parameters *= dim.value\n",
    "        total_parameters += variable_parameters\n",
    "    print(\"total_parameters cnt : %s\" % total_parameters)\n",
    "        \n",
    "    inp_tfrecord_path = tf.placeholder(dtype=tf.string, name=\"tfrecord_path\")\n",
    "    # optimizer\n",
    "    _optimizer = tf.train.AdamOptimizer(learning_rate=config.learning_rate, beta1=0.9, beta2=0.999,epsilon=1e-8)\n",
    "\n",
    "    # prepare\n",
    "    # inp_next_dict     key: decode时使用的字符串，value: tensor\n",
    "    # placeholder_dict  key: decode时使用的字符串，value: placeholder\n",
    "    #                   目的：为了让后面的流程都使用placeholder进行,这样存储模型可以以这些placeholder为输入口\n",
    "    # ori_feed_dict     key: placeholder        value: tensor\n",
    "    #                   目的：直接sess.run(ori_feed_dict)就可以得到后续流程需要的placeholder的feed_dict;\n",
    "    inp_iterator = get_input(inp_tfrecord_path)\n",
    "    inp_next_dict = inp_iterator.get_next()\n",
    "\n",
    "    # 构造placeholder输入，方便模型文件restore后的使用\n",
    "    # 这里实际上只是把 inp_next 这个“源字典”的 value 都用placeholder替换了，key未变\n",
    "    placeholder_dict = {}\n",
    "    with tf.name_scope(\"input\"):\n",
    "        # train_phase放到这里只是为了共享同一个name_scope\n",
    "        train_phase = tf.placeholder(dtype=tf.bool,name=\"train_phase\")\n",
    "        placeholder_dict[\"train_phase\"]=train_phase\n",
    "        for k,v in inp_next_dict.items():\n",
    "            if k in config.global_multi_hot_fields:\n",
    "                placeholder_dict[k]=tf.sparse_placeholder(dtype=tf.int64,name=k)\n",
    "            elif k in config.global_numeric_fields:\n",
    "                placeholder_dict[k]=tf.placeholder(dtype=tf.float32,name=k)\n",
    "            else:\n",
    "                placeholder_dict[k]=tf.placeholder(dtype=tf.int64,name=k)\n",
    "        # 构造一个feed_dict在训练的时候自动就用它，取placeholder为key，取“源字典”的value为value\n",
    "        ori_feed_dict = {placeholder_dict[k] : inp_next_dict[k] for k,v in inp_next_dict.items()}\n",
    "    \n",
    "    # deepfm\n",
    "    dropout_keep_fm = tf.cond(train_phase,lambda:config.dropout_fm,lambda:[1.0]*len(config.dropout_fm))\n",
    "    dropout_keep_deep = tf.cond(train_phase,lambda:config.dropout_deep,lambda:[1.0]*len(config.dropout_deep))\n",
    "    \n",
    "    deepfm_output = run_deepfm(config,weights,placeholder_dict,train_phase,dropout_keep_fm,dropout_keep_deep)\n",
    "    with tf.name_scope(\"output\"):\n",
    "        pred = tf.reshape(deepfm_output,[-1],name=\"pred\")\n",
    "    # label\n",
    "    label_op = placeholder_dict['label']\n",
    "    \n",
    "    label_cnt_op = tf.bincount(tf.cast(label_op,tf.int32))\n",
    "    # loss\n",
    "    empirical_risk = tf.reduce_mean(tf.losses.log_loss(label_op, pred))\n",
    "    loss_op = empirical_risk\n",
    "    if config.l2_reg>0:\n",
    "        structural_risk = tf.contrib.layers.l2_regularizer(config.l2_reg)(weights[\"concat_projection\"])\n",
    "        for i in range(len(config.deep_layers)):\n",
    "            structural_risk += tf.contrib.layers.l2_regularizer(config.l2_reg)(weights[\"layer_%d\"%i])\n",
    "        tf.summary.scalar('structural_risk_L2',structural_risk)\n",
    "        loss_op = empirical_risk + structural_risk\n",
    "    \n",
    "    # optimize\n",
    "    grad = _optimizer.compute_gradients(loss_op)\n",
    "    optimize_op = _optimizer.apply_gradients(grad)\n",
    "    \n",
    "    # summary (tensorboard)\n",
    "    tf.summary.scalar('total_loss', loss_op)\n",
    "    tf.summary.scalar('empirical_risk_logloss',empirical_risk)\n",
    "\n",
    "    for g,v in grad:\n",
    "        if g is not None:\n",
    "            _=tf.summary.histogram(v.op.name+\"/gradients\",g)\n",
    "    for v in tf.trainable_variables():\n",
    "        _=tf.summary.histogram(v.name.replace(\":0\",\"/value\"),v)\n",
    "    merge_summary = tf.summary.merge_all()#调用sess.run运行图，生成一步的训练过程数据, 是一个option\n",
    "    writer = tf.summary.FileWriter(config.summary_save_dir+\"/summary_%s\" % config.get_now(), graph)\n",
    "    init_op = tf.global_variables_initializer()\n",
    "\n",
    "    inputs = placeholder_dict\n",
    "    outputs = {\"pred\":pred}\n",
    "    mySaver = tf.train.Saver(max_to_keep=2)\n",
    "    \n",
    "train_feed={train_phase:True,inp_tfrecord_path:config.train_tfrecord_file}\n",
    "valid_feed={train_phase:False,inp_tfrecord_path:config.valid_tfrecord_file}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## apus_ad | DeepFM | session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-01T08:39:27.814011Z",
     "start_time": "2018-11-01T08:39:27.810732Z"
    }
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "import time\n",
    "import datetime\n",
    "logger = logging.getLogger()\n",
    "def setup_file_logger(log_file):\n",
    "    hdlr = logging.FileHandler(log_file)\n",
    "    formatter = logging.Formatter('%(levelname)s %(message)s')\n",
    "    hdlr.setFormatter(formatter)\n",
    "    logger.addHandler(hdlr) \n",
    "    logger.setLevel(logging.INFO)\n",
    "\n",
    "def log(message):\n",
    "    new_m = \"|{}| {}\".format(datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S'),message)\n",
    "    print(new_m)\n",
    "    logger.info(new_m)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-01T11:49:03.124754Z",
     "start_time": "2018-11-01T08:39:29.681303Z"
    },
    "code_folding": [
     3,
     34,
     60
    ],
    "deletable": false,
    "editable": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|2018-11-01 16:41:21| [e:01|b:0100] pos/neg:[1428/1644] logloss:[0.6539] auc:[0.6550] [111s]\n",
      "|2018-11-01 16:43:13| [e:01|b:0200] pos/neg:[1447/1625] logloss:[0.6424] auc:[0.6780] [111s]\n",
      "|2018-11-01 16:44:59| [e:01|b:0300] pos/neg:[1383/1689] logloss:[0.6395] auc:[0.6778] [106s]\n",
      "|2018-11-01 16:46:49| [e:01|b:0400] pos/neg:[1439/1633] logloss:[0.6346] auc:[0.6928] [109s]\n",
      "|2018-11-01 16:48:37| [e:01|b:0500] pos/neg:[1387/1685] logloss:[0.6280] auc:[0.6977] [107s]\n",
      "|2018-11-01 16:50:22| [e:01|b:0600] pos/neg:[1353/1719] logloss:[0.6273] auc:[0.6950] [105s]\n",
      "|2018-11-01 16:52:09| [e:01|b:0700] pos/neg:[1438/1634] logloss:[0.6380] auc:[0.6846] [107s]\n",
      "|2018-11-01 16:53:55| [e:01|b:0800] pos/neg:[1401/1671] logloss:[0.6196] auc:[0.7120] [105s]\n",
      "|2018-11-01 16:55:43| [e:01|b:0900] pos/neg:[1421/1651] logloss:[0.6211] auc:[0.7069] [108s]\n",
      "|2018-11-01 16:57:30| [e:01|b:1000] pos/neg:[1427/1645] logloss:[0.6136] auc:[0.7214] [107s]\n",
      "|2018-11-01 16:59:16| [e:01|b:1100] pos/neg:[1376/1696] logloss:[0.6164] auc:[0.7103] [105s]\n",
      "|2018-11-01 16:59:36| [e:01|b:1123] epoch-done\n",
      "    valid_batch_cnt: [365] [0.16s/per]\n",
      "|2018-11-01 17:04:43|  pos/neg: [461976/659304]\n",
      "|2018-11-01 17:04:44| [e:01|b:1123 valid] valid_logloss:[0.62489] valid_auc:[0.69063]\n",
      "|2018-11-01 17:04:44| logloss:[0.62489] auc:[0.69063] global_batch_cnt:[1123] gonna save model ...\n",
      "save model at /home/zhoutong/midas_model/model_2018-11-01-16-39-29\n",
      "|2018-11-01 17:06:36| [e:02|b:0100] pos/neg:[1410/1662] logloss:[0.6041] auc:[0.7321] [109s]\n",
      "|2018-11-01 17:08:23| [e:02|b:0200] pos/neg:[1408/1664] logloss:[0.5870] auc:[0.7529] [107s]\n",
      "|2018-11-01 17:10:10| [e:02|b:0300] pos/neg:[1387/1685] logloss:[0.5801] auc:[0.7598] [106s]\n",
      "|2018-11-01 17:11:58| [e:02|b:0400] pos/neg:[1375/1697] logloss:[0.5737] auc:[0.7703] [108s]\n",
      "|2018-11-01 17:13:44| [e:02|b:0500] pos/neg:[1364/1708] logloss:[0.5841] auc:[0.7520] [105s]\n",
      "|2018-11-01 17:15:31| [e:02|b:0600] pos/neg:[1376/1696] logloss:[0.5836] auc:[0.7511] [106s]\n",
      "|2018-11-01 17:17:19| [e:02|b:0700] pos/neg:[1403/1669] logloss:[0.5891] auc:[0.7514] [108s]\n",
      "|2018-11-01 17:19:04| [e:02|b:0800] pos/neg:[1375/1697] logloss:[0.5771] auc:[0.7587] [105s]\n",
      "|2018-11-01 17:20:51| [e:02|b:0900] pos/neg:[1379/1693] logloss:[0.5753] auc:[0.7626] [106s]\n",
      "|2018-11-01 17:22:38| [e:02|b:1000] pos/neg:[1428/1644] logloss:[0.5808] auc:[0.7565] [107s]\n",
      "|2018-11-01 17:24:26| [e:02|b:1100] pos/neg:[1369/1703] logloss:[0.5665] auc:[0.7712] [107s]\n",
      "|2018-11-01 17:24:47| [e:02|b:1123] epoch-done\n",
      "    valid_batch_cnt: [365] [0.26s/per]\n",
      "|2018-11-01 17:29:50|  pos/neg: [461927/659353]\n",
      "|2018-11-01 17:29:51| [e:02|b:1123 valid] valid_logloss:[0.63737] valid_auc:[0.68553]\n",
      "|2018-11-01 17:31:39| [e:03|b:0100] pos/neg:[1406/1666] logloss:[0.5611] auc:[0.7793] [108s]\n",
      "|2018-11-01 17:33:28| [e:03|b:0200] pos/neg:[1414/1658] logloss:[0.5456] auc:[0.7924] [108s]\n",
      "|2018-11-01 17:35:16| [e:03|b:0300] pos/neg:[1385/1687] logloss:[0.5275] auc:[0.8112] [107s]\n",
      "|2018-11-01 17:37:03| [e:03|b:0400] pos/neg:[1416/1656] logloss:[0.5284] auc:[0.8085] [107s]\n",
      "|2018-11-01 17:38:50| [e:03|b:0500] pos/neg:[1423/1649] logloss:[0.5442] auc:[0.7968] [106s]\n",
      "|2018-11-01 17:40:36| [e:03|b:0600] pos/neg:[1395/1677] logloss:[0.5331] auc:[0.8038] [105s]\n",
      "|2018-11-01 17:42:22| [e:03|b:0700] pos/neg:[1406/1666] logloss:[0.5232] auc:[0.8145] [105s]\n",
      "|2018-11-01 17:44:08| [e:03|b:0800] pos/neg:[1377/1695] logloss:[0.5448] auc:[0.7917] [105s]\n",
      "|2018-11-01 17:45:55| [e:03|b:0900] pos/neg:[1385/1687] logloss:[0.5466] auc:[0.7927] [106s]\n",
      "|2018-11-01 17:47:42| [e:03|b:1000] pos/neg:[1463/1609] logloss:[0.5549] auc:[0.7886] [107s]\n",
      "|2018-11-01 17:49:32| [e:03|b:1100] pos/neg:[1352/1720] logloss:[0.5554] auc:[0.7808] [109s]\n",
      "|2018-11-01 17:49:53| [e:03|b:1123] epoch-done\n",
      "    valid_batch_cnt: [365] [0.18s/per]\n",
      "|2018-11-01 17:54:50|  pos/neg: [461936/659344]\n",
      "|2018-11-01 17:54:51| [e:03|b:1123 valid] valid_logloss:[0.65678] valid_auc:[0.67890]\n",
      "|2018-11-01 17:56:40| [e:04|b:0100] pos/neg:[1429/1643] logloss:[0.5244] auc:[0.8127] [109s]\n",
      "|2018-11-01 17:58:29| [e:04|b:0200] pos/neg:[1364/1708] logloss:[0.5154] auc:[0.8165] [108s]\n",
      "|2018-11-01 18:00:17| [e:04|b:0300] pos/neg:[1355/1717] logloss:[0.5106] auc:[0.8231] [108s]\n",
      "|2018-11-01 18:02:04| [e:04|b:0400] pos/neg:[1383/1689] logloss:[0.5149] auc:[0.8169] [106s]\n",
      "|2018-11-01 18:03:52| [e:04|b:0500] pos/neg:[1377/1695] logloss:[0.5059] auc:[0.8275] [108s]\n",
      "|2018-11-01 18:05:41| [e:04|b:0600] pos/neg:[1358/1714] logloss:[0.5161] auc:[0.8165] [108s]\n",
      "|2018-11-01 18:07:27| [e:04|b:0700] pos/neg:[1384/1688] logloss:[0.5086] auc:[0.8250] [106s]\n",
      "|2018-11-01 18:09:15| [e:04|b:0800] pos/neg:[1399/1673] logloss:[0.5151] auc:[0.8201] [107s]\n",
      "|2018-11-01 18:11:01| [e:04|b:0900] pos/neg:[1336/1736] logloss:[0.5161] auc:[0.8143] [105s]\n",
      "|2018-11-01 18:12:48| [e:04|b:1000] pos/neg:[1378/1694] logloss:[0.5191] auc:[0.8148] [106s]\n",
      "|2018-11-01 18:14:33| [e:04|b:1100] pos/neg:[1332/1740] logloss:[0.5187] auc:[0.8105] [104s]\n",
      "|2018-11-01 18:14:54| [e:04|b:1123] epoch-done\n",
      "    valid_batch_cnt: [365] [0.25s/per]\n",
      "|2018-11-01 18:19:57|  pos/neg: [461945/659335]\n",
      "|2018-11-01 18:19:58| [e:04|b:1123 valid] valid_logloss:[0.67852] valid_auc:[0.67450]\n",
      "|2018-11-01 18:21:48| [e:05|b:0100] pos/neg:[1434/1638] logloss:[0.5050] auc:[0.8287] [110s]\n",
      "|2018-11-01 18:23:37| [e:05|b:0200] pos/neg:[1376/1696] logloss:[0.5119] auc:[0.8183] [108s]\n",
      "|2018-11-01 18:25:24| [e:05|b:0300] pos/neg:[1392/1680] logloss:[0.4844] auc:[0.8479] [106s]\n",
      "|2018-11-01 18:27:13| [e:05|b:0400] pos/neg:[1371/1701] logloss:[0.4922] auc:[0.8361] [108s]\n",
      "|2018-11-01 18:29:00| [e:05|b:0500] pos/neg:[1410/1662] logloss:[0.4958] auc:[0.8329] [106s]\n",
      "|2018-11-01 18:30:48| [e:05|b:0600] pos/neg:[1387/1685] logloss:[0.5022] auc:[0.8292] [107s]\n",
      "|2018-11-01 18:32:32| [e:05|b:0700] pos/neg:[1361/1711] logloss:[0.4888] auc:[0.8382] [104s]\n",
      "|2018-11-01 18:34:19| [e:05|b:0800] pos/neg:[1361/1711] logloss:[0.4997] auc:[0.8300] [107s]\n",
      "|2018-11-01 18:36:10| [e:05|b:0900] pos/neg:[1404/1668] logloss:[0.5010] auc:[0.8264] [110s]\n",
      "|2018-11-01 18:37:56| [e:05|b:1000] pos/neg:[1379/1693] logloss:[0.5066] auc:[0.8238] [106s]\n",
      "|2018-11-01 18:39:43| [e:05|b:1100] pos/neg:[1385/1687] logloss:[0.5070] auc:[0.8228] [107s]\n",
      "|2018-11-01 18:40:04| [e:05|b:1123] epoch-done\n",
      "    valid_batch_cnt: [365] [0.19s/per]\n",
      "|2018-11-01 18:45:11|  pos/neg: [461946/659334]\n",
      "|2018-11-01 18:45:11| [e:05|b:1123 valid] valid_logloss:[0.69870] valid_auc:[0.66998]\n",
      "|2018-11-01 18:47:01| [e:06|b:0100] pos/neg:[1363/1709] logloss:[0.4856] auc:[0.8413] [110s]\n",
      "|2018-11-01 18:48:49| [e:06|b:0200] pos/neg:[1371/1701] logloss:[0.4943] auc:[0.8314] [108s]\n",
      "|2018-11-01 18:50:37| [e:06|b:0300] pos/neg:[1430/1642] logloss:[0.4878] auc:[0.8403] [107s]\n",
      "|2018-11-01 18:52:24| [e:06|b:0400] pos/neg:[1454/1618] logloss:[0.4932] auc:[0.8352] [107s]\n",
      "|2018-11-01 18:54:11| [e:06|b:0500] pos/neg:[1395/1677] logloss:[0.4889] auc:[0.8373] [106s]\n",
      "|2018-11-01 18:56:00| [e:06|b:0600] pos/neg:[1385/1687] logloss:[0.4934] auc:[0.8336] [108s]\n",
      "|2018-11-01 18:57:48| [e:06|b:0700] pos/neg:[1395/1677] logloss:[0.5037] auc:[0.8291] [108s]\n",
      "|2018-11-01 18:59:39| [e:06|b:0800] pos/neg:[1422/1650] logloss:[0.5090] auc:[0.8231] [110s]\n",
      "|2018-11-01 19:01:28| [e:06|b:0900] pos/neg:[1406/1666] logloss:[0.4934] auc:[0.8350] [109s]\n",
      "|2018-11-01 19:03:17| [e:06|b:1000] pos/neg:[1417/1655] logloss:[0.5006] auc:[0.8278] [108s]\n",
      "|2018-11-01 19:05:07| [e:06|b:1100] pos/neg:[1354/1718] logloss:[0.5036] auc:[0.8230] [110s]\n",
      "|2018-11-01 19:05:29| [e:06|b:1123] epoch-done\n",
      "    valid_batch_cnt: [365] [0.21s/per]\n",
      "|2018-11-01 19:10:32|  pos/neg: [461957/659323]\n",
      "|2018-11-01 19:10:32| [e:06|b:1123 valid] valid_logloss:[0.72388] valid_auc:[0.66754]\n",
      "|2018-11-01 19:12:24| [e:07|b:0100] pos/neg:[1430/1642] logloss:[0.4771] auc:[0.8452] [111s]\n",
      "|2018-11-01 19:14:11| [e:07|b:0200] pos/neg:[1448/1624] logloss:[0.4935] auc:[0.8351] [107s]\n",
      "|2018-11-01 19:15:57| [e:07|b:0300] pos/neg:[1415/1657] logloss:[0.4701] auc:[0.8534] [106s]\n",
      "|2018-11-01 19:17:44| [e:07|b:0400] pos/neg:[1393/1679] logloss:[0.4645] auc:[0.8584] [106s]\n",
      "|2018-11-01 19:19:32| [e:07|b:0500] pos/neg:[1387/1685] logloss:[0.4837] auc:[0.8407] [107s]\n",
      "|2018-11-01 19:21:21| [e:07|b:0600] pos/neg:[1344/1728] logloss:[0.4756] auc:[0.8449] [109s]\n",
      "|2018-11-01 19:23:08| [e:07|b:0700] pos/neg:[1380/1692] logloss:[0.4852] auc:[0.8405] [106s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|2018-11-01 19:24:57| [e:07|b:0800] pos/neg:[1383/1689] logloss:[0.4680] auc:[0.8534] [108s]\n",
      "|2018-11-01 19:26:44| [e:07|b:0900] pos/neg:[1391/1681] logloss:[0.4850] auc:[0.8369] [107s]\n",
      "|2018-11-01 19:28:32| [e:07|b:1000] pos/neg:[1377/1695] logloss:[0.4727] auc:[0.8479] [108s]\n",
      "|2018-11-01 19:30:21| [e:07|b:1100] pos/neg:[1347/1725] logloss:[0.4820] auc:[0.8446] [108s]\n",
      "|2018-11-01 19:30:41| [e:07|b:1123] epoch-done\n",
      "    valid_batch_cnt: [365] [0.21s/per]\n",
      "|2018-11-01 19:35:46|  pos/neg: [461949/659331]\n",
      "|2018-11-01 19:35:46| [e:07|b:1123 valid] valid_logloss:[0.74598] valid_auc:[0.66320]\n",
      "|2018-11-01 19:37:35| [e:08|b:0100] pos/neg:[1371/1701] logloss:[0.4706] auc:[0.8491] [109s]\n",
      "|2018-11-01 19:39:23| [e:08|b:0200] pos/neg:[1345/1727] logloss:[0.4673] auc:[0.8507] [107s]\n",
      "|2018-11-01 19:41:13| [e:08|b:0300] pos/neg:[1475/1597] logloss:[0.4617] auc:[0.8605] [110s]\n",
      "|2018-11-01 19:43:01| [e:08|b:0400] pos/neg:[1416/1656] logloss:[0.4661] auc:[0.8535] [107s]\n",
      "|2018-11-01 19:44:48| [e:08|b:0500] pos/neg:[1410/1662] logloss:[0.4664] auc:[0.8526] [106s]\n",
      "|2018-11-01 19:46:36| [e:08|b:0600] pos/neg:[1378/1694] logloss:[0.4883] auc:[0.8368] [108s]\n",
      "|2018-11-01 19:48:26| [e:08|b:0700] pos/neg:[1357/1715] logloss:[0.4826] auc:[0.8375] [110s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-101-6cb194d360b2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     73\u001b[0m                 \u001b[0mbatch_cnt\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m                 \u001b[0mglobal_batch_cnt\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m                 \u001b[0mtrain_feed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mori_feed_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m                 \u001b[0mrun_ops\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0moptimize_op\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mloss_op\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlabel_op\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlabel_cnt_op\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmerge_summary\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m                 \u001b[0mrun_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_ops\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_feed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/python3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    885\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    886\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 887\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    888\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    889\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/python3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1108\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1109\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1110\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1111\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/python3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1284\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1285\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1286\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1287\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1288\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/python3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1290\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1291\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1292\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1293\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1294\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/python3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1275\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1276\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1277\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1278\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1279\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/python3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1365\u001b[0m     return tf_session.TF_SessionRun_wrapper(\n\u001b[1;32m   1366\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1367\u001b[0;31m         run_metadata)\n\u001b[0m\u001b[1;32m   1368\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1369\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_feed={train_phase:True,inp_tfrecord_path:config.train_tfrecord_file}\n",
    "valid_feed={train_phase:False,inp_tfrecord_path:config.valid_tfrecord_file}\n",
    "\n",
    "def _evaluate(sess,valid_dict):\n",
    "    pred_deque,label_deque,label_cnt_deque=deque(),deque(),deque()\n",
    "    batch_cnt = 0\n",
    "    while True:\n",
    "        try:\n",
    "            valid_dict.update(sess.run(ori_feed_dict))\n",
    "            batch_cnt += 1\n",
    "            t1 = time.time()\n",
    "            pred_,label_,label_cnt_= sess.run([pred,label_op,label_cnt_op],valid_dict)\n",
    "            pred_deque.extend(pred_)\n",
    "            label_deque.extend(label_)\n",
    "            label_cnt_deque.append(label_cnt_)\n",
    "        except tf.errors.OutOfRangeError:\n",
    "            sys.stdout.write(\"\\n\")\n",
    "            sys.stdout.flush()\n",
    "            break\n",
    "        delta_t = time.time() - t1\n",
    "        sys.stdout.write(f\"    valid_batch_cnt: [{batch_cnt:0>3d}] [{delta_t:.2f}s/per]\\r\")\n",
    "        sys.stdout.flush()\n",
    "\n",
    "    pred_arr = np.array(pred_deque)\n",
    "    label_arr = np.array(label_deque)\n",
    "    label_cnt_arr = np.array(label_cnt_deque)\n",
    "    pos=label_cnt_arr[:,1].sum()\n",
    "    neg = label_cnt_arr[:,0].sum()\n",
    "    now = time.strftime(\"|%Y-%m-%d %H:%M:%S| \", time.localtime(time.time()))\n",
    "    print(\"%s pos/neg: [%s/%s]\" % (now,pos,neg))\n",
    "    auc = roc_auc_score(label_arr,pred_arr)\n",
    "    loss = log_loss(label_arr,pred_arr,eps=1e-7)\n",
    "    return loss,auc\n",
    "\n",
    "def _simple_save(sess,path,inputs,outputs,global_batch_cnt,auc,use_simple_save = False, use_ONNX=False):\n",
    "    print(\"save model at %s\" % path)\n",
    "    if use_simple_save:\n",
    "#         if(os.path.exists(path)):\n",
    "#             shutil.rmtree(path)\n",
    "#             print(\"路径存在，删除原模型..\")\n",
    "        tf.saved_model.simple_save(sess,path+f\"/model_of_auc-{auc:.5f}\",inputs,outputs)\n",
    "    else:\n",
    "        mySaver.save(sess, path+\"/model.ckpt\", global_step=global_batch_cnt)\n",
    "    if use_ONNX:\n",
    "        onnx_graph = tf2onnx.tfonnx.process_tf_graph(sess.graph)\n",
    "        onnx_inputs = [v.name for k,v in inputs.items()]\n",
    "        onnx_outputs = [v.name for k,v in outputs.items()]\n",
    "        model_proto = onnx_graph.make_model(\"test\", onnx_inputs, onnx_outputs)\n",
    "        with open(path+\"/model.onnx\",\"wb\") as f:\n",
    "            f.write(model_proto.SerializeToString())\n",
    "    pass\n",
    "\n",
    "\n",
    "out_time=config.get_now()\n",
    "setup_file_logger(\"/home/zhoutong/logging/%s\" % out_time)\n",
    "\n",
    "model_config = tf.ConfigProto(allow_soft_placement=True, log_device_placement=False)\n",
    "model_config.gpu_options.allow_growth = True\n",
    "sess = tf.Session(config=model_config,graph=graph)\n",
    "\n",
    "\n",
    "with sess as sess:\n",
    "    sess.run(init_op)\n",
    "    global_auc,global_batch_cnt,batch_cnt,epoch_cnt =0,0,0,0\n",
    "    \n",
    "    for epoch in range(10):\n",
    "        epoch_cnt += 1\n",
    "        batch_cnt = 0\n",
    "        sess.run(inp_iterator.initializer,train_feed)\n",
    "        t0=time.time()\n",
    "        while True:\n",
    "            try:\n",
    "                batch_cnt += 1\n",
    "                global_batch_cnt += 1\n",
    "                train_feed.update(sess.run(ori_feed_dict))\n",
    "                run_ops=[optimize_op,loss_op,pred,label_op,label_cnt_op,merge_summary]\n",
    "                run_result = sess.run(run_ops,train_feed)\n",
    "                _,loss_,pred_,label_,label_cnt_,merge_summary_ = run_result\n",
    "                writer.add_summary(merge_summary_,global_batch_cnt)\n",
    "                neg,pos = label_cnt_\n",
    "                if batch_cnt % 100 == 0:\n",
    "                    auc = roc_auc_score(label_,pred_)\n",
    "                    batch_time = int(time.time()-t0)\n",
    "                    log(f\"[e:{epoch_cnt:0>2d}|b:{batch_cnt:0>4d}] pos/neg:[{pos}/{neg}] logloss:[{loss_:.4f}] auc:[{auc:.4f}] [{batch_time}s]\")\n",
    "                    t0=time.time()\n",
    "                # 存在严重缺陷，这里如果用valid初始化后，从1001batch开始都会从valid里面拿数据了\n",
    "    #             if batch_cnt % 1000 ==0:\n",
    "    #                 sess.run(inp_iterator.initializer,valid_dict)\n",
    "    #                 logloss,auc=_evaluate(sess,valid_feed)\n",
    "    #                 now = time.strftime(\"|%Y-%m-%d %H:%M:%S| \", time.localtime(time.time()))\n",
    "    #                 print(f\"{now} [e:{epoch_cnt:0>2d}|b:{batch_cnt:0>4d} valid] logloss:[{logloss:.5f}] auc:[{auc:.5f}]\")\n",
    "            except tf.errors.OutOfRangeError:\n",
    "                break\n",
    "        log(f\"[e:{epoch_cnt:0>2d}|b:{batch_cnt:0>4d}] epoch-done\")\n",
    "        sess.run(inp_iterator.initializer,valid_feed)\n",
    "        logloss,auc=_evaluate(sess,valid_feed)\n",
    "\n",
    "        log(f\"[e:{epoch_cnt:0>2d}|b:{batch_cnt:0>4d} valid] valid_logloss:[{logloss:.5f}] valid_auc:[{auc:.5f}]\")\n",
    "        if global_auc<auc:\n",
    "            global_auc = auc\n",
    "            log(f\"logloss:[{logloss:.5f}] auc:[{auc:.5f}] global_batch_cnt:[{global_batch_cnt:0>4d}] gonna save model ...\")\n",
    "#             _simple_save(sess,config.model_save_dir+f\"/model_example\",inputs,outputs,global_batch_cnt)\n",
    "            _simple_save(sess,config.model_save_dir+f\"/model_{out_time}\",inputs,outputs,global_batch_cnt,auc)\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-25T04:47:53.094942Z",
     "start_time": "2018-10-25T04:47:53.090907Z"
    }
   },
   "source": [
    "## Restore | 构造数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-30T08:10:48.078492Z",
     "start_time": "2018-10-30T08:10:48.067771Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def make_data_ckpt():\n",
    "    valid_input_dict = {\"input/train_phase:0\":False}\n",
    "    label = None\n",
    "    with tf.Session(graph=graph) as sess:\n",
    "        sess.run(inp_iterator.initializer,valid_feed)\n",
    "        tmpdict = sess.run(inp_next_dict)\n",
    "    for k,v in tmpdict.items():\n",
    "        if k==\"label\":\n",
    "            label = v\n",
    "        elif type(v) == tf.SparseTensorValue:\n",
    "            valid_input_dict.update({\"input/\"+k+\"/shape:0\":v.dense_shape, \"input/\"+k+\"/indices:0\":v.indices, \"input/\"+k+\"/values:0\":v.values})\n",
    "        else:\n",
    "            valid_input_dict.update({\"input/\"+k+\":0\":v})\n",
    "    return valid_input_dict,label\n",
    "\n",
    "def make_data_pb():\n",
    "    valid_input_dict = {\"train_phase\":False}\n",
    "    label = None\n",
    "    with tf.Session(graph=graph) as sess:\n",
    "        sess.run(inp_iterator.initializer,valid_feed)\n",
    "        tmpdict = sess.run(inp_next_dict)\n",
    "    for k,v in tmpdict.items():\n",
    "        if k==\"label\":\n",
    "            label = v\n",
    "        else:\n",
    "            valid_input_dict.update({k:v})\n",
    "    return valid_input_dict,label\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Restore | pb \n",
    "- ### simple_save API存的pb模型好像有问题，restore后auc0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-30T08:23:14.021275Z",
     "start_time": "2018-10-30T08:23:13.245696Z"
    }
   },
   "outputs": [],
   "source": [
    "from tensorflow.python.saved_model import tag_constants\n",
    "# model_p = \"/home/zhoutong/tf_serve/midas_model/v3\"\n",
    "model_p = \"/home/zhoutong/tf_serve/midas/model_2018-10-30-11-29-41/\"\n",
    "# model_p = \"/home/zhoutong/starksdk_model/model_2018-10-25-19-56-17/model_of_auc-0.87386\"\n",
    "\n",
    "# valid_input_dict,label = make_data_pb()\n",
    "valid_input_dict_ckpt,label = make_data_ckpt()\n",
    "\n",
    "\n",
    "####################\n",
    "# 方式一： 仍然用 graph  session回复模型然后预测,这个仍然需要feed_dict,用ckpt转成的pb\n",
    "class Inference(object):\n",
    "    def __init__(self,model_path,out_tensor_name=\"output/pred:0\",inp_tensor_prefix=\"input\"):\n",
    "        # params\n",
    "        self.model_p = model_path\n",
    "        self.out_tensor_name= out_tensor_name\n",
    "        self.inp_tensor_prefix = inp_tensor_prefix\n",
    "        # graph & sess\n",
    "        self.graph = tf.Graph()\n",
    "        with self.graph.as_default():\n",
    "            model_config = tf.ConfigProto(allow_soft_placement=True, log_device_placement=False)\n",
    "            model_config.gpu_options.allow_growth = True\n",
    "            self.sess = tf.Session(graph=self.graph,config=model_config)\n",
    "            # restore\n",
    "            _ = tf.saved_model.loader.load(self.sess,[tag_constants.SERVING],self.model_p)\n",
    "            init_op = tf.global_variables_initializer()\n",
    "        # init\n",
    "        self.sess.run(init_op)\n",
    "        # prepare input & output\n",
    "        self.pred = self.sess.graph.get_tensor_by_name(out_tensor_name)\n",
    "        self.to_feed_ph = []\n",
    "        for op in self.sess.graph.get_operations():\n",
    "            if op.name.startswith(self.inp_tensor_prefix) and \"label\" not in op.name :\n",
    "                ph = self.sess.graph.get_tensor_by_name(op.name+\":0\")\n",
    "                self.to_feed_ph.append(ph)\n",
    "        print(\"name of tensors(placeholder) to input:\")\n",
    "        for ph in self.to_feed_ph: \n",
    "            print(\"    \",ph.name)       \n",
    "    \n",
    "    def infer(self,inp_dict):\n",
    "        feed_dict = {ph:inp_dict[ph.name] for ph in self.to_feed_ph}\n",
    "        pred = self.sess.run(self.pred,feed_dict)\n",
    "        return pred\n",
    "\n",
    "infer_model = Inference(model_path=model_p)\n",
    "y_pred = infer_model.infer(valid_input_dict_ckpt)\n",
    "        \n",
    "print(\"*\"*50)\n",
    "#####################\n",
    "# 方式二：直接使用api， 不适用于ckpt转成的pb\n",
    "# predict_fn = tf.contrib.predictor.from_saved_model(model_p)\n",
    "# predictions = predict_fn(valid_input_dict)['pred']\n",
    "# print(\"predictions:\\n\",predictions)\n",
    "# print(\"label:\\n\",label)\n",
    "# print(\"auc:\\n\",roc_auc_score(label,predictions))\n",
    "# print(\"logloss:\\n\",log_loss(label, predictions))\n",
    "\n",
    "print(\"y_pred:\\n\",y_pred)\n",
    "print(\"label:\\n\",label)\n",
    "if label is not None:\n",
    "    print(\"auc:\\n\",roc_auc_score(label,y_pred))\n",
    "    print(\"logloss:\\n\",log_loss(label, y_pred))\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Restore | ckpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-30T08:24:07.041092Z",
     "start_time": "2018-10-30T08:24:04.367444Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "valid_input_dict,label = make_data_ckpt()\n",
    "base_path = \"/home/zhoutong/midas_model/model_example\"\n",
    "base_path =\"/home/zhoutong/starksdk_model/model_2018-10-26-14-05-09\"\n",
    "model_path = tf.train.latest_checkpoint(base_path) #  或者指定模型 +\"/model.ckpt-562\"\n",
    "\n",
    "def inference_with_ckpt(valid_input_dict,model_p,label=None):\n",
    "    g1 = tf.Graph()\n",
    "    with g1.as_default():\n",
    "        with tf.Session(graph=g1) as sess:\n",
    "            saver = tf.train.import_meta_graph(model_p+\".meta\")\n",
    "            saver.restore(sess, model_p)\n",
    "            valid_placeholder_dict= {}\n",
    "            for op in graph.get_operations():\n",
    "                if op.name.startswith(\"input\") and \"label\" not in op.name :\n",
    "                    ph = sess.graph.get_tensor_by_name(op.name+\":0\")\n",
    "                    valid_placeholder_dict.update({ph:valid_input_dict[op.name+\":0\"]})\n",
    "            y_pred_op = sess.graph.get_tensor_by_name(\"output/pred:0\")\n",
    "            y_pred = sess.run(y_pred_op,valid_placeholder_dict)\n",
    "            print(\"y_pred:\\n\",y_pred)\n",
    "    if label is not None:\n",
    "        loss = log_loss(label,y_pred,eps=1e-7)\n",
    "        auc = roc_auc_score(label,y_pred)\n",
    "        print(f\"loss:[{loss:.4f}] auc:[{auc:.4f}]\")\n",
    "    pass\n",
    "\n",
    "inference_with_ckpt(valid_input_dict,model_path,label)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resotre | ckpt -> pb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-30T06:15:53.873510Z",
     "start_time": "2018-10-30T06:15:52.015223Z"
    },
    "code_folding": [],
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# params\n",
    "ckpt_model_dir = \"/home/zhoutong/midas_model\"+\"/model_2018-10-30-11-29-41\"\n",
    "output_tensorName = \"output/pred:0\"\n",
    "input_prefix = \"input\"\n",
    "label_name = \"label\"\n",
    "\n",
    "# load the model\n",
    "checkpoint = tf.train.latest_checkpoint(ckpt_model_dir) # checkpoint = model_save_path + \"/model.ckpt-562\"\n",
    "\n",
    "# tf_server directory\n",
    "serve_path = \"/home/zhoutong/tf_serve/{model_name}/{version}\".format(model_name=\"midas_model\",version=ckpt_model_dir.split(\"/\")[-1])\n",
    "\n",
    "# transforming\n",
    "tf.reset_default_graph()\n",
    "with tf.Session() as sess:\n",
    "    saver = tf.train.import_meta_graph(checkpoint+\".meta\")\n",
    "    graph = tf.get_default_graph()\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    # to save in model\n",
    "    model_outputs = tf.saved_model.utils.build_tensor_info(graph.get_tensor_by_name(output_tensorName))\n",
    "    model_inputs_dict = {}\n",
    "    print(\"input op.names :\")\n",
    "    for op in graph.get_operations():\n",
    "        if op.name.startswith(input_prefix) and label_name not in op.name :\n",
    "            print(\"    \",op.name)\n",
    "            ph = sess.graph.get_tensor_by_name(op.name+\":0\")\n",
    "            model_inputs_dict.update({op.name+\":0\" : tf.saved_model.utils.build_tensor_info(ph)})\n",
    "    # struct save model\n",
    "    signature_definition = tf.saved_model.signature_def_utils.build_signature_def(inputs = model_inputs_dict,\n",
    "                                                                                  outputs={'outputs':model_outputs},\n",
    "                                                                                  method_name=tf.saved_model.signature_constants.PREDICT_METHOD_NAME)\n",
    "    signature_def_map = {tf.saved_model.signature_constants.DEFAULT_SERVING_SIGNATURE_DEF_KEY:signature_definition}\n",
    "    builder = tf.saved_model.builder.SavedModelBuilder(serve_path)\n",
    "    builder.add_meta_graph_and_variables(sess,[tf.saved_model.tag_constants.SERVING],signature_def_map=signature_def_map)\n",
    "    builder.save()\n",
    "    print(\"model saved at: %s\" % serve_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Restore | pb -> onnx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-30T07:32:30.797955Z",
     "start_time": "2018-10-30T07:32:30.718796Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.core.framework import graph_pb2\n",
    "\n",
    "from onnx_tf.common import get_output_node_names\n",
    "from onnx_tf.frontend import tensorflow_graph_to_onnx_model\n",
    "from google.protobuf import text_format\n",
    "\n",
    "graph_def = graph_pb2.GraphDef()\n",
    "with open(\"/home/zhoutong/tf_serve/midas_model/model_2018-10-30-11-29-41/saved_model.pb\", \"rb+\") as f:   \n",
    "    # load tf graph def\n",
    "    proto_b = f.read()\n",
    "    ?text_format.Merge\n",
    "    text_format.Merge(proto_b, graph_def)\n",
    "#     graph_def.ParseFromString(proto_b)\n",
    "    \n",
    "    \n",
    "# get output node names\n",
    "output = get_output_node_names(graph_def)\n",
    "\n",
    "# convert tf graph to onnx model\n",
    "model = tensorflow_graph_to_onnx_model(graph_def, output)\n",
    "# with open(\"output_path\", 'wb') as f:\n",
    "#     f.write(model.SerializeToString())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-30T07:04:23.252065Z",
     "start_time": "2018-10-30T07:04:23.244167Z"
    }
   },
   "outputs": [],
   "source": [
    "fileContent[:100]\n",
    "fileContent[:100].strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## apus_ad | DeepFM | Estimator API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ### train_func、eval_func -> input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_input_fn():\n",
    "    pass\n",
    "\n",
    "def eval_input_fn():\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ### model_func -> classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deepfm_model_fn(features, labels, mode):\n",
    "    # log\n",
    "    if mode == tf.estimator.ModeKeys.PREDICT:\n",
    "        tf.logging.info(\"my_model_fn: PREDICT, {}\".format(mode))\n",
    "    elif mode == tf.estimator.ModeKeys.EVAL:\n",
    "        tf.logging.info(\"my_model_fn: EVAL, {}\".format(mode))\n",
    "    elif mode == tf.estimator.ModeKeys.TRAIN:\n",
    "        tf.logging.info(\"my_model_fn: TRAIN, {}\".format(mode))\n",
    "\n",
    "    inp_list = features\n",
    "    weights = _initialize_weights()\n",
    "    with tf.name_scope(\"input\"):\n",
    "        train_phase = tf.placeholder(tf.bool,name=\"train_phase\")\n",
    "        dropout_keep_fm = tf.cond(train_phase,lambda:config.dropout_fm,lambda:[1.0]*len(config.dropout_fm))\n",
    "        dropout_keep_deep = tf.cond(train_phase,lambda:config.dropout_deep,lambda:[1.0]*len(config.dropout_deep))\n",
    "    deepfm_output = run_deepfm(config,weights,inp_list,train_phase,dropout_keep_fm,dropout_keep_deep)\n",
    "    # prediction\n",
    "    with tf.name_scope(\"output\"):\n",
    "        pred = tf.reshape(deepfm_output,[-1],name=\"pred\")\n",
    "    # logloss\n",
    "    loss_op = tf.reduce_mean(tf.losses.log_loss(labels, pred))\n",
    "    if config.l2_reg>0:\n",
    "        loss_op += tf.contrib.layers.l2_regularizer(config.l2_reg)(weights[\"concat_projection\"])\n",
    "        for i in range(len(config.deep_layers)):\n",
    "            loss_op += tf.contrib.layers.l2_regularizer(config.l2_reg)(weights[\"layer_%d\"%i])\n",
    "    # train_op\n",
    "    _optimizer = tf.train.AdamOptimizer(learning_rate=config.learning_rate, beta1=0.9, beta2=0.999,epsilon=1e-8)\n",
    "    grads = _optimizer.compute_gradients(loss)\n",
    "    for grad, var in grads:\n",
    "        if grad is not None:\n",
    "            tf.summary.histogram(var.op.name + '/gradients', grad)\n",
    "    for var in tf.trainable_variables():\n",
    "        tf.summary.histogram(var.name, var)\n",
    "    train_op = _optimizer.apply_gradients(grads)\n",
    "\n",
    "    # tf.summary\n",
    "    accuracy = tf.metrics.accuracy(labels, predictions=pred, name='acc_op')\n",
    "    recall = tf.metrics.recall(labels, predictions=pred, name='recall_op')\n",
    "    auc = tf.metrics.auc(labels, predictions=pred, name='auc_op')\n",
    "    metrics = {'accuracy': accuracy, 'recall': recall, 'auc': auc}\n",
    "    tf.summary.scalar('my_accuracy', accuracy[1])\n",
    "    tf.summary.scalar('my_precision', precision[1])\n",
    "    tf.summary.scalar('my_recall', recall[1])\n",
    "    tf.summary.scalar('my_auc', auc[1])\n",
    "\n",
    "    if mode==tf.estimator.ModeKeys.PREDICT:\n",
    "        predictions = {\"pred\":pred}\n",
    "        return tf.estimator.EstimatorSpec(mode, predictions=predictions)\n",
    "    if mode==tf.estimator.ModeKeys.EVAL:\n",
    "        return tf.estimator.EstimatorSpec(mode, loss=loss, eval_metric_ops=metrics)\n",
    "    if mode==tf.estimator.ModeKeys.TRAIN:\n",
    "        return tf.estimator.EstimatorSpec(mode, loss=loss, train_op=train_op)\n",
    "\n",
    "\n",
    "e_config = tf.estimator.RunConfig(save_checkpoints_steps=100,\n",
    "                                  save_summary_steps=10,\n",
    "                                  keep_checkpoint_max=3)\n",
    "classifier = tf.estimator.Estimator(model_fn=deepfm_model_fn, \n",
    "                                    model_dir=config.model_dir+\"/estimator\",\n",
    "                                    config=e_config)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ### Traning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.train(input_fn=lambda: train_inpu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ### Evaluation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ### Prediction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 测试 | FM部分是否正常（观察到embd不更新）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-31T12:09:00.357011Z",
     "start_time": "2018-10-31T11:50:28.813951Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "gg = tf.Graph()\n",
    "import shutil\n",
    "with gg.as_default():\n",
    "    # 需要： weights feat_total_idx_sp feat_total_value_sp\n",
    "    dropout_keep_fm=[1.0,1.0]\n",
    "    inp_tfrecord_path = tf.placeholder(dtype=tf.string, name=\"tfrecord_path\")\n",
    "    _optimizer = tf.train.AdamOptimizer(learning_rate=config.learning_rate, beta1=0.9, beta2=0.999,epsilon=1e-8)\n",
    "    inp_iterator = get_input(inp_tfrecord_path)\n",
    "    inp_next_dict = inp_iterator.get_next()\n",
    "    feat_total_idx_sp,feat_total_value_sp = _get_total_feature(inp_next_dict)\n",
    "\n",
    "    weights = _initialize_weights()\n",
    "    y_first_order = tf.nn.embedding_lookup_sparse(\n",
    "            weights[\"feature_bias\"],\n",
    "            sp_ids=feat_total_idx_sp,\n",
    "            sp_weights=feat_total_value_sp,\n",
    "            combiner=\"sum\")\n",
    "\n",
    "    y_first_order = tf.nn.dropout(\n",
    "            y_first_order,\n",
    "            dropout_keep_fm[0],\n",
    "            name=\"y_first_order_dropout\")\n",
    "\n",
    "\n",
    "    # sum_square part\n",
    "    summed_features_emb_square = tf.square(\n",
    "        tf.nn.embedding_lookup_sparse(\n",
    "            weights[\"feature_embeddings\"],\n",
    "            sp_ids=feat_total_idx_sp,\n",
    "            sp_weights=feat_total_value_sp,\n",
    "            combiner=\"sum\"))\n",
    "    # square_sum part\n",
    "    squared_sum_features_emb = tf.nn.embedding_lookup_sparse(\n",
    "        tf.square(weights[\"feature_embeddings\"]),\n",
    "        sp_ids=feat_total_idx_sp,\n",
    "        sp_weights=tf.square(feat_total_value_sp),\n",
    "        combiner=\"sum\")\n",
    "    # second order\n",
    "    y_second_order = 0.5 * tf.subtract(\n",
    "        summed_features_emb_square,\n",
    "        squared_sum_features_emb)  # None * K\n",
    "    y_second_order = tf.nn.dropout(y_second_order,\n",
    "                                   dropout_keep_fm[1])  # None * K\n",
    "\n",
    "    # label\n",
    "    label = tf.cast(inp_next_dict['label'],dtype=tf.float32)\n",
    "    # fm-way\n",
    "    y_fm = tf.reshape(tf.reduce_sum(y_first_order+y_second_order,axis=1),[-1])\n",
    "#     concat-way\n",
    "    concat_input = tf.concat([y_first_order,y_second_order],axis=1)\n",
    "    y_concat = tf.reshape(tf.matmul(concat_input, weights[\"concat_projection\"])+weights['concat_bias'],[-1])\n",
    "    \n",
    "#     y_pred = tf.nn.sigmoid(y_fm)\n",
    "    y_pred = tf.nn.sigmoid(y_concat)\n",
    "    cost = tf.reduce_mean(tf.losses.log_loss(label, y_pred))\n",
    "    opt=tf.train.AdamOptimizer(learning_rate=0.001)\n",
    "    grad = opt.compute_gradients(cost)\n",
    "    train_op=opt.apply_gradients(grad)\n",
    "    feed_dict = {inp_tfrecord_path:config.train_tfrecord_file}\n",
    "\n",
    "    # summary\n",
    "    summary_save_path = \"/home/zhoutong/tmp_summary\"\n",
    "    if os.path.exists(summary_save_path): shutil.rmtree(summary_save_path)\n",
    "    _ = tf.summary.scalar(\"logloss\",cost)\n",
    "    for g,v in grad:\n",
    "        if g is not None:\n",
    "            _=tf.summary.histogram(v.op.name+\"/gradients\",g)\n",
    "    for v in tf.trainable_variables():\n",
    "        _=tf.summary.histogram(v.name.replace(\":0\",\"/value\"),v)\n",
    "    summary_op = tf.summary.merge_all()\n",
    "    writer = tf.summary.FileWriter(summary_save_path, tf.get_default_graph())\n",
    "\n",
    "with tf.Session(graph=gg) as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    epoch=10\n",
    "    i = 0\n",
    "    for _ in range(epoch):\n",
    "        sess.run(inp_iterator.initializer,feed_dict)\n",
    "        while True:\n",
    "            i += 1\n",
    "            try:\n",
    "                to_run = [train_op,weights,cost,summary_op,y_first_order,y_second_order,y_fm]\n",
    "                _,w,c,summary,y_1st,y_2nd,y_nosigmoid=sess.run(to_run,feed_dict,)\n",
    "                writer.add_summary(summary,i)\n",
    "                if (i % 20 ==0 and i < 200) or (i % 100==0 and i >=500):\n",
    "                    print(\"*\"*20,f\"at range:{i}\",\"*\"*20)\n",
    "                    c\n",
    "            except tf.errors.OutOfRangeError:\n",
    "                break\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 测试 | TFRecordDataset作为placeholder的输入\n",
    "- 构造一个Dataset作为输入\n",
    "- 因为TFRecordDataset传的是文件路径，意味着如果要预测的数据必须先存成一个文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset1 = tf.data.TFRecordDataset(valid_tfrecord_file,compression_type = compression_type)\n",
    "test_ph = tf.placeholder()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 测试 | 每个epoch重新加载dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-25T02:27:09.541600Z",
     "start_time": "2018-10-25T02:27:09.233191Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sess = tf.Session()\n",
    "\n",
    "# sample = [[1,2,3] for _ in range(5)]\n",
    "train = np.random.sample((10,3))\n",
    "valid = np.random.sample((4,3))*10\n",
    "\n",
    "train,valid\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices(train).batch(2)\n",
    "valid_dataset = tf.data.Dataset.from_tensor_slices(valid).batch(2)\n",
    "#################\n",
    "# 第一种\n",
    "# it = tf.data.Iterator.from_structure(train_dataset.output_types, train_dataset.output_shapes)\n",
    "# train_init_op = it.make_initializer(train_dataset)\n",
    "# valid_init_op = it.make_initializer(valid_dataset)\n",
    "# epoch = 0\n",
    "# for e in range(3):\n",
    "#     epoch += 1\n",
    "#     sess.run(train_init_op)\n",
    "#     print(\"train-data:\")\n",
    "#     while True:\n",
    "#         try:\n",
    "#             sess.run(it.get_next())\n",
    "#         except tf.errors.OutOfRangeError:\n",
    "#             print(\"epcoh: %s train-data done\" % epoch)\n",
    "#             sess.run(valid_init_op)\n",
    "#             print(\"valid-data:\")\n",
    "#             while True:\n",
    "#                 try:\n",
    "#                     sess.run(it.get_next())\n",
    "#                 except tf.errors.OutOfRangeError:\n",
    "#                     break\n",
    "#             print(\"*\"*40)\n",
    "#             break\n",
    "\n",
    "\n",
    "\n",
    "################\n",
    "# 第二种\n",
    "it_train = train_dataset.make_initializable_iterator()\n",
    "it_valid = valid_dataset.make_initializable_iterator()\n",
    "epoch = 0\n",
    "for e in range(3):\n",
    "    epoch += 1\n",
    "    sess.run(it_train.initializer)\n",
    "    print(\"train-data:\")\n",
    "    while True:\n",
    "        try:\n",
    "            sess.run(it_train.get_next())\n",
    "        except tf.errors.OutOfRangeError:\n",
    "            print(\"epcoh: %s train-data done\" % epoch)\n",
    "            sess.run(it_valid.initializer)\n",
    "            print(\"valid-data:\")\n",
    "            while True:\n",
    "                try:\n",
    "                    sess.run(it_valid.get_next())\n",
    "                except tf.errors.OutOfRangeError:\n",
    "                    break\n",
    "            print(\"*\"*40)\n",
    "            break\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 测试 | tf.cond"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-19T07:20:14.062385Z",
     "start_time": "2018-10-19T07:20:13.979961Z"
    },
    "code_folding": [
     4
    ]
   },
   "outputs": [],
   "source": [
    "a_graph = tf.Graph()\n",
    "with a_graph.as_default():\n",
    "    def _get_train_input():\n",
    "        \"\"\"\n",
    "        目前预处理没有区分验证集和训练集\n",
    "        \"\"\"\n",
    "\n",
    "        train = np.random.sample((10,3))\n",
    "        train_dataset = tf.data.Dataset.from_tensor_slices(train).batch(2)\n",
    "        iterator = train_dataset.make_initializable_iterator()\n",
    "        return iterator\n",
    "\n",
    "    # 验证集解析\n",
    "    def _get_valid_input():\n",
    "        \"\"\"\n",
    "        目前预处理没有区分验证集和训练集\n",
    "        \"\"\"\n",
    "        valid = np.random.sample((10,3))*100\n",
    "        valid_dataset = tf.data.Dataset.from_tensor_slices(valid).batch(2)\n",
    "        iterator = valid_dataset.make_initializable_iterator()\n",
    "        return iterator\n",
    "\n",
    "\n",
    "    train_phase = tf.placeholder(dtype=tf.bool)\n",
    "    tfrecord_path = tf.placeholder(dtype=tf.string)\n",
    "    compression_type = tf.placeholder(dtype=tf.string)\n",
    "\n",
    "    train_iterator = _get_train_input()\n",
    "    valid_iterator = _get_valid_input()\n",
    "\n",
    "    def f_true():\n",
    "        result = train_iterator.get_next()\n",
    "        return result\n",
    "    def f_false():\n",
    "        result = valid_iterator.get_next()\n",
    "        return result\n",
    "\n",
    "    inp_list = tf.cond(train_phase,f_true,f_false)\n",
    "\n",
    "with tf.Session(graph=a_graph) as sess:\n",
    "    feed_dict = {train_phase:True,tfrecord_path:tfrecord_file,compression_type:\"GZIP\"}\n",
    "    sess.run(inp_iterator.initializer,feed_dict)\n",
    "    sess.run(inp_list,feed_dict)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 测试 | 两个稀疏向量的操作"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-19T01:55:57.054984Z",
     "start_time": "2018-10-19T01:55:56.986541Z"
    }
   },
   "outputs": [],
   "source": [
    "sess = tf.Session()\n",
    "\n",
    "a = tf.SparseTensor(indices=[[0,1],[0,2]], values=[1,1], dense_shape=[5,3])\n",
    "b = tf.SparseTensor(indices=[[1,5],[1,7],[1,8],[1,9]], values=[2,2,2,2], dense_shape=[5,7])\n",
    "indices=b.indices\n",
    "userindices = tf.where(tf.equal(indices[:, 0], 1))\n",
    "print(\"a:\")\n",
    "sess.run(a)\n",
    "print(\"b:\")\n",
    "sess.run(b)\n",
    "print(\"concat:\")\n",
    "sess.run(tf.sparse_concat(axis=1,sp_inputs=[a,b]))\n",
    "\n",
    "# sess.run(tf.sparse_add(a,b)) # shape不同不能add\n",
    "\n",
    "\n",
    "\n",
    "# a_add_b = tf.sparse_add(a,b)\n",
    "# indices = a_add_b.indices\n",
    "# values = a_add_b.values\n",
    "# count_list = tf.unique_with_counts(indices[:, 0]).count\n",
    "# count_list_ = tf.concat([[0],count_list],axis=0)[:-1]\n",
    "# count_result = tf.transpose(tf.stack([count_list_,count_list]))\n",
    "# print(\"count:\")\n",
    "# sess.run(count_result)\n",
    "# print(\"result:\")\n",
    "# sess.run(tf.map_fn(lambda x: values[x[0]:x[1]], count_result))\n",
    "# print(\"userindices\")\n",
    "# sess.run(userindices)\n",
    "# sess.run(tf.gather(indices,userindices))\n",
    "# print(\"gather value\")\n",
    "# sess.run(tf.gather(values,userindices))\n",
    "# sess.run(userindices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 测试 | 构造行号 | tf.tile或者pyfunc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-19T01:55:57.178849Z",
     "start_time": "2018-10-19T01:55:57.074850Z"
    },
    "code_folding": [
     5
    ]
   },
   "outputs": [],
   "source": [
    "sess = tf.Session()\n",
    "ff = tf.constant([[833,119,8922,333],[837,119,8999,330]])\n",
    "\n",
    "print(\"ff:\")\n",
    "sess.run(ff)\n",
    "\n",
    "############\n",
    "# tf.tile\n",
    "\n",
    "idx = tf.range(tf.shape(ff)[0])\n",
    "idx_2d = tf.Variable(tf.reshape(idx,[-1,1]))\n",
    "idx_2d_full = tf.tile(idx_2d,[1,tf.shape(ff)[1]])\n",
    "result = tf.concat([tf.reshape(idx_2d_full,[-1,1]),tf.reshape(ff,[-1,1])],axis=1)\n",
    "sess.run(tf.global_variables_initializer())\n",
    "print(\"idx:\")\n",
    "sess.run(idx_2d)\n",
    "sess.run(idx_2d_full)\n",
    "print(\"tf.tile result:\")\n",
    "sess.run(result)\n",
    "\n",
    "############\n",
    "# pyfunc\n",
    "def add_idx(arr):\n",
    "    result = []\n",
    "    for i in range(len(arr)):\n",
    "        for j in arr[i]:\n",
    "            result.append([i, j])\n",
    "    return np.array(result)\n",
    "result_1 = tf.py_func(func=add_idx, inp=[ff],Tout=[tf.int64])\n",
    "print(\"py_func_result1:\")\n",
    "sess.run(result_1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-22T02:05:49.832789Z",
     "start_time": "2018-10-22T02:05:49.817074Z"
    }
   },
   "outputs": [],
   "source": [
    "import functools\n",
    "\n",
    "def wrap_func(ori_func,new_func):\n",
    "    @functools.wraps(ori_func)\n",
    "    def run(*args, **kwargs):\n",
    "        return new_func(ori_func, *args, **kwargs)\n",
    "    return run\n",
    "def replaced_print(ori_function, parameter):\n",
    "    now = time.strftime(\"%Y-%m-%d %H:%M\", time.localtime(time.time()))\n",
    "    new_params = now+\": \"+parameter\n",
    "    return ori_function(new_params)\n",
    "# old_print = print\n",
    "print=wrap_func(print,replaced_print)\n",
    "print(\"print方法已经被hook,会自动输出时间前缀\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-01T05:35:40.224017Z",
     "start_time": "2018-11-01T05:35:40.198909Z"
    }
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tf2onnx'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-65-d72f3345319c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtf2onnx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplaceholder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"input\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mx_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0midentity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"output\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tf2onnx'"
     ]
    }
   ],
   "source": [
    "import tf2onnx\n",
    "with tf.Session() as sess:\n",
    "    x = tf.placeholder(tf.float32, [2, 3], name=\"input\")\n",
    "    x_ = tf.add(x, x)\n",
    "    _ = tf.identity(x_, name=\"output\")\n",
    "    onnx_graph = tf2onnx.tfonnx.process_tf_graph(sess.graph)\n",
    "    model_proto = onnx_graph.make_model(\"test\", [\"input:0\"], [\"output:0\"])\n",
    "    with open(\"/home/zhoutong/tmp/model.onnx\", \"wb\") as f:\n",
    "        f.write(model_proto.SerializeToString())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
