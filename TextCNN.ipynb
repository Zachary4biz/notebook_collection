{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-26T09:31:23.718884Z",
     "start_time": "2019-08-26T09:31:23.548202Z"
    }
   },
   "outputs": [],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "%matplotlib inline\n",
    "from tqdm.auto import tqdm\n",
    "import concurrent.futures\n",
    "from multiprocessing import Pool\n",
    "import copy,os,sys,psutil\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-26T10:58:44.988550Z",
     "start_time": "2019-08-26T10:58:44.984903Z"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import datetime\n",
    "import gensim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from zac_pyutils.ExqUtils import zprint\n",
    "from zac_pyutils import ExqUtils\n",
    "from collections import deque\n",
    "from tqdm.auto import tqdm\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "# from gensim.models.wrappers import FastText\n",
    "import fasttext\n",
    "import json\n",
    "import re\n",
    "import inspect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-26T12:19:06.009760Z",
     "start_time": "2019-08-26T12:19:05.999935Z"
    },
    "code_folding": [
     0,
     9,
     16,
     45
    ]
   },
   "outputs": [],
   "source": [
    "class TrainingConfig(object):\n",
    "    epoches = 5\n",
    "    batchSize = 128\n",
    "    evaluateEvery = 100\n",
    "    checkpointEvery = 100\n",
    "    learningRate = 0.001\n",
    "    minWordCnt = 5\n",
    "\n",
    "\n",
    "class ModelConfig(object):\n",
    "    numFilters = 64\n",
    "\n",
    "    filterSizes = [2, 3, 4, 5]\n",
    "    dropoutKeepProb = 0.5\n",
    "    l2RegLambda = 0.001\n",
    "\n",
    "class Config(object):\n",
    "    job = \"taste\"\n",
    "    basePath = \"/home/zhoutong/nlp/data/textcnn\"\n",
    "    dataSource = basePath + \"/labeled_timeliness_region_taste_emotion_sample.json\"\n",
    "    # dataSource = dataSource + \".sample_h10k\"\n",
    "    summaryDir = basePath+\"/summary\"\n",
    "    cnnmodelPath_pb = basePath + \"/textcnn_model_pb\"\n",
    "    cnnmodelPath_ckpt = basePath+\"/textcnn_model_ckpt/model.ckpt\"\n",
    "\n",
    "    weDim = 300\n",
    "    ft_modelPath = basePath + '/cc.en.300.bin'\n",
    "\n",
    "\n",
    "    padSize = 16\n",
    "    pad = '<PAD>'\n",
    "    pad_initV = np.zeros(weDim)\n",
    "    unk = '<UNK>'\n",
    "    unk_initV = np.random.randn(weDim)\n",
    "\n",
    "    # numClasses = 4  # 二分类设置为1，多分类设置为类别的数目\n",
    "    numClasses_dict = {\"taste\":4,\"timeliness\":9,\"emotion\":3}\n",
    "    numClasses = numClasses_dict[job]  # 二分类设置为1，多分类设置为类别的数目\n",
    "\n",
    "    testRatio = 0.2  # 测试集的比例\n",
    "\n",
    "    training = TrainingConfig()\n",
    "\n",
    "    model = ModelConfig()\n",
    "\n",
    "class Utils():\n",
    "    # 清理符号\n",
    "    @staticmethod\n",
    "    def clean_punctuation(inp_text):\n",
    "        res = re.sub(r\"[~!@#$%^&*()_+-={\\}|\\[\\]:\\\";'<>?,./“”]\", r' ', inp_text)\n",
    "        res = re.sub(r\"\\\\u200[Bb]\", r' ', res)\n",
    "        res = re.sub(r\"\\n+\", r\" \", res)\n",
    "        res = re.sub(r\"\\s+\", \" \", res)\n",
    "        return res.strip()\n",
    "    @staticmethod\n",
    "    def pad_list(inp_list,width,pad_const):\n",
    "        if len(inp_list) >= width:\n",
    "            return inp_list[:width]\n",
    "        else:\n",
    "            return inp_list+[pad_const]*(width-len(inp_list))\n",
    "    \n",
    "config = Config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-26T12:25:27.483690Z",
     "start_time": "2019-08-26T12:25:27.475979Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8000"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "527480"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "128"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "4120"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.trainReviews.shape[0]\n",
    "527480\n",
    "config.training.batchSize\n",
    "527480//config.training.batchSize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-27T12:56:16.467488Z",
     "start_time": "2019-08-27T12:56:13.226077Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N\n"
     ]
    }
   ],
   "source": [
    "# fail\n",
    "import timeout_decorator\n",
    "@timeout_decorator.timeout(seconds=3, use_signals=False, exception_message=\"timeout\")\n",
    "def get_input(res_inpt):\n",
    "    res_inpt=input(\"是否重新生成persist数据？(y/n)\")\n",
    "\n",
    "res = None\n",
    "try:\n",
    "    get_input(res)\n",
    "except Exception as e:\n",
    "    res = \"N\"\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-27T12:57:18.589594Z",
     "start_time": "2019-08-27T12:47:48.417Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# fail\n",
    "class StoppableThread(Thread):\n",
    "    def __init__(self,target,args,timeout=1):\n",
    "        super(Thread,self).__init__()\n",
    "        self.timeout = timeout\n",
    "        self.stopped = False\n",
    "        self.t = Thread(target=target,args=args)\n",
    "        self.t.setDaemon(True)\n",
    "\n",
    "    def start(self):\n",
    "        self.t.start()\n",
    "        while not self.stopped:\n",
    "            self.t.join(self.timeout)\n",
    "        print(\"thread stopped.\")\n",
    "    \n",
    "    def stop(self):\n",
    "        self.stopped=True\n",
    "\n",
    "res = \"n\"\n",
    "t1 = StoppableThread(target=get_input,args=(res,))\n",
    "t1.start()\n",
    "time.sleep(3)\n",
    "t1.terminate()\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StoppableThread(Thread):\n",
    "    def __init__(self,target,args,timeout=1):\n",
    "        super(Thread,self).__init__()\n",
    "        self.timeout = timeout\n",
    "        self.stopped = False\n",
    "        self.t = Thread(target=target,args=args)\n",
    "        self.t.setDaemon(True)\n",
    "\n",
    "    def start(self):\n",
    "        self.t.start()\n",
    "        while not self.stopped:\n",
    "            self.t.join(self.timeout)\n",
    "        print(\"thread stopped.\")\n",
    "    \n",
    "    def stop(self):\n",
    "        self.stopped=True\n",
    "\n",
    "\n",
    "def func():\n",
    "    pass\n",
    "    \n",
    "    \n",
    "t1 = StoppableThread(target=func)\n",
    "t1.start()\n",
    "for i in range(5):\n",
    "    time.sleep(1)\n",
    "    print(i)\n",
    "t1.terminate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-27T13:21:17.781221Z",
     "start_time": "2019-08-27T13:21:17.778795Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "abc\n"
     ]
    }
   ],
   "source": [
    "print(\"abc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-27T13:35:37.496106Z",
     "start_time": "2019-08-27T13:35:36.483934Z"
    },
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input:  2\n",
      "0\n",
      "thread stopped.\n",
      "to_inp:  2\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "deque index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-ea8f4a97c338>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0mt1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"to_inp: \"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mto_inp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"result in queue:\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mqueue\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m: deque index out of range"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "processed:  3\n"
     ]
    }
   ],
   "source": [
    "from threading import Thread\n",
    "import time\n",
    "import queue\n",
    "\n",
    "class StoppableThread(Thread):\n",
    "    class TimeoutException(Exception):\n",
    "        pass\n",
    "    \n",
    "    def __init__(self,target,args=None,time_limit=1,delta=0.05):\n",
    "        super(Thread,self).__init__()\n",
    "        self.delta = delta\n",
    "        self.stopped = False\n",
    "        if args is None:\n",
    "            self.t = Thread(target=target)\n",
    "        else:\n",
    "            self.t = Thread(target=target,args=args)\n",
    "        self.t.setDaemon(True)\n",
    "        \n",
    "        self.timing_thread = Thread(target=self.timing,args=(time_limit,))\n",
    "        self.timing_thread.setDaemon(True)\n",
    "    \n",
    "    def timing(self,timeout):\n",
    "        time.sleep(timeout)\n",
    "        self.stopped=True\n",
    "        \n",
    "    def start(self):\n",
    "        self.t.start()\n",
    "        self.timing_thread.start()\n",
    "        while not self.stopped:\n",
    "            self.t.join(self.delta)\n",
    "            time.sleep(0.05)\n",
    "        raise TimeoutException(\"thread timeout\")\n",
    "\n",
    "    \n",
    "    def stop(self):\n",
    "        self.stopped=True\n",
    "\n",
    "\n",
    "q = queue.Queue()\n",
    "def func(inp):\n",
    "    print(\"input: \",inp)\n",
    "    for i in range(3):\n",
    "        time.sleep(1)\n",
    "        print(i)\n",
    "    inp += 1\n",
    "    print(\"processed: \",inp)\n",
    "    q.put(inp)\n",
    "\n",
    "to_inp = 2\n",
    "t1 = StoppableThread(target=func,args=(to_inp,), time_limit=1)\n",
    "t1.start()\n",
    "print(\"to_inp: \",to_inp)\n",
    "print(\"result in queue:\",q.queue[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-27T14:13:51.290554Z",
     "start_time": "2019-08-27T14:13:51.287178Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(q.queue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-27T13:36:07.352263Z",
     "start_time": "2019-08-27T13:36:07.348271Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "result in queue: 3\n"
     ]
    }
   ],
   "source": [
    "print(\"result in queue:\",q.queue[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-27T13:25:15.004502Z",
     "start_time": "2019-08-27T13:25:14.999739Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['__class__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " '_get',\n",
       " '_init',\n",
       " '_put',\n",
       " '_qsize',\n",
       " 'all_tasks_done',\n",
       " 'empty',\n",
       " 'full',\n",
       " 'get',\n",
       " 'get_nowait',\n",
       " 'join',\n",
       " 'maxsize',\n",
       " 'mutex',\n",
       " 'not_empty',\n",
       " 'not_full',\n",
       " 'put',\n",
       " 'put_nowait',\n",
       " 'qsize',\n",
       " 'queue',\n",
       " 'task_done',\n",
       " 'unfinished_tasks']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(q.queue[0])\n",
    "dir(q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-26T11:01:41.634029Z",
     "start_time": "2019-08-26T11:01:41.628153Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|2019-08-26 19:01:41| 各数据路径：\n",
      "basePath路径：/home/zhoutong/nlp/data/textcnn\n",
      "样本数据来源: /home/zhoutong/nlp/data/textcnn/labeled_timeliness_region_taste_emotion_sample.json.sample_h10k\n",
      "summary目录：/home/zhoutong/nlp/data/textcnn/summary\n",
      "\n",
      "|2019-08-26 19:01:41| 模型参数如下：\n",
      "dropoutKeepProb 0.5\n",
      "filterSizes [2, 3, 4, 5]\n",
      "l2RegLambda 0.0\n",
      "numFilters 64\n",
      "|2019-08-26 19:01:41| 训练参数如下：\n",
      "checkpointEvery 100\n",
      "epoches 5\n",
      "evaluateEvery 100\n",
      "learningRate 0.001\n",
      "minWordCnt 5\n"
     ]
    }
   ],
   "source": [
    "zprint(\"各数据路径：\")\n",
    "print(\"basePath路径：{}\\n样本数据来源: {}\\nsummary目录：{}\\n\".format(config.basePath,config.dataSource,config.summaryDir))\n",
    "zprint(\"模型参数如下：\")\n",
    "for k,v in inspect.getmembers(config.model):\n",
    "    if not k.startswith(\"_\"):\n",
    "        print(k,v)\n",
    "zprint(\"训练参数如下：\")\n",
    "for k,v in inspect.getmembers(config.training):\n",
    "    if not k.startswith(\"_\"):\n",
    "        print(k,v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TextCNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-26T09:31:15.729419Z",
     "start_time": "2019-08-26T09:31:15.709725Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "class TextCNN(object):\n",
    "    \"\"\"\n",
    "    Text CNN 用于文本分类\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config, wordEmbedding):\n",
    "\n",
    "        # 定义模型的输入\n",
    "        self.inputX = tf.placeholder(tf.int32, [None, config.padSize], name=\"inputX\")\n",
    "        self.inputY = tf.placeholder(tf.int32, [None], name=\"inputY\")\n",
    "\n",
    "        self.dropoutKeepProb = tf.placeholder(tf.float32, name=\"dropoutKeepProb\")\n",
    "\n",
    "        # 定义l2损失\n",
    "        l2Loss = tf.constant(0.0)\n",
    "\n",
    "        # 词嵌入层\n",
    "        with tf.name_scope(\"embedding\"):\n",
    "            # 利用预训练的词向量初始化词嵌入矩阵\n",
    "            self.W = tf.Variable(tf.cast(wordEmbedding, dtype=tf.float32, name=\"word2vec\"), name=\"W\")\n",
    "            # 利用词嵌入矩阵将输入的数据中的词转换成词向量，维度[batch_size, sequence_length, embedding_size]\n",
    "            self.embeddedWords = tf.nn.embedding_lookup(self.W, self.inputX)\n",
    "            # 卷积的输入是思维[batch_size, width, height, channel]，因此需要增加维度，用tf.expand_dims来增大维度\n",
    "            self.embeddedWordsExpanded = tf.expand_dims(self.embeddedWords, -1)\n",
    "\n",
    "        # 创建卷积和池化层\n",
    "        pooledOutputs = []\n",
    "        # 有三种size的filter，2, 3， 4， 5，textCNN是个多通道单层卷积的模型，可以看作三个单层的卷积模型的融合\n",
    "        for i, filterSize in enumerate(config.model.filterSizes):\n",
    "            with tf.name_scope(\"conv-maxpool-%s\" % filterSize):\n",
    "                # 卷积层，卷积核尺寸为filterSize * embeddingSize，卷积核的个数为numFilters\n",
    "                # 初始化权重矩阵和偏置\n",
    "                # 4-D (height,width,in_channels,out_channels) | out_channels是多少就用多少个 Height*Width 的卷积核\n",
    "                filterShape = [filterSize, config.weDim, 1, config.model.numFilters]\n",
    "                W = tf.Variable(tf.truncated_normal(filterShape, stddev=0.1), name=\"W\")\n",
    "                b = tf.Variable(tf.constant(0.1, shape=[config.model.numFilters]), name=\"b\")\n",
    "                convRes = tf.nn.conv2d(\n",
    "                    input=self.embeddedWordsExpanded,\n",
    "                    filter=W,\n",
    "                    strides=[1, 1, 1, 1],\n",
    "                    padding=\"VALID\",\n",
    "                    name=\"conv\")\n",
    "\n",
    "                # relu函数的非线性映射\n",
    "                h = tf.nn.relu(tf.nn.bias_add(convRes, b), name=\"relu\")\n",
    "\n",
    "                # 池化层，最大池化，池化是对卷积后的序列取一个最大值\n",
    "                pooled = tf.nn.max_pool(\n",
    "                    h,\n",
    "                    ksize=[1, config.padSize - filterSize + 1, 1, 1],\n",
    "                    # ksize shape: [batch, height, width, channels]\n",
    "                    strides=[1, 1, 1, 1],\n",
    "                    padding='VALID',\n",
    "                    name=\"pool\")\n",
    "                pooledOutputs.append(pooled)  # 将三种size的filter的输出一起加入到列表中\n",
    "\n",
    "        # 得到CNN网络的输出长度\n",
    "        numFiltersTotal = config.model.numFilters * len(config.model.filterSizes)\n",
    "\n",
    "        # 池化后的维度不变，按照最后的维度channel来concat\n",
    "        self.hPool = tf.concat(pooledOutputs, 3)\n",
    "\n",
    "        # 摊平成二维的数据输入到全连接层\n",
    "        self.hPoolFlat = tf.reshape(self.hPool, [-1, numFiltersTotal])\n",
    "\n",
    "        # dropout\n",
    "        with tf.name_scope(\"dropout\"):\n",
    "            self.hDrop = tf.nn.dropout(self.hPoolFlat, self.dropoutKeepProb)\n",
    "\n",
    "        # 全连接层的输出\n",
    "        with tf.name_scope(\"output\"):\n",
    "            outputW = tf.get_variable(\n",
    "                \"outputW\",\n",
    "                shape=[numFiltersTotal, config.numClasses],\n",
    "                initializer=tf.contrib.layers.xavier_initializer())\n",
    "            outputB = tf.Variable(tf.constant(0.1, shape=[config.numClasses]), name=\"outputB\")\n",
    "            l2Loss += tf.nn.l2_loss(outputW)\n",
    "            l2Loss += tf.nn.l2_loss(outputB)\n",
    "            self.logits = tf.nn.xw_plus_b(self.hDrop, outputW, outputB, name=\"logits\")\n",
    "            if config.numClasses == 1:\n",
    "                self.predictions = tf.cast(tf.greater_equal(self.logits, 0.0), tf.int32, name=\"predictions\")\n",
    "            elif config.numClasses > 1:\n",
    "                self.predictions = tf.argmax(self.logits, axis=-1, name=\"predictions\")\n",
    "\n",
    "            print(self.predictions)\n",
    "\n",
    "        # 计算二元交叉熵损失\n",
    "        with tf.name_scope(\"loss\"):\n",
    "\n",
    "            if config.numClasses == 1:\n",
    "                losses = tf.nn.sigmoid_cross_entropy_with_logits(logits=self.logits,\n",
    "                                                                 labels=tf.cast(tf.reshape(self.inputY, [-1, 1]),\n",
    "                                                                                dtype=tf.float32))\n",
    "            elif config.numClasses > 1:\n",
    "                losses = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=self.logits, labels=self.inputY)\n",
    "\n",
    "            self.loss = tf.reduce_mean(losses) + config.model.l2RegLambda * l2Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-26T09:31:15.772483Z",
     "start_time": "2019-08-26T09:31:15.755476Z"
    },
    "code_folding": [
     0,
     5,
     15,
     32,
     52,
     72,
     90,
     106,
     122,
     139,
     154
    ]
   },
   "outputs": [],
   "source": [
    "class MetricUtils():\n",
    "    \"\"\"\n",
    "    定义各类性能指标\n",
    "    \"\"\"\n",
    "    @staticmethod\n",
    "    def mean(item: list) -> float:\n",
    "        \"\"\"\n",
    "        计算列表中元素的平均值\n",
    "        :param item: 列表对象\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        res = sum(item) / len(item) if len(item) > 0 else 0\n",
    "        return res\n",
    "\n",
    "    @staticmethod\n",
    "    def accuracy(pred_y, true_y):\n",
    "        \"\"\"\n",
    "        计算二类和多类的准确率\n",
    "        :param pred_y: 预测结果\n",
    "        :param true_y: 真实结果\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        if isinstance(pred_y[0], list):\n",
    "            pred_y = [item[0] for item in pred_y]\n",
    "        corr = 0\n",
    "        for i in range(len(pred_y)):\n",
    "            if pred_y[i] == true_y[i]:\n",
    "                corr += 1\n",
    "        acc = corr / len(pred_y) if len(pred_y) > 0 else 0\n",
    "        return acc\n",
    "\n",
    "    @staticmethod\n",
    "    def binary_precision(pred_y, true_y, positive=1):\n",
    "        \"\"\"\n",
    "        二类的精确率计算\n",
    "        :param pred_y: 预测结果\n",
    "        :param true_y: 真实结果\n",
    "        :param positive: 正例的索引表示\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        corr = 0\n",
    "        pred_corr = 0\n",
    "        for i in range(len(pred_y)):\n",
    "            if pred_y[i] == positive:\n",
    "                pred_corr += 1\n",
    "                if pred_y[i] == true_y[i]:\n",
    "                    corr += 1\n",
    "\n",
    "        prec = corr / pred_corr if pred_corr > 0 else 0\n",
    "        return prec\n",
    "\n",
    "    @staticmethod\n",
    "    def binary_recall(pred_y, true_y, positive=1):\n",
    "        \"\"\"\n",
    "        二类的召回率\n",
    "        :param pred_y: 预测结果\n",
    "        :param true_y: 真实结果\n",
    "        :param positive: 正例的索引表示\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        corr = 0\n",
    "        true_corr = 0\n",
    "        for i in range(len(pred_y)):\n",
    "            if true_y[i] == positive:\n",
    "                true_corr += 1\n",
    "                if pred_y[i] == true_y[i]:\n",
    "                    corr += 1\n",
    "\n",
    "        rec = corr / true_corr if true_corr > 0 else 0\n",
    "        return rec\n",
    "\n",
    "    @staticmethod\n",
    "    def binary_f_beta(pred_y, true_y, beta=1.0, positive=1):\n",
    "        \"\"\"\n",
    "        二类的f beta值\n",
    "        :param pred_y: 预测结果\n",
    "        :param true_y: 真实结果\n",
    "        :param beta: beta值\n",
    "        :param positive: 正例的索引表示\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        precision = MetricUtils.binary_precision(pred_y, true_y, positive)\n",
    "        recall = MetricUtils.binary_recall(pred_y, true_y, positive)\n",
    "        try:\n",
    "            f_b = (1 + beta * beta) * precision * recall / (beta * beta * precision + recall)\n",
    "        except:\n",
    "            f_b = 0\n",
    "        return f_b\n",
    "\n",
    "    @staticmethod\n",
    "    def multi_precision(pred_y, true_y, labels):\n",
    "        \"\"\"\n",
    "        多类的精确率\n",
    "        :param pred_y: 预测结果\n",
    "        :param true_y: 真实结果\n",
    "        :param labels: 标签列表\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        if isinstance(pred_y[0], list):\n",
    "            pred_y = [item[0] for item in pred_y]\n",
    "\n",
    "        precisions = [MetricUtils.binary_precision(pred_y, true_y, label) for label in labels]\n",
    "        prec = MetricUtils.mean(precisions)\n",
    "        return prec\n",
    "\n",
    "    @staticmethod\n",
    "    def multi_recall(pred_y, true_y, labels):\n",
    "        \"\"\"\n",
    "        多类的召回率\n",
    "        :param pred_y: 预测结果\n",
    "        :param true_y: 真实结果\n",
    "        :param labels: 标签列表\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        if isinstance(pred_y[0], list):\n",
    "            pred_y = [item[0] for item in pred_y]\n",
    "\n",
    "        recalls = [MetricUtils.binary_recall(pred_y, true_y, label) for label in labels]\n",
    "        rec = MetricUtils.mean(recalls)\n",
    "        return rec\n",
    "\n",
    "    @staticmethod\n",
    "    def multi_f_beta(pred_y, true_y, labels, beta=1.0):\n",
    "        \"\"\"\n",
    "        多类的f beta值\n",
    "        :param pred_y: 预测结果\n",
    "        :param true_y: 真实结果\n",
    "        :param labels: 标签列表\n",
    "        :param beta: beta值\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        if isinstance(pred_y[0], list):\n",
    "            pred_y = [item[0] for item in pred_y]\n",
    "\n",
    "        f_betas = [MetricUtils.binary_f_beta(pred_y, true_y, beta, label) for label in labels]\n",
    "        f_beta = MetricUtils.mean(f_betas)\n",
    "        return f_beta\n",
    "\n",
    "    @staticmethod\n",
    "    def get_binary_metrics(pred_y, true_y, f_beta=1.0):\n",
    "        \"\"\"\n",
    "        得到二分类的性能指标\n",
    "        :param pred_y:\n",
    "        :param true_y:\n",
    "        :param f_beta:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        acc = MetricUtils.accuracy(pred_y, true_y)\n",
    "        recall = MetricUtils.binary_recall(pred_y, true_y)\n",
    "        precision = MetricUtils.binary_precision(pred_y, true_y)\n",
    "        f_beta = MetricUtils.binary_f_beta(pred_y, true_y, f_beta)\n",
    "        return acc, recall, precision, f_beta\n",
    "\n",
    "    @staticmethod\n",
    "    def get_multi_metrics(pred_y, true_y, labels, f_beta=1.0):\n",
    "        \"\"\"\n",
    "        得到多分类的性能指标\n",
    "        :param pred_y:\n",
    "        :param true_y:\n",
    "        :param labels:\n",
    "        :param f_beta:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        acc = MetricUtils.accuracy(pred_y, true_y)\n",
    "        recall = MetricUtils.multi_recall(pred_y, true_y, labels)\n",
    "        precision = MetricUtils.multi_precision(pred_y, true_y, labels)\n",
    "        f_beta = MetricUtils.multi_f_beta(pred_y, true_y, labels, f_beta)\n",
    "        return acc, recall, precision, f_beta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Class Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-26T09:31:15.880769Z",
     "start_time": "2019-08-26T09:31:15.829831Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "class Dataset(object):\n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "        self._dataSource = config.dataSource\n",
    "\n",
    "        self.testRatio = config.testRatio\n",
    "        self._we_fp = config.basePath+\"/wordEmbeddingInfo\"  # \\t分割 词,idx,embedding\n",
    "        self._tokens_arr_fp = config.basePath+\"/tokens_arr.npy\"\n",
    "        self._labels_arr_fp = config.basePath+\"/labels_arr.npy\"\n",
    "        self._emb_arr_fp = config.basePath+\"/emb_arr.npy\"\n",
    "        self.ft_modelPath = config.ft_modelPath\n",
    "        self.ft_model = None\n",
    "\n",
    "\n",
    "        self.trainReviews = np.array([])\n",
    "        self.trainLabels = np.array([])\n",
    "\n",
    "        self.evalReviews = np.array([])\n",
    "        self.evalLabels = np.array([])\n",
    "\n",
    "        self.token2idx = {}\n",
    "        self.wordEmbedding = None\n",
    "        self.labelSet = []\n",
    "        self.totalWordCnt = 0\n",
    "\n",
    "    def _readData(self, filePath):\n",
    "        f_iter = ExqUtils.load_file_as_iter(filePath)\n",
    "        tokens_list = deque()\n",
    "        label_list = deque()\n",
    "        zprint(\"loading data from: \"+filePath)\n",
    "        for l in tqdm(f_iter,desc=\"readData\"):\n",
    "            info = json.loads(l)\n",
    "            text,label = info['title'],info[self.config.job]\n",
    "            tokens = Utils.pad_list(Utils.clean_punctuation(text).split(\" \"),width=self.config.padSize,pad_const=self.config.pad)\n",
    "            tokens_list.append(tokens)\n",
    "            label_list.append(label)\n",
    "        return np.array(tokens_list), np.array(label_list)\n",
    "\n",
    "    def _initStopWord(self, stopWordPath):\n",
    "        with open(stopWordPath, \"r\") as fr:\n",
    "            self._stopWordSet = set(fr.read().splitlines())\n",
    "\n",
    "    def _initFasttextModel(self):\n",
    "        if self.ft_model is None:\n",
    "            self.ft_model = fasttext.load_model(self.ft_modelPath)\n",
    "\n",
    "    def _tokens2idx(self,tokens_arr):\n",
    "        tokensSet = set(np.unique(tokens_arr))\n",
    "\n",
    "        pass\n",
    "\n",
    "    def dataGen_persist(self):\n",
    "        \"\"\"\n",
    "                初始化训练集和验证集\n",
    "                \"\"\"\n",
    "        zprint(\"init fasttext model\")\n",
    "        self._initFasttextModel()\n",
    "\n",
    "        # 初始化数据集\n",
    "        tokens_arr, label_arr = self._readData(self._dataSource)\n",
    "        self.labelSet = set(np.unique(label_arr))\n",
    "        tokensSet = set(np.unique(tokens_arr))\n",
    "\n",
    "        self.totalWordCnt = len(tokensSet)\n",
    "        wordEmb = np.zeros(shape=[self.totalWordCnt, self.ft_model.get_dimension()])\n",
    "        # (idx,token,emb)保存到文本文件\n",
    "        zprint(\"预测词向量总计: {} , 词向量存入文件: {}\".format(self.totalWordCnt, self._we_fp))\n",
    "        with open(self._we_fp, \"w\") as fw:\n",
    "            # 加上 <PAD> 和 <UNK> 及其初始化\n",
    "            for idx, token in tqdm(enumerate(tokensSet), total=self.totalWordCnt,desc=\"tokensSet\"):\n",
    "                if token == self.config.pad:\n",
    "                    emb = self.config.pad_initV\n",
    "                elif token == self.config.unk:\n",
    "                    emb = self.config.unk_initV\n",
    "                else:\n",
    "                    emb = self.ft_model[token]\n",
    "                self.token2idx.update({token:idx})\n",
    "                wordEmb[idx] = emb\n",
    "                fw.writelines(str(idx) + \"\\t\" + token + \"\\t\" + \",\".join([str(i) for i in list(emb)]) + \"\\n\")\n",
    "\n",
    "        # tokens变为idx保存为npy\n",
    "        zprint(\"tokens映射为索引保存到npy\")\n",
    "        tokensIdx_arr = np.zeros_like(tokens_arr, dtype=np.int64)\n",
    "        for i,tokens in enumerate(tokens_arr):\n",
    "            for j,token in enumerate(tokens):\n",
    "                tokensIdx_arr[i][j] = self.token2idx[token]\n",
    "        np.save(self._tokens_arr_fp,tokensIdx_arr)\n",
    "\n",
    "        zprint(\"labels保存到npy\")\n",
    "        np.save(self._labels_arr_fp,label_arr)\n",
    "\n",
    "        zprint(\"idx对应的emb保存到npy\")\n",
    "        np.save(self._emb_arr_fp,wordEmb)\n",
    "\n",
    "    def loadData(self):\n",
    "        self.wordEmbedding = np.load(self._emb_arr_fp)\n",
    "\n",
    "        tokensIdx_arr = np.load(self._tokens_arr_fp)\n",
    "        label_arr = np.load(self._labels_arr_fp)\n",
    "        self.labelSet = set(np.unique(label_arr))\n",
    "        # 初始化训练集和测试集\n",
    "        sss = StratifiedShuffleSplit(n_splits=1, test_size=self.testRatio, random_state=2019)\n",
    "        train_idx, test_idx = list(sss.split(np.zeros(label_arr.shape[0]), label_arr))[0]\n",
    "\n",
    "        self.trainReviews = tokensIdx_arr[train_idx]\n",
    "        self.trainLabels = label_arr[train_idx]\n",
    "\n",
    "        self.evalReviews = tokensIdx_arr[test_idx]\n",
    "        self.evalLabels = label_arr[test_idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Produce Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-26T09:31:30.301109Z",
     "start_time": "2019-08-26T09:31:30.223973Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train data shape: (8000, 16)\n",
      "train label shape: (8000,)\n",
      "eval data shape: (2000, 16)\n",
      "eval data shape: (2000,)\n",
      "wordEmbedding info file: /home/zhoutong/nlp/data/textcnn/wordEmbeddingInfo\n"
     ]
    }
   ],
   "source": [
    "data = Dataset(config)\n",
    "# data.dataGen_persist()\n",
    "data.loadData()\n",
    "print(\"train data shape: {}\".format(data.trainReviews.shape))\n",
    "print(\"train label shape: {}\".format(data.trainLabels.shape))\n",
    "print(\"eval data shape: {}\".format(data.evalReviews.shape))\n",
    "print(\"eval data shape: {}\".format(data.evalLabels.shape))\n",
    "print(\"wordEmbedding info file: {}\".format(data._we_fp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-26T09:31:30.882681Z",
     "start_time": "2019-08-26T09:31:30.873443Z"
    }
   },
   "outputs": [],
   "source": [
    "trainReviews = data.trainReviews\n",
    "trainLabels = data.trainLabels\n",
    "evalReviews = data.evalReviews\n",
    "evalLabels = data.evalLabels\n",
    "\n",
    "wordEmbedding = data.wordEmbedding\n",
    "labelList = data.labelSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-26T09:31:31.465916Z",
     "start_time": "2019-08-26T09:31:31.460943Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 2050, 10847, 19363, ...,  4047, 16429,  5742],\n",
       "       [12456,  6102, 10647, ...,  5742,  5742,  5742],\n",
       "       [19116,  3220,  6635, ..., 17476, 20345, 19071],\n",
       "       ...,\n",
       "       [19409,  1934, 13300, ...,  5742,  5742,  5742],\n",
       "       [12130,  2611,  3892, ...,  5742,  5742,  5742],\n",
       "       [12918, 14681, 14452, ...,  5742,  5742,  5742]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "array([1, 0, 1, ..., 1, 1, 2])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainReviews\n",
    "trainLabels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 开始构建计算图"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-26T06:37:50.008427Z",
     "start_time": "2019-08-26T06:37:50.002752Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2000,)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(90,)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evalLabels.shape\n",
    "batchY = np.array(evalLabels[10: 100], dtype=\"float32\")\n",
    "batchY.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-26T09:32:02.382434Z",
     "start_time": "2019-08-26T09:32:02.374555Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def nextBatch(x, y, batchSize):\n",
    "    \"\"\"\n",
    "    生成batch数据集，用生成器的方式输出\n",
    "    \"\"\"\n",
    "    perm = np.arange(len(x))\n",
    "    np.random.shuffle(perm)\n",
    "    x,y = x[perm],y[perm]\n",
    "\n",
    "    numBatches = len(x) // batchSize\n",
    "\n",
    "    for i in range(numBatches):\n",
    "        start = i * batchSize\n",
    "        end = start + batchSize\n",
    "        batchX = np.array(x[start: end], dtype=\"int64\")\n",
    "        batchY = np.array(y[start: end], dtype=\"float32\")\n",
    "\n",
    "        yield batchX, batchY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-26T07:40:19.208395Z",
     "start_time": "2019-08-26T07:40:19.205742Z"
    },
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-26T07:40:20.601966Z",
     "start_time": "2019-08-26T07:40:20.144384Z"
    },
    "code_folding": [
     18,
     49,
     52,
     67
    ],
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"output/predictions:0\", shape=(?,), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "self = cnn\n",
    "# 定义模型的输入\n",
    "self.inputX = tf.placeholder(tf.int32, [None, config.padSize], name=\"inputX\")\n",
    "self.inputY = tf.placeholder(tf.int32, [None], name=\"inputY\")\n",
    "self.dropoutKeepProb = tf.placeholder(tf.float32, name=\"dropoutKeepProb\")\n",
    "# 定义l2损失\n",
    "l2Loss = tf.constant(0.0)\n",
    "# 词嵌入层\n",
    "with tf.name_scope(\"embedding\"):\n",
    "    # 利用预训练的词向量初始化词嵌入矩阵\n",
    "    self.W = tf.Variable(tf.cast(wordEmbedding, dtype=tf.float32, name=\"word2vec\"), name=\"W\")\n",
    "    # 利用词嵌入矩阵将输入的数据中的词转换成词向量，维度[batch_size, sequence_length, embedding_size]\n",
    "    self.embeddedWords = tf.nn.embedding_lookup(self.W, self.inputX)\n",
    "    # 卷积的输入是思维[batch_size, width, height, channel]，因此需要增加维度，用tf.expand_dims来增大维度\n",
    "    self.embeddedWordsExpanded = tf.expand_dims(self.embeddedWords, -1)\n",
    "# 创建卷积和池化层\n",
    "pooledOutputs = []\n",
    "# 有三种size的filter，2, 3， 4， 5，textCNN是个多通道单层卷积的模型，可以看作三个单层的卷积模型的融合\n",
    "for i, filterSize in enumerate(config.model.filterSizes):\n",
    "    with tf.name_scope(\"conv-maxpool-%s\" % filterSize):\n",
    "        # 卷积层，卷积核尺寸为filterSize * embeddingSize，卷积核的个数为numFilters\n",
    "        # 初始化权重矩阵和偏置\n",
    "        filterShape = [filterSize, config.weDim, 1, config.model.numFilters]\n",
    "        W = tf.Variable(tf.truncated_normal(filterShape, stddev=0.1), name=\"W\")\n",
    "        b = tf.Variable(tf.constant(0.1, shape=[config.model.numFilters]), name=\"b\")\n",
    "        convRes = tf.nn.conv2d(\n",
    "            input=self.embeddedWordsExpanded,\n",
    "            filter=W,\n",
    "            strides=[1, 1, 1, 1],\n",
    "            padding=\"VALID\",\n",
    "            name=\"conv\")\n",
    "        # relu函数的非线性映射\n",
    "        h = tf.nn.relu(tf.nn.bias_add(convRes, b), name=\"relu\")\n",
    "        # 池化层，最大池化，池化是对卷积后的序列取一个最大值\n",
    "        pooled = tf.nn.max_pool(\n",
    "            h,\n",
    "            ksize=[1, config.padSize - filterSize + 1, 1, 1],\n",
    "            # ksize shape: [batch, height, width, channels]\n",
    "            strides=[1, 1, 1, 1],\n",
    "            padding='VALID',\n",
    "            name=\"pool\")\n",
    "        pooledOutputs.append(pooled)  # 将三种size的filter的输出一起加入到列表中\n",
    "# 得到CNN网络的输出长度\n",
    "numFiltersTotal = config.model.numFilters * len(config.model.filterSizes)\n",
    "# 池化后的维度不变，按照最后的维度channel来concat\n",
    "self.hPool = tf.concat(pooledOutputs, 3)\n",
    "# 摊平成二维的数据输入到全连接层\n",
    "self.hPoolFlat = tf.reshape(self.hPool, [-1, numFiltersTotal])\n",
    "# dropout\n",
    "with tf.name_scope(\"dropout\"):\n",
    "    self.hDrop = tf.nn.dropout(self.hPoolFlat, self.dropoutKeepProb)\n",
    "# 全连接层的输出\n",
    "with tf.name_scope(\"output\"):\n",
    "    outputW = tf.get_variable(\n",
    "        \"outputW\",\n",
    "        shape=[numFiltersTotal, config.numClasses],\n",
    "        initializer=tf.contrib.layers.xavier_initializer())\n",
    "    outputB = tf.Variable(tf.constant(0.1, shape=[config.numClasses]), name=\"outputB\")\n",
    "    l2Loss += tf.nn.l2_loss(outputW)\n",
    "    l2Loss += tf.nn.l2_loss(outputB)\n",
    "    self.logits = tf.nn.xw_plus_b(self.hDrop, outputW, outputB, name=\"logits\")\n",
    "    if config.numClasses == 1:\n",
    "        self.predictions = tf.cast(tf.greater_equal(self.logits, 0.0), tf.int32, name=\"predictions\")\n",
    "    elif config.numClasses > 1:\n",
    "        self.predictions = tf.argmax(self.logits, axis=-1, name=\"predictions\")\n",
    "    print(self.predictions)\n",
    "# 计算二元交叉熵损失\n",
    "with tf.name_scope(\"loss\"):\n",
    "    if config.numClasses == 1:\n",
    "        losses = tf.nn.sigmoid_cross_entropy_with_logits(logits=self.logits,\n",
    "                                                         labels=tf.cast(tf.reshape(self.inputY, [-1, 1]),\n",
    "                                                                        dtype=tf.float32))\n",
    "    elif config.numClasses > 1:\n",
    "        losses = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=self.logits, labels=self.inputY)\n",
    "    self.loss = tf.reduce_mean(losses) + config.model.l2RegLambda * l2Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-26T09:14:21.063567Z",
     "start_time": "2019-08-26T09:14:21.014919Z"
    },
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([Dimension(None), Dimension(4)])"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "TensorShape([Dimension(None)])"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(TensorShape([Dimension(None), Dimension(256)]),\n",
       " TensorShape([Dimension(256), Dimension(4)]),\n",
       " TensorShape([Dimension(4)]))"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "TensorShape([Dimension(None), Dimension(256)])"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "((128, 16), (128,))"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "[5, 300, 1, 64]"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "TensorShape([Dimension(None), Dimension(12), Dimension(1), Dimension(64)])"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "TensorShape([Dimension(None), Dimension(1), Dimension(1), Dimension(64)])"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor 'conv-maxpool-2/pool:0' shape=(?, 1, 1, 64) dtype=float32>,\n",
       " <tf.Tensor 'conv-maxpool-3/pool:0' shape=(?, 1, 1, 64) dtype=float32>,\n",
       " <tf.Tensor 'conv-maxpool-4/pool:0' shape=(?, 1, 1, 64) dtype=float32>,\n",
       " <tf.Tensor 'conv-maxpool-5/pool:0' shape=(?, 1, 1, 64) dtype=float32>]"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(256, '=', 64, '*', 4)"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "TensorShape([Dimension(None), Dimension(1), Dimension(1), Dimension(256)])"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(TensorShape([Dimension(None), Dimension(1), Dimension(1), Dimension(256)]),\n",
       " TensorShape([Dimension(None), Dimension(1), Dimension(1), Dimension(256)]))"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "TensorShape([Dimension(None), Dimension(256)])"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "TensorShape([Dimension(None), Dimension(256)])"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "TensorShape([Dimension(None), Dimension(4)])"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "self.logits.shape\n",
    "self.inputY.shape\n",
    "self.hDrop.shape,outputW.shape,outputB.shape\n",
    "self.hPoolFlat.shape\n",
    "inpX.shape,inpY.shape\n",
    "filterShape\n",
    "convRes.shape\n",
    "pooled.shape\n",
    "pooledOutputs\n",
    "numFiltersTotal,\"=\",config.model.numFilters,\"*\",len(config.model.filterSizes)\n",
    "self.hPool.shape\n",
    "tf.concat(pooledOutputs, 3).shape,tf.concat(pooledOutputs, -1).shape\n",
    "self.hPoolFlat.shape\n",
    "self.hDrop.shape\n",
    "self.logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-26T09:23:22.436394Z",
     "start_time": "2019-08-26T09:23:21.222210Z"
    },
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    },
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(128, 16, 300, 1)"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "[5, 300, 1, 64]"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(5, 300, 1, 64)"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(128, 12, 1, 64)"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(128, 1, 1, 64)"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(256, 4)"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(4,)"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(128, 4)"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(128,)"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(128,)"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "array([1.4023113 , 1.5657477 , 1.7323208 , 2.3154604 , 1.4236103 ,\n",
       "       2.3104873 , 2.510226  , 1.7112172 , 2.4531314 , 3.5707343 ,\n",
       "       2.0037222 , 2.2307396 , 0.6760866 , 0.7952584 , 1.7398844 ,\n",
       "       0.4866007 , 1.4628849 , 2.2971668 , 2.4402852 , 1.1971685 ,\n",
       "       0.28154853, 1.8142693 , 2.8085344 , 1.6108375 , 1.4340734 ,\n",
       "       1.6402532 , 1.6090662 , 2.47478   , 1.2308149 , 2.1750264 ,\n",
       "       2.5136476 , 0.5574802 , 2.345002  , 1.055381  , 1.4210742 ,\n",
       "       2.1778467 , 0.8857384 , 2.002428  , 2.3761125 , 2.1489208 ,\n",
       "       1.3527851 , 2.9691591 , 2.0523012 , 1.3553982 , 2.3866122 ,\n",
       "       2.062508  , 0.8311429 , 1.4849635 , 3.2463124 , 2.0339174 ,\n",
       "       1.3725076 , 1.7733464 , 2.1743388 , 1.7351596 , 1.7145491 ,\n",
       "       0.9842802 , 2.284831  , 1.6461021 , 2.2718644 , 2.4052873 ,\n",
       "       2.4376135 , 1.916029  , 3.0347264 , 1.1442926 , 2.091535  ,\n",
       "       1.6416998 , 2.4054003 , 0.5788255 , 0.07663927, 1.9334015 ,\n",
       "       2.3130248 , 2.3649788 , 2.2262034 , 1.9178288 , 2.0890977 ,\n",
       "       2.0890977 , 1.6813436 , 2.2066913 , 0.760509  , 2.0675669 ,\n",
       "       1.6530172 , 2.576899  , 1.2309769 , 1.9865499 , 0.67231566,\n",
       "       3.0987873 , 1.0004792 , 2.5118027 , 1.5107104 , 2.3416672 ,\n",
       "       1.9770949 , 0.40880018, 2.0573184 , 1.0498761 , 1.8278272 ,\n",
       "       2.028242  , 1.4590733 , 2.5557837 , 1.9305687 , 2.9346132 ,\n",
       "       2.2830634 , 1.865175  , 1.517356  , 2.4470446 , 2.9245982 ,\n",
       "       1.855438  , 0.7076356 , 1.2217379 , 2.0463588 , 1.8412752 ,\n",
       "       0.56589174, 1.2196368 , 2.5901513 , 2.341505  , 2.9309037 ,\n",
       "       2.2656276 , 1.1190796 , 0.6028662 , 1.3627071 , 2.0853777 ,\n",
       "       2.1359735 , 2.6151414 , 0.6374204 , 1.9924712 , 2.5303442 ,\n",
       "       3.624809  , 2.2974653 , 1.6893773 ], dtype=float32)"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    fd={self.inputX:inpX,self.inputY:inpY,self.dropoutKeepProb:0.8}\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    sess.run(self.embeddedWordsExpanded,feed_dict=fd).shape\n",
    "    filterShape\n",
    "    sess.run(W).shape\n",
    "    sess.run(convRes,feed_dict=fd).shape\n",
    "    sess.run(pooled,feed_dict=fd).shape\n",
    "    sess.run(outputW,fd).shape\n",
    "    sess.run(outputB,fd).shape\n",
    "    sess.run(self.logits,fd).shape\n",
    "    sess.run(self.predictions,fd).shape\n",
    "    sess.run(self.inputY,fd).shape\n",
    "    sess.run(tf.nn.sparse_softmax_cross_entropy_with_logits(logits=self.logits, labels=self.inputY),fd)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-26T09:32:21.165414Z",
     "start_time": "2019-08-26T09:32:21.162336Z"
    }
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-26T09:35:38.352541Z",
     "start_time": "2019-08-26T09:35:38.346390Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8000, 16)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "128"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainReviews.shape\n",
    "config.batchSize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-27T03:33:35.491076Z",
     "start_time": "2019-08-27T03:33:35.486796Z"
    }
   },
   "outputs": [],
   "source": [
    "from zac_pyutils import TFUtils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-27T06:24:11.220234Z",
     "start_time": "2019-08-27T06:24:06.687366Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /home/zhoutong/nlp/data/textcnn/textcnn_model_ckpt/model.ckpt-20480\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'output/predictions:0' shape=(?,) dtype=int64>"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Froze 11 variables.\n",
      "INFO:tensorflow:Converted 11 variables to const ops.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   \n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"-1\"\n",
    "tf.reset_default_graph() # 防止运行多次后session创建、运行会变慢\n",
    "ckpt_fp=\"/home/zhoutong/nlp/data/textcnn/textcnn_model_ckpt/model.ckpt-20480\"\n",
    "pb_fp = \"/home/zhoutong/nlp/data/textcnn/tmp_model.pb.sample\"\n",
    "saver = tf.train.import_meta_graph(ckpt_fp + '.meta', clear_devices=True)\n",
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, ckpt_fp)\n",
    "    output_tensor = sess.graph.get_tensor_by_name('output/predictions:0')\n",
    "    output_tensor\n",
    "    output_graph_def = tf.graph_util.convert_variables_to_constants(\n",
    "            sess=sess,\n",
    "            input_graph_def=sess.graph_def,\n",
    "            output_node_names=['output/predictions'])\n",
    "    with tf.gfile.GFile(pb_fp, \"wb\") as f:  # 保存模型\n",
    "        f.write(output_graph_def.SerializeToString())  # 序列化输出"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-27T06:26:30.389896Z",
     "start_time": "2019-08-27T06:16:46.256Z"
    }
   },
   "outputs": [],
   "source": [
    "# import os\n",
    "# os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   \n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"-1\"\n",
    "TFUtils.convert_ckpt2pb(ckpt_fp=\"/home/zhoutong/nlp/data/textcnn/textcnn_model_ckpt/model.ckpt-20480\",\n",
    "                        pb_fp = \"/home/zhoutong/nlp/data/textcnn/tmp_model.pb.sample\",\n",
    "                        output_name_list=['output/predictions'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
