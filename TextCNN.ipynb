{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-23T02:51:41.714585Z",
     "start_time": "2019-08-23T02:51:41.445782Z"
    }
   },
   "outputs": [],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "%matplotlib inline\n",
    "from tqdm.auto import tqdm\n",
    "import concurrent.futures\n",
    "from multiprocessing import Pool\n",
    "import copy,os,sys,psutil\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-23T02:51:43.631688Z",
     "start_time": "2019-08-23T02:51:41.946145Z"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import datetime\n",
    "import gensim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from zac_pyutils.ExqUtils import zprint\n",
    "from zac_pyutils import ExqUtils\n",
    "from collections import deque\n",
    "from tqdm.auto import tqdm\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "# from gensim.models.wrappers import FastText\n",
    "import fasttext\n",
    "import json\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-23T02:51:43.663194Z",
     "start_time": "2019-08-23T02:51:43.654935Z"
    },
    "code_folding": [
     0,
     7,
     42
    ]
   },
   "outputs": [],
   "source": [
    "class TrainingConfig(object):\n",
    "    epoches = 5\n",
    "    evaluateEvery = 100\n",
    "    checkpointEvery = 100\n",
    "    learningRate = 0.001\n",
    "    minWordCnt = 5\n",
    "\n",
    "class ModelConfig(object):\n",
    "    embeddingSize = 200\n",
    "    numFilters = 128\n",
    "\n",
    "    filterSizes = [2, 3, 4, 5]\n",
    "    dropoutKeepProb = 0.5\n",
    "    l2RegLambda = 0.0\n",
    "\n",
    "class Config(object):\n",
    "    _job = \"taste\"\n",
    "    _basePath = \"/home/zhoutong/nlp/data\"\n",
    "    dataSource = _basePath + \"/labeled_{}_train.json\".format(_job)\n",
    "    dataSource = dataSource + \".sample_h10k\"\n",
    "    testSource = _basePath + \"/labeled_{}_test.json\".format(_job)\n",
    "\n",
    "    weDim = 300\n",
    "    weFilePath = _basePath + \"/wordEmbeddingInfo\"  # \\t分割 词,idx,embedding\n",
    "    ft_modelPath = _basePath + '/cc.en.300.bin'\n",
    "\n",
    "    batchSize = 128\n",
    "    pad_size = 1024\n",
    "    pad = '<PAD>'\n",
    "    pad_initV = np.zeros(weDim)\n",
    "    unk = '<UNK>'\n",
    "    unk_initV = np.random.randn(weDim)\n",
    "\n",
    "\n",
    "    numClasses = 1  # 二分类设置为1，多分类设置为类别的数目\n",
    "\n",
    "    testRatio = 0.2  # 测试集的比例\n",
    "\n",
    "    training = TrainingConfig()\n",
    "\n",
    "    model = ModelConfig()\n",
    "\n",
    "class Utils():\n",
    "    # 清理符号\n",
    "    @staticmethod\n",
    "    def clean_punctuation(inp_text):\n",
    "        res = re.sub(r\"[~!@#$%^&*()_+-={\\}|\\[\\]:\\\";'<>?,./“”]\", r' ', inp_text)\n",
    "        res = re.sub(r\"\\\\u200[Bb]\", r' ', res)\n",
    "        res = re.sub(r\"\\n+\", r\" \", res)\n",
    "        res = re.sub(r\"\\s+\", \" \", res)\n",
    "        return res\n",
    "    @staticmethod\n",
    "    def pad_list(inp_list,width,pad_const):\n",
    "        if len(inp_list) >= width:\n",
    "            return inp_list[:width]\n",
    "        else:\n",
    "            return inp_list+[pad_const]*(width-len(inp_list))\n",
    "    \n",
    "config = Config()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TextCNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-23T02:51:52.215689Z",
     "start_time": "2019-08-23T02:51:52.186639Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "class TextCNN(object):\n",
    "    \"\"\"\n",
    "    Text CNN 用于文本分类\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config, wordEmbedding):\n",
    "\n",
    "        # 定义模型的输入\n",
    "        self.inputX = tf.placeholder(tf.int32, [None, config.pad_size], name=\"inputX\")\n",
    "        self.inputY = tf.placeholder(tf.int32, [None], name=\"inputY\")\n",
    "\n",
    "        self.dropoutKeepProb = tf.placeholder(tf.float32, name=\"dropoutKeepProb\")\n",
    "\n",
    "        # 定义l2损失\n",
    "        l2Loss = tf.constant(0.0)\n",
    "\n",
    "        # 词嵌入层\n",
    "        with tf.name_scope(\"embedding\"):\n",
    "            # 利用预训练的词向量初始化词嵌入矩阵\n",
    "            self.W = tf.Variable(tf.cast(wordEmbedding, dtype=tf.float32, name=\"word2vec\"), name=\"W\")\n",
    "            # 利用词嵌入矩阵将输入的数据中的词转换成词向量，维度[batch_size, sequence_length, embedding_size]\n",
    "            self.embeddedWords = tf.nn.embedding_lookup(self.W, self.inputX)\n",
    "            # 卷积的输入是思维[batch_size, width, height, channel]，因此需要增加维度，用tf.expand_dims来增大维度\n",
    "            self.embeddedWordsExpanded = tf.expand_dims(self.embeddedWords, -1)\n",
    "\n",
    "        # 创建卷积和池化层\n",
    "        pooledOutputs = []\n",
    "        # 有三种size的filter，2, 3， 4， 5，textCNN是个多通道单层卷积的模型，可以看作三个单层的卷积模型的融合\n",
    "        for i, filterSize in enumerate(config.model.filterSizes):\n",
    "            with tf.name_scope(\"conv-maxpool-%s\" % filterSize):\n",
    "                # 卷积层，卷积核尺寸为filterSize * embeddingSize，卷积核的个数为numFilters\n",
    "                # 初始化权重矩阵和偏置\n",
    "                filterShape = [filterSize, config.model.embeddingSize, 1, config.model.numFilters]\n",
    "                W = tf.Variable(tf.truncated_normal(filterShape, stddev=0.1), name=\"W\")\n",
    "                b = tf.Variable(tf.constant(0.1, shape=[config.model.numFilters]), name=\"b\")\n",
    "                convRes = tf.nn.conv2d(\n",
    "                    inpput=self.embeddedWordsExpanded,\n",
    "                    filter=W,\n",
    "                    strides=[1, 1, 1, 1],\n",
    "                    padding=\"VALID\",\n",
    "                    name=\"conv\")\n",
    "\n",
    "                # relu函数的非线性映射\n",
    "                h = tf.nn.relu(tf.nn.bias_add(convRes, b), name=\"relu\")\n",
    "\n",
    "                # 池化层，最大池化，池化是对卷积后的序列取一个最大值\n",
    "                pooled = tf.nn.max_pool(\n",
    "                    h,\n",
    "                    ksize=[1, config.pad_size - filterSize + 1, 1, 1],\n",
    "                    # ksize shape: [batch, height, width, channels]\n",
    "                    strides=[1, 1, 1, 1],\n",
    "                    padding='VALID',\n",
    "                    name=\"pool\")\n",
    "                pooledOutputs.append(pooled)  # 将三种size的filter的输出一起加入到列表中\n",
    "\n",
    "        # 得到CNN网络的输出长度\n",
    "        numFiltersTotal = config.model.numFilters * len(config.model.filterSizes)\n",
    "\n",
    "        # 池化后的维度不变，按照最后的维度channel来concat\n",
    "        self.hPool = tf.concat(pooledOutputs, 3)\n",
    "\n",
    "        # 摊平成二维的数据输入到全连接层\n",
    "        self.hPoolFlat = tf.reshape(self.hPool, [-1, numFiltersTotal])\n",
    "\n",
    "        # dropout\n",
    "        with tf.name_scope(\"dropout\"):\n",
    "            self.hDrop = tf.nn.dropout(self.hPoolFlat, self.dropoutKeepProb)\n",
    "\n",
    "        # 全连接层的输出\n",
    "        with tf.name_scope(\"output\"):\n",
    "            outputW = tf.get_variable(\n",
    "                \"outputW\",\n",
    "                shape=[numFiltersTotal, config.numClasses],\n",
    "                initializer=tf.contrib.layers.xavier_initializer())\n",
    "            outputB = tf.Variable(tf.constant(0.1, shape=[config.numClasses]), name=\"outputB\")\n",
    "            l2Loss += tf.nn.l2_loss(outputW)\n",
    "            l2Loss += tf.nn.l2_loss(outputB)\n",
    "            self.logits = tf.nn.xw_plus_b(self.hDrop, outputW, outputB, name=\"logits\")\n",
    "            if config.numClasses == 1:\n",
    "                self.predictions = tf.cast(tf.greater_equal(self.logits, 0.0), tf.int32, name=\"predictions\")\n",
    "            elif config.numClasses > 1:\n",
    "                self.predictions = tf.argmax(self.logits, axis=-1, name=\"predictions\")\n",
    "\n",
    "            print(self.predictions)\n",
    "\n",
    "        # 计算二元交叉熵损失\n",
    "        with tf.name_scope(\"loss\"):\n",
    "\n",
    "            if config.numClasses == 1:\n",
    "                losses = tf.nn.sigmoid_cross_entropy_with_logits(logits=self.logits,\n",
    "                                                                 labels=tf.cast(tf.reshape(self.inputY, [-1, 1]),\n",
    "                                                                                dtype=tf.float32))\n",
    "            elif config.numClasses > 1:\n",
    "                losses = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=self.logits, labels=self.inputY)\n",
    "\n",
    "            self.loss = tf.reduce_mean(losses) + config.model.l2RegLambda * l2Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-23T02:52:21.014805Z",
     "start_time": "2019-08-23T02:52:20.987455Z"
    },
    "code_folding": [
     0,
     5,
     15,
     32,
     52,
     72,
     90,
     106,
     122,
     139,
     154
    ]
   },
   "outputs": [],
   "source": [
    "class MetricUtils():\n",
    "    \"\"\"\n",
    "    定义各类性能指标\n",
    "    \"\"\"\n",
    "    @staticmethod\n",
    "    def mean(item: list) -> float:\n",
    "        \"\"\"\n",
    "        计算列表中元素的平均值\n",
    "        :param item: 列表对象\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        res = sum(item) / len(item) if len(item) > 0 else 0\n",
    "        return res\n",
    "\n",
    "    @staticmethod\n",
    "    def accuracy(pred_y, true_y):\n",
    "        \"\"\"\n",
    "        计算二类和多类的准确率\n",
    "        :param pred_y: 预测结果\n",
    "        :param true_y: 真实结果\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        if isinstance(pred_y[0], list):\n",
    "            pred_y = [item[0] for item in pred_y]\n",
    "        corr = 0\n",
    "        for i in range(len(pred_y)):\n",
    "            if pred_y[i] == true_y[i]:\n",
    "                corr += 1\n",
    "        acc = corr / len(pred_y) if len(pred_y) > 0 else 0\n",
    "        return acc\n",
    "\n",
    "    @staticmethod\n",
    "    def binary_precision(pred_y, true_y, positive=1):\n",
    "        \"\"\"\n",
    "        二类的精确率计算\n",
    "        :param pred_y: 预测结果\n",
    "        :param true_y: 真实结果\n",
    "        :param positive: 正例的索引表示\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        corr = 0\n",
    "        pred_corr = 0\n",
    "        for i in range(len(pred_y)):\n",
    "            if pred_y[i] == positive:\n",
    "                pred_corr += 1\n",
    "                if pred_y[i] == true_y[i]:\n",
    "                    corr += 1\n",
    "\n",
    "        prec = corr / pred_corr if pred_corr > 0 else 0\n",
    "        return prec\n",
    "\n",
    "    @staticmethod\n",
    "    def binary_recall(pred_y, true_y, positive=1):\n",
    "        \"\"\"\n",
    "        二类的召回率\n",
    "        :param pred_y: 预测结果\n",
    "        :param true_y: 真实结果\n",
    "        :param positive: 正例的索引表示\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        corr = 0\n",
    "        true_corr = 0\n",
    "        for i in range(len(pred_y)):\n",
    "            if true_y[i] == positive:\n",
    "                true_corr += 1\n",
    "                if pred_y[i] == true_y[i]:\n",
    "                    corr += 1\n",
    "\n",
    "        rec = corr / true_corr if true_corr > 0 else 0\n",
    "        return rec\n",
    "\n",
    "    @staticmethod\n",
    "    def binary_f_beta(pred_y, true_y, beta=1.0, positive=1):\n",
    "        \"\"\"\n",
    "        二类的f beta值\n",
    "        :param pred_y: 预测结果\n",
    "        :param true_y: 真实结果\n",
    "        :param beta: beta值\n",
    "        :param positive: 正例的索引表示\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        precision = MetricUtils.binary_precision(pred_y, true_y, positive)\n",
    "        recall = MetricUtils.binary_recall(pred_y, true_y, positive)\n",
    "        try:\n",
    "            f_b = (1 + beta * beta) * precision * recall / (beta * beta * precision + recall)\n",
    "        except:\n",
    "            f_b = 0\n",
    "        return f_b\n",
    "\n",
    "    @staticmethod\n",
    "    def multi_precision(pred_y, true_y, labels):\n",
    "        \"\"\"\n",
    "        多类的精确率\n",
    "        :param pred_y: 预测结果\n",
    "        :param true_y: 真实结果\n",
    "        :param labels: 标签列表\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        if isinstance(pred_y[0], list):\n",
    "            pred_y = [item[0] for item in pred_y]\n",
    "\n",
    "        precisions = [MetricUtils.binary_precision(pred_y, true_y, label) for label in labels]\n",
    "        prec = MetricUtils.mean(precisions)\n",
    "        return prec\n",
    "\n",
    "    @staticmethod\n",
    "    def multi_recall(pred_y, true_y, labels):\n",
    "        \"\"\"\n",
    "        多类的召回率\n",
    "        :param pred_y: 预测结果\n",
    "        :param true_y: 真实结果\n",
    "        :param labels: 标签列表\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        if isinstance(pred_y[0], list):\n",
    "            pred_y = [item[0] for item in pred_y]\n",
    "\n",
    "        recalls = [MetricUtils.binary_recall(pred_y, true_y, label) for label in labels]\n",
    "        rec = MetricUtils.mean(recalls)\n",
    "        return rec\n",
    "\n",
    "    @staticmethod\n",
    "    def multi_f_beta(pred_y, true_y, labels, beta=1.0):\n",
    "        \"\"\"\n",
    "        多类的f beta值\n",
    "        :param pred_y: 预测结果\n",
    "        :param true_y: 真实结果\n",
    "        :param labels: 标签列表\n",
    "        :param beta: beta值\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        if isinstance(pred_y[0], list):\n",
    "            pred_y = [item[0] for item in pred_y]\n",
    "\n",
    "        f_betas = [MetricUtils.binary_f_beta(pred_y, true_y, beta, label) for label in labels]\n",
    "        f_beta = MetricUtils.mean(f_betas)\n",
    "        return f_beta\n",
    "\n",
    "    @staticmethod\n",
    "    def get_binary_metrics(pred_y, true_y, f_beta=1.0):\n",
    "        \"\"\"\n",
    "        得到二分类的性能指标\n",
    "        :param pred_y:\n",
    "        :param true_y:\n",
    "        :param f_beta:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        acc = MetricUtils.accuracy(pred_y, true_y)\n",
    "        recall = MetricUtils.binary_recall(pred_y, true_y)\n",
    "        precision = MetricUtils.binary_precision(pred_y, true_y)\n",
    "        f_beta = MetricUtils.binary_f_beta(pred_y, true_y, f_beta)\n",
    "        return acc, recall, precision, f_beta\n",
    "\n",
    "    @staticmethod\n",
    "    def get_multi_metrics(pred_y, true_y, labels, f_beta=1.0):\n",
    "        \"\"\"\n",
    "        得到多分类的性能指标\n",
    "        :param pred_y:\n",
    "        :param true_y:\n",
    "        :param labels:\n",
    "        :param f_beta:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        acc = MetricUtils.accuracy(pred_y, true_y)\n",
    "        recall = MetricUtils.multi_recall(pred_y, true_y, labels)\n",
    "        precision = MetricUtils.multi_precision(pred_y, true_y, labels)\n",
    "        f_beta = MetricUtils.multi_f_beta(pred_y, true_y, labels, f_beta)\n",
    "        return acc, recall, precision, f_beta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Class Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-23T03:09:12.805788Z",
     "start_time": "2019-08-23T03:09:12.783187Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "class Dataset(object):\n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "        self._dataSource = config.dataSource\n",
    "\n",
    "        self.testRatio = config.testRatio\n",
    "        self._we_fp = config.weFilePath\n",
    "        self.ft_modelPath = config.ft_modelPath\n",
    "        self.ft_model = None\n",
    "        self.totalWordCnt = 0\n",
    "\n",
    "        self.trainReviews = np.array([])\n",
    "        self.trainLabels = np.array([])\n",
    "\n",
    "        self.evalReviews = np.array([])\n",
    "        self.evalLabels = np.array([])\n",
    "\n",
    "        self.wordEmbedding = None\n",
    "\n",
    "        self.labelList = []\n",
    "\n",
    "\n",
    "    def _readData(self, filePath):\n",
    "        f_iter = ExqUtils.load_file_as_iter(filePath)\n",
    "        tokens_list = deque()\n",
    "        label_list = deque()\n",
    "        zprint(\"loading data from: \"+filePath)\n",
    "        for l in tqdm(f_iter):\n",
    "            text,label = l.strip().split(\"__label__\")\n",
    "            tokens = Utils.pad_list(Utils.clean_punctuation(text).split(\" \"),width=self.config.pad_size,pad_const=self.config.pad)\n",
    "            tokens_list.append(tokens)\n",
    "            label_list.append(label)\n",
    "        return np.array(tokens_list), np.array(label_list).reshape(-1,1)\n",
    "\n",
    "    def _initStopWord(self, stopWordPath):\n",
    "        with open(stopWordPath, \"r\") as fr:\n",
    "            self._stopWordSet = set(fr.read().splitlines())\n",
    "\n",
    "    def _initFasttextModel(self):\n",
    "        if self.ft_model is None:\n",
    "            zprint(\"init fasttext model\")\n",
    "            self.ft_model = fasttext.load_model(self.ft_modelPath)\n",
    "\n",
    "    def dataGen(self):\n",
    "        \"\"\"\n",
    "        初始化训练集和验证集\n",
    "        \"\"\"\n",
    "\n",
    "        self._initFasttextModel()\n",
    "\n",
    "        # 初始化数据集\n",
    "        tokens_arr, label_arr = self._readData(self._dataSource)\n",
    "\n",
    "        # tokens 到 emb 存在文件里\n",
    "        zprint(\"set所有词\")\n",
    "        tokensSet = set(t for tokens in tokens_arr for t in tokens)\n",
    "        zprint(\"预测词向量存入文件: {}\".format(self._we_fp))\n",
    "        with open(self._we_fp,\"w\") as fw:\n",
    "            # 加上 <PAD> 和 <UNK> 及其初始化\n",
    "            for idx,token in tqdm(enumerate(tokensSet),total=len(tokensSet)):\n",
    "                if token == self.config.pad:\n",
    "                    emb = self.config.pad_initV\n",
    "                elif token == self.config.unk:\n",
    "                    emb = self.config.unk_initV\n",
    "                else:\n",
    "                    emb = self.ft_model[token]\n",
    "                fw.writelines(str(idx)+\"\\t\"+token+\"\\t\"+\",\".join([str(i) for i in list(emb)])+\"\\n\")\n",
    "\n",
    "\n",
    "        self.totalWordCnt = len(tokensSet)\n",
    "\n",
    "\n",
    "        # 初始化训练集和测试集\n",
    "        sss = StratifiedShuffleSplit(n_splits=1, test_size=self.testRatio, random_state=2019)\n",
    "        train_idx, test_idx = list(sss.split(np.zeros(label_arr.shape[0]), label_arr))[0]\n",
    "        \n",
    "        self.trainReviews = tokens_arr[train_idx]\n",
    "        self.trainLabels = label_arr[train_idx]\n",
    "\n",
    "        self.evalReviews = tokens_arr[test_idx]\n",
    "        self.evalLabels = label_arr[test_idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Produce Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-23T03:10:43.629421Z",
     "start_time": "2019-08-23T03:09:15.008591Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|2019-08-23 11:09:15| init fasttext model\n",
      "|2019-08-23 11:09:25| loading data from: /home/zhoutong/nlp/data/labeled_taste_train.json.sample_h10k\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "60e800da7b754a2abacdf8b2cca16be0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "|2019-08-23 11:09:29| set所有词\n",
      "|2019-08-23 11:09:33| 预测词向量存入文件: /home/zhoutong/nlp/data/wordEmbeddingInfo\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f3f2c649d01403ab18a059ef45cd1e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=111246), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "train data shape: (8000, 1024)\n",
      "train label shape: (8000, 1)\n",
      "eval data shape: (2000, 1024)\n",
      "wordEmbedding info file: /home/zhoutong/nlp/data/wordEmbeddingInfo\n"
     ]
    }
   ],
   "source": [
    "data = Dataset(config)\n",
    "data.dataGen()\n",
    "print(\"train data shape: {}\".format(data.trainReviews.shape))\n",
    "print(\"train label shape: {}\".format(data.trainLabels.shape))\n",
    "print(\"eval data shape: {}\".format(data.evalReviews.shape))\n",
    "print(\"wordEmbedding info file: {}\".format(data._we_fp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-23T03:26:26.468975Z",
     "start_time": "2019-08-23T03:26:22.807801Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|2019-08-23 11:26:22| loading data from: /home/zhoutong/nlp/data/labeled_taste_train.json.sample_h10k\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a9449967136445ca7a6dc7e77830bf7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "tokens_arr, label_arr = data._readData(data._dataSource)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-23T03:27:15.791820Z",
     "start_time": "2019-08-23T03:27:09.370820Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['On', 'Auspicious', 'Day', ..., '<PAD>', '<PAD>', '<PAD>'],\n",
       "       ['China', 's', 'Xi', ..., '<PAD>', '<PAD>', '<PAD>'],\n",
       "       ['NDA', 'candidates', 'of', ..., '<PAD>', '<PAD>', '<PAD>'],\n",
       "       ...,\n",
       "       ['Glass', 'Director', 'on', ..., '<PAD>', '<PAD>', '<PAD>'],\n",
       "       ['First', 'confirmed', 'case', ..., '<PAD>', '<PAD>', '<PAD>'],\n",
       "       ['Vin', 'Diesel', 'hints', ..., '<PAD>', '<PAD>', '<PAD>']],\n",
       "      dtype='<U41')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "{'',\n",
       " 'Stopped',\n",
       " 'MOU',\n",
       " 'Bhowmik',\n",
       " 'Demar',\n",
       " 'fuels',\n",
       " 'Narkeldanga',\n",
       " 'Tiro',\n",
       " 'loyalties',\n",
       " 'vindictiveness',\n",
       " 'Jacquelyn',\n",
       " 'Partap',\n",
       " 'soured',\n",
       " 'ToloNews',\n",
       " 'Allergy',\n",
       " 'Bardas',\n",
       " 'Assad',\n",
       " 'Gogh’s',\n",
       " 'circumvented',\n",
       " 'Memoir',\n",
       " 'ensure',\n",
       " 'quantum',\n",
       " 'locally',\n",
       " 'AAG',\n",
       " 'Introduces',\n",
       " 'WANTS',\n",
       " 'Biju',\n",
       " 'Talaash',\n",
       " 'Warhol',\n",
       " 'Menino',\n",
       " 'mediation',\n",
       " '–which',\n",
       " 'Paralympics',\n",
       " 'WftuDflJZhQBP',\n",
       " '🎮',\n",
       " 'sunpictures',\n",
       " 'divas',\n",
       " '‘photos',\n",
       " 'Mediation',\n",
       " 'EXTEND',\n",
       " 'Kapoors',\n",
       " 'Luz',\n",
       " 'Audrain',\n",
       " 'Shwait',\n",
       " 'automobiles',\n",
       " 'crackle',\n",
       " 'Sanjeevini',\n",
       " 'Matilde',\n",
       " 'Julius',\n",
       " 'Anchal',\n",
       " 'Tinder',\n",
       " 'enriched',\n",
       " 'nuances',\n",
       " 'Vaz',\n",
       " 'thermocol',\n",
       " 'Bawne',\n",
       " 'creeckit',\n",
       " 'Air',\n",
       " 'Swchh',\n",
       " 'Chevrel',\n",
       " 'KLEMS',\n",
       " 'balloons',\n",
       " 'flyzonefitness',\n",
       " 'horsepower',\n",
       " 'mutated',\n",
       " '‘Mystic',\n",
       " 'Hanley',\n",
       " 'Published',\n",
       " 'Patralekhaahas',\n",
       " 'Sjoerd',\n",
       " 'Rebekah',\n",
       " 'Montpellier',\n",
       " 'briefsTenders',\n",
       " 'PM’',\n",
       " 'Midari',\n",
       " 'enacting',\n",
       " 'personhood',\n",
       " 'Brake',\n",
       " 'Kampati',\n",
       " 'Eric',\n",
       " 'ceremonies',\n",
       " 'Live',\n",
       " 'Malida',\n",
       " 'Albania',\n",
       " 'royal',\n",
       " 'goo',\n",
       " 'optical',\n",
       " 'Alec',\n",
       " 'dads',\n",
       " 'bund',\n",
       " 'compassionately',\n",
       " 'Come',\n",
       " 'flop',\n",
       " 'fudge',\n",
       " 'Chandauli',\n",
       " 'looters',\n",
       " 'Arvidsson',\n",
       " 'transfigured',\n",
       " 'Uttarayana',\n",
       " 'Tremours',\n",
       " 'HOCKEY',\n",
       " 'betta',\n",
       " 'Works’',\n",
       " 'slotting',\n",
       " 'Beaudentity',\n",
       " 'Saraya',\n",
       " 'Karuna',\n",
       " 'tional',\n",
       " 'wrecksite',\n",
       " 'Париже🔥🌐',\n",
       " 'Tripping',\n",
       " 'middle’',\n",
       " 'Jaal',\n",
       " 'Overtake',\n",
       " 'ethane',\n",
       " 'Boys',\n",
       " 'Rcom',\n",
       " 'diary',\n",
       " 'Guangzhou',\n",
       " 'Jordana',\n",
       " 'muse',\n",
       " 'movement',\n",
       " 'shared',\n",
       " 'fiesta',\n",
       " '‘deemed',\n",
       " 'Consultancy',\n",
       " 'Harichandan',\n",
       " 'Gursimrat',\n",
       " 'transmits',\n",
       " 'la',\n",
       " 'शुक्रिया।',\n",
       " 'clean',\n",
       " 'scehdule',\n",
       " 'Siveri',\n",
       " 'antivirals',\n",
       " 'Goddesses',\n",
       " 'biharboard',\n",
       " 'infrared',\n",
       " 'foreseeable',\n",
       " 'Gaganyan',\n",
       " 'canceling',\n",
       " 'revise',\n",
       " 'Sicko',\n",
       " 'Vodacom',\n",
       " 'Cusp',\n",
       " 'graphic',\n",
       " 'unsportsmanlike',\n",
       " 'Mouni',\n",
       " 'idUSL',\n",
       " 'Edelweiss',\n",
       " 'McAuliffe',\n",
       " 'fetishisation',\n",
       " 'Cabello',\n",
       " 'Greeks',\n",
       " 'postmatch',\n",
       " 'Rahulji',\n",
       " 'Frankly',\n",
       " 'Mantri’s',\n",
       " '‘Performance',\n",
       " 'india',\n",
       " '…two',\n",
       " 'swivelled',\n",
       " 'Garrison',\n",
       " 'Adda’',\n",
       " 'Mounded',\n",
       " 'Shazahn',\n",
       " 'increased',\n",
       " 'militias',\n",
       " 'Purser',\n",
       " 'Gauges',\n",
       " 'fuck',\n",
       " 'Commuters',\n",
       " 'unguided',\n",
       " 'Encounters',\n",
       " 'IZ',\n",
       " 'Melpo',\n",
       " 'Srinivasulu',\n",
       " 'Seoul’s',\n",
       " 'Vishwam',\n",
       " 'totally',\n",
       " 'tweets',\n",
       " 'Jhunsi',\n",
       " 'seizing',\n",
       " 'Vyasa’s',\n",
       " 'gemba',\n",
       " 'Bilger’s',\n",
       " 'Nolle',\n",
       " 'martyrs’',\n",
       " 'Celebrations',\n",
       " 'Liner',\n",
       " 'complainant’s',\n",
       " 'Ihatsu',\n",
       " 'ShareChat',\n",
       " 'enlarged',\n",
       " 'jammed',\n",
       " 'BCCLPriyanka',\n",
       " 'ENTERTAINMENT',\n",
       " 'XD',\n",
       " 'wE',\n",
       " 'vindo',\n",
       " 'electrical',\n",
       " 'Choti',\n",
       " 'IQVIA',\n",
       " 'Goopy',\n",
       " 'best—the',\n",
       " 'corporation',\n",
       " 'healthiness',\n",
       " 'virulently',\n",
       " 'tremble',\n",
       " 'Gough',\n",
       " 'pilots’',\n",
       " 'Chitpore',\n",
       " 'Buzza',\n",
       " 'crushes',\n",
       " 'Captur',\n",
       " 'Charlie',\n",
       " 'Konark',\n",
       " 'ODIRanchi',\n",
       " 'Zadibal',\n",
       " 'terminology',\n",
       " 'rotting',\n",
       " '‘honour',\n",
       " 'Spirited',\n",
       " 'Vikky',\n",
       " 'Karti',\n",
       " 'matrimony',\n",
       " 'Forming',\n",
       " 'Foolproof',\n",
       " 'galvanising',\n",
       " 'pathetic',\n",
       " 'Commissioner',\n",
       " 'Determine',\n",
       " 'Navada',\n",
       " 'Chandaneswar',\n",
       " 'Law',\n",
       " 'kgs',\n",
       " 'ImTeCHO',\n",
       " 'Cheat',\n",
       " 'CMRL',\n",
       " 'Sovan',\n",
       " 'Garza',\n",
       " 'Newborns',\n",
       " 'Crest',\n",
       " 'cyclones',\n",
       " 'evolutions',\n",
       " 'CTO',\n",
       " 'Heinecke',\n",
       " '‘off',\n",
       " 'CAUSE',\n",
       " 'Got',\n",
       " 'Collegian',\n",
       " 'GTPL',\n",
       " 'emasculation',\n",
       " 'Lionel',\n",
       " 'ArmyDay',\n",
       " 'IMSI',\n",
       " 'alienation',\n",
       " 'pirouette',\n",
       " 'TI',\n",
       " 'Dilwale',\n",
       " 'Mannings',\n",
       " 'Annexes',\n",
       " 'CCRB',\n",
       " 'spaghetti',\n",
       " 'transgenders',\n",
       " 'Bitcom',\n",
       " '‘fridging',\n",
       " 'Firstpost',\n",
       " 'underweight',\n",
       " 'Nuneaton',\n",
       " 'baseball',\n",
       " 'shooter',\n",
       " 'Bennetts',\n",
       " 'Arti',\n",
       " 'explanations',\n",
       " 'conversely',\n",
       " 'Baripada',\n",
       " 'Beta',\n",
       " 'Snowy',\n",
       " 'WVEC',\n",
       " 'Musaram',\n",
       " 'Chemicals',\n",
       " 'palace',\n",
       " 'precocity',\n",
       " 'theProtection',\n",
       " '–or',\n",
       " 'Temperature',\n",
       " 'Jeronimo',\n",
       " 'Vajinepally',\n",
       " 'Bulka',\n",
       " 'reassuring',\n",
       " 'keyfobs',\n",
       " 'reckoned',\n",
       " 'Mutton',\n",
       " '‘plan',\n",
       " 'chloroform',\n",
       " 'osteoporotic',\n",
       " 'Reacted',\n",
       " 'RHT',\n",
       " 'Hubbard',\n",
       " 'freaky',\n",
       " 'muck',\n",
       " 'Parcels',\n",
       " 'Netflix‘s',\n",
       " 'VisvavidyalayamTirupati',\n",
       " 'Customized',\n",
       " 'Gstaad',\n",
       " 'presumes',\n",
       " 'briefsKulwant',\n",
       " 'orchestra',\n",
       " 'manchild',\n",
       " 'shootouts',\n",
       " 'Sakra',\n",
       " 'Prabbhakar’s',\n",
       " 'retweet',\n",
       " 'Greenway’s',\n",
       " 'BIDMC',\n",
       " 'Shoe',\n",
       " 'AnilKapoor',\n",
       " 'Skycom',\n",
       " 'Mankade',\n",
       " 'Duddeda',\n",
       " 'equipments',\n",
       " 'Dynamo',\n",
       " 'undercut',\n",
       " 'vehement',\n",
       " 'BatsmenODI',\n",
       " 'beingness',\n",
       " 'shouldn\\x92t',\n",
       " 'Let’s',\n",
       " 'Panthavoor',\n",
       " 'Heist',\n",
       " 'WSCUC',\n",
       " 'Loretta',\n",
       " 'Gates’s',\n",
       " '‘middleman’',\n",
       " 'Sukhada',\n",
       " 'Bahunjan',\n",
       " 'Polypipes',\n",
       " 'cloaked',\n",
       " 'editorials',\n",
       " 'airforce',\n",
       " 'explores',\n",
       " 'confounds',\n",
       " 'imageJAIPUR',\n",
       " 'dictation',\n",
       " 'emergencies',\n",
       " 'MacGregor',\n",
       " 'first…',\n",
       " 'Shameik',\n",
       " 'Sian',\n",
       " 'NCLTL',\n",
       " 'рrоduсt',\n",
       " 'decolonising',\n",
       " 'briefsResidents',\n",
       " 'together',\n",
       " 'Rica',\n",
       " 'kgf',\n",
       " 'EH',\n",
       " 'fishermen',\n",
       " 'shyer',\n",
       " 'Regiment',\n",
       " 'yueng',\n",
       " 'tore',\n",
       " 'Arundhati',\n",
       " 'Ira',\n",
       " 'DreamGirl',\n",
       " 'Subtle',\n",
       " 'foesThe',\n",
       " 'Richman',\n",
       " 'Khas',\n",
       " 'M',\n",
       " 'washington',\n",
       " 'Chiropractor',\n",
       " 'Exposure',\n",
       " 'sohaalikhan',\n",
       " 'reinvest',\n",
       " 'contribiution',\n",
       " 'Ramkali',\n",
       " 'insiders',\n",
       " 'struck',\n",
       " 'Trayvon',\n",
       " '‘other’',\n",
       " 'haunts',\n",
       " 'Adviser',\n",
       " 'robberies',\n",
       " 'agent’s',\n",
       " 'clears',\n",
       " 'offstage',\n",
       " 'Prakashnagar',\n",
       " 'tweaks',\n",
       " 'Steyn',\n",
       " 'policyholders',\n",
       " 'odissi',\n",
       " 'Vidheya',\n",
       " 'HollywoodLife',\n",
       " 'pilgrim',\n",
       " 'MPBattery',\n",
       " 'snowy',\n",
       " 'dulay',\n",
       " 'Devasahayam',\n",
       " 'bharatanatyam',\n",
       " 'roader',\n",
       " 'banegi',\n",
       " 'consequent',\n",
       " 'Sanjay’s',\n",
       " 'Popeye',\n",
       " 'Components',\n",
       " 'Pocahontas',\n",
       " 'Rowen',\n",
       " 'Sportzpics',\n",
       " 'summary',\n",
       " 'StreicherLaura',\n",
       " 'Aneliya',\n",
       " 'Boucher',\n",
       " 'Exactly',\n",
       " 'KSWDC',\n",
       " 'bipolar',\n",
       " 'flourishes',\n",
       " 'transit',\n",
       " 'dislocation',\n",
       " 'WAR',\n",
       " 'flora',\n",
       " 'infusions',\n",
       " 'stilla',\n",
       " 'dopamine',\n",
       " 'snowstorms',\n",
       " 'Fraudsters',\n",
       " 'GearNuke',\n",
       " 'ninja',\n",
       " 'Loew',\n",
       " 'Myung',\n",
       " 'rework',\n",
       " 'Samruddhi',\n",
       " 'LISTED',\n",
       " 'payer',\n",
       " 'Critic',\n",
       " 'Lamuanpuia',\n",
       " 'Madhvi',\n",
       " 'Triad',\n",
       " 'Parmarth',\n",
       " 'Strack',\n",
       " 'Late',\n",
       " 'recruiting',\n",
       " 'Scorching',\n",
       " 'Jordyn',\n",
       " 'forgotten',\n",
       " 'navodhanam',\n",
       " 'Chengdu',\n",
       " 'Attrill',\n",
       " 'Provisional',\n",
       " 'raptures',\n",
       " 'schedule',\n",
       " 'Descent',\n",
       " 'Ants',\n",
       " 'creditors’',\n",
       " 'Naturally',\n",
       " 'BANK',\n",
       " 'Side',\n",
       " 'incidentally',\n",
       " 'Kitch',\n",
       " 'Ilan',\n",
       " 'Pitched',\n",
       " 'stars–',\n",
       " 'inaugural',\n",
       " 'anarchy',\n",
       " 'incorporated',\n",
       " 'bodys',\n",
       " 'Ladies',\n",
       " 'Sameet',\n",
       " '‘Nahi',\n",
       " 'approvals',\n",
       " 'Raghavan',\n",
       " 'squishies',\n",
       " 'Cech',\n",
       " 'Citizen',\n",
       " 'Fantoosh—and',\n",
       " 'freshness',\n",
       " 'implications',\n",
       " 'neophyte',\n",
       " 'creepy',\n",
       " 'MW',\n",
       " 'truer',\n",
       " 'third',\n",
       " 'Springer',\n",
       " 'ideologies',\n",
       " 'PHOTOS',\n",
       " 'Rayka',\n",
       " 'Steen',\n",
       " 'exhibition',\n",
       " 'NCDEX',\n",
       " 'onlyMUMBAI',\n",
       " 'prevalent',\n",
       " 'stonewall',\n",
       " 'nw',\n",
       " 'relaxable',\n",
       " 'vote…',\n",
       " 'Barratt',\n",
       " 'Hematological',\n",
       " 'Wheelchair',\n",
       " 'whistle',\n",
       " 'Ventures',\n",
       " 'Anushka’s',\n",
       " 'magistrate',\n",
       " 'peddled',\n",
       " 'boutique',\n",
       " 'Cerner',\n",
       " 'Atsushi',\n",
       " 'Rushina',\n",
       " 'Propylene',\n",
       " 'Min',\n",
       " 'suits',\n",
       " 'goofing',\n",
       " 'Sarsaul',\n",
       " 'Evolution',\n",
       " 'Abigail',\n",
       " 'tez',\n",
       " 'petroleum',\n",
       " 'BfE',\n",
       " 'favorably',\n",
       " 'TrialKit',\n",
       " 'Maharathi',\n",
       " 'Navigator’s',\n",
       " 'Visage',\n",
       " 'uk',\n",
       " 'radius',\n",
       " 'McTaggart',\n",
       " 'CelebrationEveryone',\n",
       " 'Vaigai',\n",
       " 'DevLearn',\n",
       " 'Vizzion',\n",
       " 'creaky',\n",
       " 'Gham…',\n",
       " 'Caboo',\n",
       " 'Harshit',\n",
       " 'Sarbh',\n",
       " 'Balangiri',\n",
       " 'Ebrahimi',\n",
       " 'constants',\n",
       " 'philanthropist',\n",
       " 'stains',\n",
       " 'Venmo',\n",
       " 'Doval',\n",
       " 'el',\n",
       " 'ramen',\n",
       " 'Kulcha',\n",
       " 'Sidharth',\n",
       " 'whosoever',\n",
       " 'Jetta',\n",
       " 'bypass',\n",
       " 'hei',\n",
       " 'gasp',\n",
       " 'Aware',\n",
       " 'Nalgonda',\n",
       " 'Ramdas',\n",
       " 'watchable',\n",
       " 'Rambox',\n",
       " 'omnibox',\n",
       " 'JMM',\n",
       " 'waters',\n",
       " 'purifier',\n",
       " 'OmniPeace',\n",
       " 'Valera',\n",
       " 'Chart',\n",
       " 'butchers',\n",
       " 'Zooms',\n",
       " 'baar’',\n",
       " 'seriously',\n",
       " 'Draw',\n",
       " 'Nikka',\n",
       " 'interesting',\n",
       " 'dietary',\n",
       " 'Israeli',\n",
       " 'Poachers',\n",
       " 'brawl',\n",
       " 'music',\n",
       " '‘revenge',\n",
       " 'disobedience',\n",
       " 'Pb',\n",
       " 'Jila',\n",
       " 'Nel',\n",
       " 'UNCHR',\n",
       " 'Zplus',\n",
       " 'Fahrenheit',\n",
       " 'decides',\n",
       " 'rheumatoid',\n",
       " 'understanding',\n",
       " 'parwa',\n",
       " 'Rousey’s',\n",
       " 'MEERUT',\n",
       " 'papers',\n",
       " 'Rohrich',\n",
       " 'rekking',\n",
       " 'Chirumarthi',\n",
       " 'Hara',\n",
       " 'Parminder',\n",
       " 'Chapitô',\n",
       " 'Rikiya',\n",
       " 'aristocrat',\n",
       " 'Woodpecker',\n",
       " 'Greta',\n",
       " 'skull',\n",
       " 'Curaon',\n",
       " 'SOLOMON',\n",
       " 'arrondissement',\n",
       " 'Personal',\n",
       " 'NRL',\n",
       " 'Into',\n",
       " 'customise',\n",
       " 'Premchand',\n",
       " 'quake',\n",
       " 'BelowShares',\n",
       " 'Nanthan',\n",
       " 'heinous',\n",
       " 'aerobats',\n",
       " 'fabricating',\n",
       " 'wet',\n",
       " 'Laxma',\n",
       " 'cafeteria',\n",
       " 'martyr',\n",
       " 'cursive',\n",
       " 'menstruation',\n",
       " 'Peregrine',\n",
       " 'Namibian',\n",
       " 'Dubey’s',\n",
       " 'Onida',\n",
       " 'Jagannadh',\n",
       " 'spoilsport',\n",
       " 'Revealing',\n",
       " '‘Bolo',\n",
       " 'Corporal',\n",
       " 'Laurence',\n",
       " 'Gc',\n",
       " 'bc',\n",
       " 'friendships',\n",
       " 'categorisation',\n",
       " 'தமிழ்',\n",
       " 'Natali',\n",
       " 'Brave',\n",
       " 'Avramidis',\n",
       " 'Estadio',\n",
       " 'suspension',\n",
       " 'BJPs',\n",
       " 'day',\n",
       " 'TheWrap',\n",
       " 'entrol',\n",
       " 'Akhtar‘s',\n",
       " 'MNCH',\n",
       " 'Suguna',\n",
       " 'gloves',\n",
       " 'Storming',\n",
       " 'nextech',\n",
       " 'basing',\n",
       " 'separately',\n",
       " 'dorchestercollection',\n",
       " 'Champions',\n",
       " 'depleted',\n",
       " 'McCawley',\n",
       " 'Klassiker',\n",
       " 'patrolman',\n",
       " 'Subsidised',\n",
       " 'multifarious',\n",
       " 'zinc',\n",
       " 'Dharmadhikari',\n",
       " 'Hatsuno',\n",
       " 'Sabre',\n",
       " 'editable',\n",
       " 'PPM',\n",
       " 'Addison’s',\n",
       " 'mudslide',\n",
       " 'Falooda',\n",
       " 'Welle',\n",
       " 'category’',\n",
       " 'Muthappa',\n",
       " 'Enes',\n",
       " 'Mitchell',\n",
       " 'Kripesh',\n",
       " 'Singh',\n",
       " 'Terriers',\n",
       " 'Belvedere',\n",
       " 'supremacists',\n",
       " 'Rajeshwar',\n",
       " 'Joaquim',\n",
       " 'DCPCR',\n",
       " 'Dench',\n",
       " 'dinned',\n",
       " 'Vadakkumcherry',\n",
       " 'chalon',\n",
       " 'Hausmarchen',\n",
       " 'Pharma',\n",
       " 'Poornchander',\n",
       " 'orLT',\n",
       " 'shing',\n",
       " 'Sacking',\n",
       " 'cash',\n",
       " 'Cinematic',\n",
       " 'BREAKOUT',\n",
       " 'Stupidity',\n",
       " 'BALLS',\n",
       " 'Thakurmunda',\n",
       " 'heels',\n",
       " 'RATH’',\n",
       " 'Isco’s',\n",
       " 'Species',\n",
       " 'Vijayakant',\n",
       " 'bid',\n",
       " 'Dettol',\n",
       " 'Japhet',\n",
       " 'Palms',\n",
       " 'Tampere',\n",
       " 'Kaliah',\n",
       " 'penetrate',\n",
       " 'Plants',\n",
       " 'Hinders',\n",
       " 'Callers',\n",
       " 'Brahmapuri’s',\n",
       " 'Haqeeqat',\n",
       " 'fostered',\n",
       " 'marked',\n",
       " 'Itoje',\n",
       " 'Ganashakti',\n",
       " 'Maxim',\n",
       " 'broadened',\n",
       " 'Amrapali’s',\n",
       " 'family’s',\n",
       " 'Love',\n",
       " 'sop',\n",
       " 'auspiciousness',\n",
       " 'sneeze',\n",
       " 'Vitamin',\n",
       " 'EU’s',\n",
       " 'Hospital—in',\n",
       " 'Hansaria',\n",
       " 'Chandrakar',\n",
       " 'tamper',\n",
       " 'involved',\n",
       " 'woodcutters',\n",
       " 'Jaggayyapet',\n",
       " '±',\n",
       " 'NEWSA',\n",
       " 'Tihar',\n",
       " 'paras',\n",
       " 'Latheef',\n",
       " 'tfiuuvm',\n",
       " 'Queries',\n",
       " 'Suby',\n",
       " 'Kanner',\n",
       " 'substrate',\n",
       " 'Camps',\n",
       " 'holdings',\n",
       " 'Dhenkanak',\n",
       " 'REACTIONS',\n",
       " 'rousted',\n",
       " 'relates',\n",
       " 'Observations',\n",
       " 'mash',\n",
       " 'trilateral',\n",
       " 'Ekatarina',\n",
       " 'NMC’s',\n",
       " 'Lundgren',\n",
       " 'Insight',\n",
       " 'Wizz',\n",
       " 'Experienced',\n",
       " 'UNESCO',\n",
       " 'wipes',\n",
       " 'collateral',\n",
       " 'slowed',\n",
       " 'drive…',\n",
       " 'Lobo',\n",
       " 'rebooted',\n",
       " 'mtr',\n",
       " 'bucadibeppo',\n",
       " 'pathway',\n",
       " 'Avimukteshwaranand',\n",
       " 'DikshitNEW',\n",
       " 'Koranyi',\n",
       " 'Millionaires',\n",
       " 'Vijai',\n",
       " 'raag',\n",
       " 'johndavidpfeiffer',\n",
       " 'suffocation',\n",
       " 'companionability',\n",
       " 'Lakhwinder',\n",
       " 'peak',\n",
       " 'LegReal',\n",
       " 'dithering',\n",
       " 'it\\x92s',\n",
       " 'genes—think',\n",
       " 'decoder',\n",
       " 'Siberian',\n",
       " 'Dharmic',\n",
       " 'Bahl’s',\n",
       " 'DC’s',\n",
       " 'Frankel',\n",
       " 'smoothie',\n",
       " 'Jhund',\n",
       " 'lobbied',\n",
       " 'Janjua',\n",
       " 'Shakarpur',\n",
       " 'linesman',\n",
       " 'Bose’s',\n",
       " 'samitis',\n",
       " 'admiot',\n",
       " 'SAUDI',\n",
       " 'Babhnauli',\n",
       " 'needing',\n",
       " 'Parallel',\n",
       " 'tutorial',\n",
       " 'WALMART',\n",
       " 'winner’s',\n",
       " 'Cheluvanarayana',\n",
       " 'night',\n",
       " 'Charlote',\n",
       " 'crude',\n",
       " 'PMR',\n",
       " 'revival',\n",
       " 'joust',\n",
       " 'sausage',\n",
       " 'BJP’s',\n",
       " 'japna',\n",
       " 'Mister',\n",
       " 'cakewalk',\n",
       " 'upliftment',\n",
       " 'Mw',\n",
       " 'bayou',\n",
       " 'honest',\n",
       " 'wagers',\n",
       " 'yajna',\n",
       " 'Bitcoin’s',\n",
       " 'Balgahat',\n",
       " 'ceremonial',\n",
       " 'MEM',\n",
       " 'Pouille',\n",
       " 'burqa',\n",
       " 'Khansahib',\n",
       " 'frank',\n",
       " 'Sesha',\n",
       " 'sanyansi',\n",
       " 'wrinkles',\n",
       " 'them…',\n",
       " 'Splendour',\n",
       " 'Maiya',\n",
       " 'Yatra’',\n",
       " 'dung',\n",
       " 'Adu',\n",
       " 'Dressing',\n",
       " 'trenches',\n",
       " 'Bokakhat',\n",
       " 'indescribable',\n",
       " 'precedes',\n",
       " 'Femme',\n",
       " 'prisoner',\n",
       " 'lobbying',\n",
       " 'ADMK',\n",
       " 'Sabooj',\n",
       " 'Doute',\n",
       " 'Exchange',\n",
       " 'conversational',\n",
       " 'warsOberoi',\n",
       " 'scrum',\n",
       " 'Lille',\n",
       " 'Nabin',\n",
       " 'Karachi',\n",
       " 'disheartened',\n",
       " 'regularize',\n",
       " 'Ankama',\n",
       " 'Construction',\n",
       " 'Jahangirpuri',\n",
       " 'apathetic',\n",
       " 'cognition',\n",
       " 'dated',\n",
       " 'Vengai',\n",
       " 'Crimestoppers',\n",
       " 'dissolution',\n",
       " 'Midtown',\n",
       " 'affixed',\n",
       " 'Truecaller',\n",
       " 'Ardern',\n",
       " 'crusher',\n",
       " 'Alderweireld',\n",
       " 'Blamed',\n",
       " 'Gegong',\n",
       " 'Spy',\n",
       " 'Sada',\n",
       " 'Challenger',\n",
       " 'DEPARTMENT',\n",
       " 'REPUBLICANS',\n",
       " 'mota',\n",
       " 'Khejroliya',\n",
       " 'subsumes',\n",
       " 'unfurl',\n",
       " 'Suumaya',\n",
       " 'burial',\n",
       " 'Waltair',\n",
       " 'முக்கிய',\n",
       " 'marketing',\n",
       " 'Scopes',\n",
       " 'Steelport',\n",
       " 'Hallomajra',\n",
       " 'Gurman',\n",
       " 'Hooch',\n",
       " 'burgers',\n",
       " 'liberalised',\n",
       " 'filmmaker',\n",
       " 'chalky',\n",
       " 'JLL',\n",
       " 'Raipur',\n",
       " 'Lunch',\n",
       " 'alphabetical',\n",
       " 'Netanyahu’s',\n",
       " 'Anoop',\n",
       " 'Syncrony',\n",
       " 'Kakopora',\n",
       " 'Somaya',\n",
       " 'noose',\n",
       " 'DAVOS',\n",
       " 'prise',\n",
       " 'sauda',\n",
       " 'Blackfish',\n",
       " 'Tantrum',\n",
       " 'Rabbani',\n",
       " 'Captivated',\n",
       " 'Hull',\n",
       " 'Inflation',\n",
       " 'angwanwadi',\n",
       " 'Bristol',\n",
       " 'pirs',\n",
       " 'LGBTQ',\n",
       " 'Auradkar',\n",
       " '•Why',\n",
       " 'Vasil',\n",
       " 'sequentially',\n",
       " '‘Start',\n",
       " 'MI',\n",
       " '‘Resonance',\n",
       " 'Imahira',\n",
       " 'Kaisar',\n",
       " 'huh',\n",
       " 'hurting',\n",
       " 'conceded',\n",
       " 'pipped',\n",
       " 'watch’s',\n",
       " '‘half',\n",
       " 'Untwal',\n",
       " 'Mate',\n",
       " 'Kitts',\n",
       " 'Delegations',\n",
       " 'terrific',\n",
       " 'Keller',\n",
       " 'Hasan',\n",
       " 'Leaking',\n",
       " 'hires',\n",
       " 'KumaraswamyBENGALURU',\n",
       " 'SRINAGAR',\n",
       " 'spearheaded',\n",
       " 'examination',\n",
       " 'Jagdalpur’s',\n",
       " 'deduct',\n",
       " 'Mithilesh',\n",
       " 'Azarmi',\n",
       " 'reticent',\n",
       " 'boyfriend’s',\n",
       " 'Hamsik',\n",
       " 'pled',\n",
       " 'Haranadh',\n",
       " 'Manemma',\n",
       " 'Newborn',\n",
       " 'blowback',\n",
       " 'ABC',\n",
       " 'Dadlani',\n",
       " '‘review',\n",
       " 'speakeasytalks',\n",
       " 'Owaisi',\n",
       " 'Drip',\n",
       " 'Kafelnikov',\n",
       " 'Rajpura',\n",
       " 'monopolising',\n",
       " 'frivolous',\n",
       " 'Industrie',\n",
       " 'Aranganur',\n",
       " 'Fixes',\n",
       " 'Awardee',\n",
       " 'blend',\n",
       " 'surgence',\n",
       " 'Constantly',\n",
       " 'Dior’s',\n",
       " 'Kamna',\n",
       " 'Nordschleife',\n",
       " 'KR',\n",
       " 'hyeounreacho',\n",
       " 'rasher',\n",
       " 'Enfield’s',\n",
       " 'AstrologyLEO',\n",
       " 'roam',\n",
       " 'indiatimes',\n",
       " 'unnoticeable',\n",
       " 'descend',\n",
       " 'MEDIA',\n",
       " 'Akanksha',\n",
       " 'CMSE',\n",
       " ...}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "{'0', '1', '2', '3'}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens_arr\n",
    "set(np.unique(tokens_arr))\n",
    "set(np.unique(trainLabels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-23T03:06:02.920497Z",
     "start_time": "2019-08-23T03:06:02.916039Z"
    }
   },
   "outputs": [],
   "source": [
    "trainReviews = data.trainReviews\n",
    "trainLabels = data.trainLabels\n",
    "evalReviews = data.evalReviews\n",
    "evalLabels = data.evalLabels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-23T03:13:09.531020Z",
     "start_time": "2019-08-23T03:13:09.517856Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['Bandhan', 'Bank', 'to', ..., '<PAD>', '<PAD>', '<PAD>'],\n",
       "       ['Woman', 'who', 'entered', ..., '<PAD>', '<PAD>', '<PAD>'],\n",
       "       ['All', 'eyes', 'on', ..., '<PAD>', '<PAD>', '<PAD>'],\n",
       "       ...,\n",
       "       ['Dhampur', 'Sugar', 'Consolidated', ..., '<PAD>', '<PAD>',\n",
       "        '<PAD>'],\n",
       "       ['Writer', 'Namvar', 'Singh', ..., '<PAD>', '<PAD>', '<PAD>'],\n",
       "       ['This', 'may', 'be', ..., '<PAD>', '<PAD>', '<PAD>']],\n",
       "      dtype='<U41')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "array([['2'],\n",
       "       ['1'],\n",
       "       ['1'],\n",
       "       ...,\n",
       "       ['2'],\n",
       "       ['1'],\n",
       "       ['1']], dtype='<U1')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "{'0', '1', '2', '3'}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainReviews\n",
    "trainLabels\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 开始构建计算图"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-23T02:51:11.656333Z",
     "start_time": "2019-08-23T02:51:11.649170Z"
    }
   },
   "outputs": [],
   "source": [
    "def nextBatch(x, y, batchSize):\n",
    "    \"\"\"\n",
    "    生成batch数据集，用生成器的方式输出\n",
    "    \"\"\"\n",
    "    perm = np.arange(len(x))\n",
    "    np.random.shuffle(perm)\n",
    "    x,y = x[perm],y[perm]\n",
    "\n",
    "    numBatches = len(x) // batchSize\n",
    "\n",
    "    for i in range(numBatches):\n",
    "        start = i * batchSize\n",
    "        end = start + batchSize\n",
    "        batchX = np.array(x[start: end], dtype=\"int64\")\n",
    "        batchY = np.array(y[start: end], dtype=\"float32\")\n",
    "\n",
    "        yield batchX, batchY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0,
     77
    ]
   },
   "outputs": [],
   "source": [
    "with tf.Graph().as_default():\n",
    "    session_conf = tf.ConfigProto(allow_soft_placement=True, log_device_placement=False)\n",
    "    session_conf.gpu_options.allow_growth = True\n",
    "    session_conf.gpu_options.per_process_gpu_memory_fraction = 0.9  # 配置gpu占用率\n",
    "\n",
    "    sess = tf.Session(config=session_conf)\n",
    "\n",
    "    # 定义会话\n",
    "    with sess.as_default():\n",
    "        cnn = TextCNN(config, wordEmbedding)\n",
    "\n",
    "        globalStep = tf.Variable(0, name=\"globalStep\", trainable=False)\n",
    "        # 定义优化函数，传入学习速率参数\n",
    "        optimizer = tf.train.AdamOptimizer(config.training.learningRate)\n",
    "        # 计算梯度,得到梯度和变量\n",
    "        gradsAndVars = optimizer.compute_gradients(cnn.loss)\n",
    "        # 将梯度应用到变量下，生成训练器\n",
    "        trainOp = optimizer.apply_gradients(gradsAndVars, global_step=globalStep)\n",
    "\n",
    "        # 用summary绘制tensorBoard\n",
    "        gradSummaries = []\n",
    "        for g, v in gradsAndVars:\n",
    "            if g is not None:\n",
    "                tf.summary.histogram(\"{}/grad/hist\".format(v.name), g)\n",
    "                tf.summary.scalar(\"{}/grad/sparsity\".format(v.name), tf.nn.zero_fraction(g))\n",
    "\n",
    "        outDir = os.path.abspath(os.path.join(os.path.curdir, \"summarys\"))\n",
    "        print(\"Writing to {}\\n\".format(outDir))\n",
    "\n",
    "        lossSummary = tf.summary.scalar(\"loss\", cnn.loss)\n",
    "        summaryOp = tf.summary.merge_all()\n",
    "\n",
    "        trainSummaryDir = os.path.join(outDir, \"train\")\n",
    "        trainSummaryWriter = tf.summary.FileWriter(trainSummaryDir, sess.graph)\n",
    "\n",
    "        evalSummaryDir = os.path.join(outDir, \"eval\")\n",
    "        evalSummaryWriter = tf.summary.FileWriter(evalSummaryDir, sess.graph)\n",
    "\n",
    "        # 初始化所有变量\n",
    "        saver = tf.train.Saver(tf.global_variables(), max_to_keep=5)\n",
    "\n",
    "        # 保存模型的一种方式，保存为pb文件\n",
    "        savedModelPath = \"../model/textCNN/savedModel\"\n",
    "        if os.path.exists(savedModelPath):\n",
    "            os.rmdir(savedModelPath)\n",
    "        builder = tf.saved_model.builder.SavedModelBuilder(savedModelPath)\n",
    "\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "\n",
    "\n",
    "        def trainStep(batchX, batchY):\n",
    "            \"\"\"\n",
    "            训练函数\n",
    "            \"\"\"\n",
    "            feed_dict = {\n",
    "                cnn.inputX: batchX,\n",
    "                cnn.inputY: batchY,\n",
    "                cnn.dropoutKeepProb: config.model.dropoutKeepProb\n",
    "            }\n",
    "            _, summary, step, loss, predictions = sess.run(\n",
    "                [trainOp, summaryOp, globalStep, cnn.loss, cnn.predictions],\n",
    "                feed_dict)\n",
    "            timeStr = datetime.datetime.now().isoformat()\n",
    "\n",
    "            if config.numClasses == 1:\n",
    "                acc, recall, prec, f_beta = get_binary_metrics(pred_y=predictions, true_y=batchY)\n",
    "\n",
    "\n",
    "            elif config.numClasses > 1:\n",
    "                acc, recall, prec, f_beta = get_multi_metrics(pred_y=predictions, true_y=batchY,\n",
    "                                                              labels=labelList)\n",
    "\n",
    "            trainSummaryWriter.add_summary(summary, step)\n",
    "\n",
    "            return loss, acc, prec, recall, f_beta\n",
    "\n",
    "\n",
    "        def devStep(batchX, batchY):\n",
    "            \"\"\"\n",
    "            验证函数\n",
    "            \"\"\"\n",
    "            feed_dict = {\n",
    "                cnn.inputX: batchX,\n",
    "                cnn.inputY: batchY,\n",
    "                cnn.dropoutKeepProb: 1.0\n",
    "            }\n",
    "            summary, step, loss, predictions = sess.run(\n",
    "                [summaryOp, globalStep, cnn.loss, cnn.predictions],\n",
    "                feed_dict)\n",
    "\n",
    "            if config.numClasses == 1:\n",
    "\n",
    "                acc, precision, recall, f_beta = get_binary_metrics(pred_y=predictions, true_y=batchY)\n",
    "            elif config.numClasses > 1:\n",
    "                acc, precision, recall, f_beta = get_multi_metrics(pred_y=predictions, true_y=batchY, labels=labelList)\n",
    "\n",
    "            evalSummaryWriter.add_summary(summary, step)\n",
    "\n",
    "            return loss, acc, precision, recall, f_beta\n",
    "\n",
    "\n",
    "        for i in range(config.training.epoches):\n",
    "            # 训练模型\n",
    "            print(\"start training model\")\n",
    "            for batchTrain in nextBatch(trainReviews, trainLabels, config.batchSize):\n",
    "                loss, acc, prec, recall, f_beta = trainStep(batchTrain[0], batchTrain[1])\n",
    "\n",
    "                currentStep = tf.train.global_step(sess, globalStep)\n",
    "                print(\"train: step: {}, loss: {}, acc: {}, recall: {}, precision: {}, f_beta: {}\".format(\n",
    "                    currentStep, loss, acc, recall, prec, f_beta))\n",
    "                if currentStep % config.training.evaluateEvery == 0:\n",
    "                    print(\"\\nEvaluation:\")\n",
    "\n",
    "                    losses = []\n",
    "                    accs = []\n",
    "                    f_betas = []\n",
    "                    precisions = []\n",
    "                    recalls = []\n",
    "\n",
    "                    for batchEval in nextBatch(evalReviews, evalLabels, config.batchSize):\n",
    "                        loss, acc, precision, recall, f_beta = devStep(batchEval[0], batchEval[1])\n",
    "                        losses.append(loss)\n",
    "                        accs.append(acc)\n",
    "                        f_betas.append(f_beta)\n",
    "                        precisions.append(precision)\n",
    "                        recalls.append(recall)\n",
    "\n",
    "                    time_str = datetime.datetime.now().isoformat()\n",
    "                    print(\"{}, step: {}, loss: {}, acc: {},precision: {}, recall: {}, f_beta: {}\".format(time_str,\n",
    "                                                                                                         currentStep,\n",
    "                                                                                                         mean(losses),\n",
    "                                                                                                         mean(accs),\n",
    "                                                                                                         mean(\n",
    "                                                                                                             precisions),\n",
    "                                                                                                         mean(recalls),\n",
    "                                                                                                         mean(f_betas)))\n",
    "\n",
    "                if currentStep % config.training.checkpointEvery == 0:\n",
    "                    # 保存模型的另一种方法，保存checkpoint文件\n",
    "                    path = saver.save(sess, \"../model/textCNN/model/my-model\", global_step=currentStep)\n",
    "                    print(\"Saved model checkpoint to {}\\n\".format(path))\n",
    "\n",
    "        inputs = {\"inputX\": tf.saved_model.utils.build_tensor_info(cnn.inputX),\n",
    "                  \"keepProb\": tf.saved_model.utils.build_tensor_info(cnn.dropoutKeepProb)}\n",
    "\n",
    "        outputs = {\"predictions\": tf.saved_model.utils.build_tensor_info(cnn.predictions)}\n",
    "\n",
    "        prediction_signature = tf.saved_model.signature_def_utils.build_signature_def(inputs=inputs, outputs=outputs,\n",
    "                                                                                      method_name=tf.saved_model.signature_constants.PREDICT_METHOD_NAME)\n",
    "        legacy_init_op = tf.group(tf.tables_initializer(), name=\"legacy_init_op\")\n",
    "        builder.add_meta_graph_and_variables(sess, [tf.saved_model.tag_constants.SERVING],\n",
    "                                             signature_def_map={\"predict\": prediction_signature},\n",
    "                                             legacy_init_op=legacy_init_op)\n",
    "\n",
    "        builder.save()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
