{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-20T08:37:35.062666Z",
     "start_time": "2019-08-20T08:37:35.057704Z"
    }
   },
   "outputs": [],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "%matplotlib inline\n",
    "from tqdm.auto import tqdm\n",
    "import concurrent.futures\n",
    "from multiprocessing import Pool\n",
    "import copy,os,sys,psutil\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-20T08:37:38.244744Z",
     "start_time": "2019-08-20T08:37:35.318382Z"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import datetime\n",
    "import gensim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from zac_pyutils.ExqUtils import zprint\n",
    "from zac_pyutils import ExqUtils\n",
    "from collections import deque\n",
    "from tqdm.auto import tqdm\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "# from gensim.models.wrappers import FastText\n",
    "import fasttext\n",
    "import json\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-20T08:37:25.405149Z",
     "start_time": "2019-08-20T08:37:25.396426Z"
    },
    "code_folding": [
     0,
     7,
     15,
     38
    ]
   },
   "outputs": [],
   "source": [
    "class TrainingConfig(object):\n",
    "    epoches = 5\n",
    "    evaluateEvery = 100\n",
    "    checkpointEvery = 100\n",
    "    learningRate = 0.001\n",
    "    minWordCnt = 5\n",
    "\n",
    "class ModelConfig(object):\n",
    "    embeddingSize = 200\n",
    "    numFilters = 128\n",
    "\n",
    "    filterSizes = [2, 3, 4, 5]\n",
    "    dropoutKeepProb = 0.5\n",
    "    l2RegLambda = 0.0\n",
    "\n",
    "class Config(object):\n",
    "    batchSize = 128\n",
    "    pad_size = 1024\n",
    "    pad = '<PAD>'\n",
    "    unk = '<UNK>'\n",
    "\n",
    "    _job = \"taste\"\n",
    "    _basePath = \"/home/zhoutong/nlp/data\"\n",
    "    dataSource = _basePath + \"/labeled_{}_train.json\".format(_job)\n",
    "    dataSource = dataSource+\".sample_h10k\"\n",
    "    testSource = _basePath + \"/labeled_{}_test.json\".format(_job)\n",
    "\n",
    "    weFilePath = _basePath+\"/wordEmbeddingInfo\" # \\t分割 词,idx,embedding\n",
    "    ft_modelPath = _basePath+'/cc.en.300.bin'\n",
    "\n",
    "    numClasses = 1  # 二分类设置为1，多分类设置为类别的数目\n",
    "\n",
    "    testRatio = 0.2  # 测试集的比例\n",
    "\n",
    "    training = TrainingConfig()\n",
    "\n",
    "    model = ModelConfig()\n",
    "\n",
    "class Utils():\n",
    "    # 清理符号\n",
    "    @staticmethod\n",
    "    def clean_punctuation(inp_text):\n",
    "        res = re.sub(r\"[~!@#$%^&*()_+-={\\}|\\[\\]:\\\";'<>?,./“”]\", r' ', inp_text)\n",
    "        res = re.sub(r\"\\\\u200[Bb]\", r' ', res)\n",
    "        res = re.sub(r\"\\n+\", r\" \", res)\n",
    "        res = re.sub(r\"\\s+\", \" \", res)\n",
    "        return res\n",
    "    @staticmethod\n",
    "    def pad_list(inp_list,width,pad_const):\n",
    "        if len(inp_list) >= width:\n",
    "            return inp_list[:width]\n",
    "        else:\n",
    "            return inp_list+[pad_const]*(width-len(inp_list))\n",
    "    \n",
    "config = Config()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TextCNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-20T08:37:26.422907Z",
     "start_time": "2019-08-20T08:37:26.398745Z"
    },
    "code_folding": [
     0,
     5,
     17,
     29,
     66,
     70,
     87
    ]
   },
   "outputs": [],
   "source": [
    "class TextCNN(object):\n",
    "    \"\"\"\n",
    "    Text CNN 用于文本分类\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config, wordEmbedding):\n",
    "\n",
    "        # 定义模型的输入\n",
    "        self.inputX = tf.placeholder(tf.int32, [None, config.sequenceLength], name=\"inputX\")\n",
    "        self.inputY = tf.placeholder(tf.int32, [None], name=\"inputY\")\n",
    "\n",
    "        self.dropoutKeepProb = tf.placeholder(tf.float32, name=\"dropoutKeepProb\")\n",
    "\n",
    "        # 定义l2损失\n",
    "        l2Loss = tf.constant(0.0)\n",
    "\n",
    "        # 词嵌入层\n",
    "        with tf.name_scope(\"embedding\"):\n",
    "\n",
    "            # 利用预训练的词向量初始化词嵌入矩阵\n",
    "            self.W = tf.Variable(tf.cast(wordEmbedding, dtype=tf.float32, name=\"word2vec\"), name=\"W\")\n",
    "            # 利用词嵌入矩阵将输入的数据中的词转换成词向量，维度[batch_size, sequence_length, embedding_size]\n",
    "            self.embeddedWords = tf.nn.embedding_lookup(self.W, self.inputX)\n",
    "            # 卷积的输入是思维[batch_size, width, height, channel]，因此需要增加维度，用tf.expand_dims来增大维度\n",
    "            self.embeddedWordsExpanded = tf.expand_dims(self.embeddedWords, -1)\n",
    "\n",
    "        # 创建卷积和池化层\n",
    "        pooledOutputs = []\n",
    "        # 有三种size的filter，3， 4， 5，textCNN是个多通道单层卷积的模型，可以看作三个单层的卷积模型的融合\n",
    "        for i, filterSize in enumerate(config.model.filterSizes):\n",
    "            with tf.name_scope(\"conv-maxpool-%s\" % filterSize):\n",
    "                # 卷积层，卷积核尺寸为filterSize * embeddingSize，卷积核的个数为numFilters\n",
    "                # 初始化权重矩阵和偏置\n",
    "                filterShape = [filterSize, config.model.embeddingSize, 1, config.model.numFilters]\n",
    "                W = tf.Variable(tf.truncated_normal(filterShape, stddev=0.1), name=\"W\")\n",
    "                b = tf.Variable(tf.constant(0.1, shape=[config.model.numFilters]), name=\"b\")\n",
    "                conv = tf.nn.conv2d(\n",
    "                    self.embeddedWordsExpanded,\n",
    "                    W,\n",
    "                    strides=[1, 1, 1, 1],\n",
    "                    padding=\"VALID\",\n",
    "                    name=\"conv\")\n",
    "\n",
    "                # relu函数的非线性映射\n",
    "                h = tf.nn.relu(tf.nn.bias_add(conv, b), name=\"relu\")\n",
    "\n",
    "                # 池化层，最大池化，池化是对卷积后的序列取一个最大值\n",
    "                pooled = tf.nn.max_pool(\n",
    "                    h,\n",
    "                    ksize=[1, config.sequenceLength - filterSize + 1, 1, 1],\n",
    "                    # ksize shape: [batch, height, width, channels]\n",
    "                    strides=[1, 1, 1, 1],\n",
    "                    padding='VALID',\n",
    "                    name=\"pool\")\n",
    "                pooledOutputs.append(pooled)  # 将三种size的filter的输出一起加入到列表中\n",
    "\n",
    "        # 得到CNN网络的输出长度\n",
    "        numFiltersTotal = config.model.numFilters * len(config.model.filterSizes)\n",
    "\n",
    "        # 池化后的维度不变，按照最后的维度channel来concat\n",
    "        self.hPool = tf.concat(pooledOutputs, 3)\n",
    "\n",
    "        # 摊平成二维的数据输入到全连接层\n",
    "        self.hPoolFlat = tf.reshape(self.hPool, [-1, numFiltersTotal])\n",
    "\n",
    "        # dropout\n",
    "        with tf.name_scope(\"dropout\"):\n",
    "            self.hDrop = tf.nn.dropout(self.hPoolFlat, self.dropoutKeepProb)\n",
    "\n",
    "        # 全连接层的输出\n",
    "        with tf.name_scope(\"output\"):\n",
    "            outputW = tf.get_variable(\n",
    "                \"outputW\",\n",
    "                shape=[numFiltersTotal, config.numClasses],\n",
    "                initializer=tf.contrib.layers.xavier_initializer())\n",
    "            outputB = tf.Variable(tf.constant(0.1, shape=[config.numClasses]), name=\"outputB\")\n",
    "            l2Loss += tf.nn.l2_loss(outputW)\n",
    "            l2Loss += tf.nn.l2_loss(outputB)\n",
    "            self.logits = tf.nn.xw_plus_b(self.hDrop, outputW, outputB, name=\"logits\")\n",
    "            if config.numClasses == 1:\n",
    "                self.predictions = tf.cast(tf.greater_equal(self.logits, 0.0), tf.int32, name=\"predictions\")\n",
    "            elif config.numClasses > 1:\n",
    "                self.predictions = tf.argmax(self.logits, axis=-1, name=\"predictions\")\n",
    "\n",
    "            print(self.predictions)\n",
    "\n",
    "        # 计算二元交叉熵损失\n",
    "        with tf.name_scope(\"loss\"):\n",
    "\n",
    "            if config.numClasses == 1:\n",
    "                losses = tf.nn.sigmoid_cross_entropy_with_logits(logits=self.logits,\n",
    "                                                                 labels=tf.cast(tf.reshape(self.inputY, [-1, 1]),\n",
    "                                                                                dtype=tf.float32))\n",
    "            elif config.numClasses > 1:\n",
    "                losses = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=self.logits, labels=self.inputY)\n",
    "\n",
    "            self.loss = tf.reduce_mean(losses) + config.model.l2RegLambda * l2Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Class Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-20T08:46:22.774889Z",
     "start_time": "2019-08-20T08:46:22.761397Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "class Dataset(object):\n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "        self._dataSource = config.dataSource\n",
    "\n",
    "        self.testRatio = config.testRatio\n",
    "        self._we_fp = config.weFilePath\n",
    "        self.ft_modelPath = config.ft_modelPath\n",
    "        self.ft_model = None\n",
    "        self.totalWordCnt = 0\n",
    "\n",
    "        self.trainReviews = np.array([])\n",
    "        self.trainLabels = np.array([])\n",
    "\n",
    "        self.evalReviews = np.array([])\n",
    "        self.evalLabels = np.array([])\n",
    "\n",
    "        self.wordEmbedding = None\n",
    "\n",
    "        self.labelList = []\n",
    "\n",
    "\n",
    "    def _readData(self, filePath):\n",
    "        f_iter = ExqUtils.load_file_as_iter(filePath)\n",
    "        tokens_list = deque()\n",
    "        label_list = deque()\n",
    "        zprint(\"loading data from: \"+filePath)\n",
    "        for l in tqdm(f_iter):\n",
    "            text,label = l.strip().split(\"__label__\")\n",
    "            tokens = Utils.pad_list(Utils.clean_punctuation(text).split(\" \"),width=config.pad_size,pad_const=config.pad)\n",
    "            tokens_list.append(tokens)\n",
    "            label_list.append(label)\n",
    "        return np.array(tokens_list), np.array(label_list).reshape(-1,1)\n",
    "\n",
    "    def _initStopWord(self, stopWordPath):\n",
    "        with open(stopWordPath, \"r\") as fr:\n",
    "            self._stopWordSet = set(fr.read().splitlines())\n",
    "\n",
    "    def _initFasttextModel(self):\n",
    "        if self.ft_model is None:\n",
    "            zprint(\"init fasttext model\")\n",
    "            self.ft_model = fasttext.load_model(self.ft_modelPath)\n",
    "\n",
    "    def dataGen(self):\n",
    "        \"\"\"\n",
    "        初始化训练集和验证集\n",
    "        \"\"\"\n",
    "\n",
    "        self._initFasttextModel()\n",
    "\n",
    "        # 初始化数据集\n",
    "        tokens_arr, label_arr = self._readData(self._dataSource)\n",
    "\n",
    "        # tokens 到 emb 存在文件里\n",
    "        zprint(\"set所有词\")\n",
    "        tokensSet = set(t for tokens in tokens_arr for t in tokens)\n",
    "        zprint(\"预测词向量存入文件: {}\".format(self._we_fp))\n",
    "        with open(self._we_fp,\"w\") as fw:\n",
    "            for idx,token in tqdm(enumerate(tokensSet),total=len(tokensSet)):\n",
    "                emb = self.ft_model[token]\n",
    "                fw.writelines(str(idx)+\"\\t\"+token+\",\".join([str(i) for i in list(emb)])+\"\\n\")\n",
    "        self.totalWordCnt = len(tokensSet)\n",
    "\n",
    "\n",
    "        # 初始化训练集和测试集\n",
    "        sss = StratifiedShuffleSplit(n_splits=1, test_size=self.testRatio, random_state=2019)\n",
    "        train_idx, test_idx = list(sss.split(np.zeros(label_arr.shape[0]), label_arr))[0]\n",
    "        \n",
    "        self.trainReviews = tokens_arr[train_idx]\n",
    "        self.trainLabels = label_arr[train_idx]\n",
    "\n",
    "        self.evalReviews = tokens_arr[test_idx]\n",
    "        self.evalLabels = label_arr[test_idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Produce Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-20T08:50:43.532342Z",
     "start_time": "2019-08-20T08:46:32.104299Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|2019-08-20 16:46:32| init fasttext model\n",
      "|2019-08-20 16:46:54| loading data from: /home/zhoutong/nlp/data/labeled_taste_train.json.sample_h10k\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "584496d7aac74dafb021f0ee78cedcec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "|2019-08-20 16:47:04| set所有词\n",
      "|2019-08-20 16:47:13| 预测词向量存入文件: /home/zhoutong/nlp/data/wordEmbeddingInfo\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "183b7b9f6df5490d9fc4cb2ac8bdccfd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=111246), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "train data shape: (8000, 1024)\n",
      "train label shape: (8000, 1)\n",
      "eval data shape: (2000, 1024)\n",
      "wordEmbedding info file: /home/zhoutong/nlp/data/wordEmbeddingInfo\n"
     ]
    }
   ],
   "source": [
    "data = Dataset(config)\n",
    "data.dataGen()\n",
    "print(\"train data shape: {}\".format(data.trainReviews.shape))\n",
    "print(\"train label shape: {}\".format(data.trainLabels.shape))\n",
    "print(\"eval data shape: {}\".format(data.evalReviews.shape))\n",
    "print(\"wordEmbedding info file: {}\".format(data._we_fp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-20T09:22:49.500497Z",
     "start_time": "2019-08-20T09:22:49.496946Z"
    }
   },
   "outputs": [],
   "source": [
    "trainReviews = data.trainReviews\n",
    "trainLabels = data.trainLabels\n",
    "evalReviews = data.evalReviews\n",
    "evalLabels = data.evalLabels\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 开始构建计算图"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nextBatch(x, y, batchSize):\n",
    "    \"\"\"\n",
    "    生成batch数据集，用生成器的方式输出\n",
    "    \"\"\"\n",
    "    perm = np.arange(len(x))\n",
    "    np.random.shuffle(perm)\n",
    "    x,y = x[perm],y[perm]\n",
    "\n",
    "    numBatches = len(x) // batchSize\n",
    "\n",
    "    for i in range(numBatches):\n",
    "        start = i * batchSize\n",
    "        end = start + batchSize\n",
    "        batchX = np.array(x[start: end], dtype=\"int64\")\n",
    "        batchY = np.array(y[start: end], dtype=\"float32\")\n",
    "\n",
    "        yield batchX, batchY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     77
    ]
   },
   "outputs": [],
   "source": [
    "with tf.Graph().as_default():\n",
    "    session_conf = tf.ConfigProto(allow_soft_placement=True, log_device_placement=False)\n",
    "    session_conf.gpu_options.allow_growth = True\n",
    "    session_conf.gpu_options.per_process_gpu_memory_fraction = 0.9  # 配置gpu占用率\n",
    "\n",
    "    sess = tf.Session(config=session_conf)\n",
    "\n",
    "    # 定义会话\n",
    "    with sess.as_default():\n",
    "        cnn = TextCNN(config, wordEmbedding)\n",
    "\n",
    "        globalStep = tf.Variable(0, name=\"globalStep\", trainable=False)\n",
    "        # 定义优化函数，传入学习速率参数\n",
    "        optimizer = tf.train.AdamOptimizer(config.training.learningRate)\n",
    "        # 计算梯度,得到梯度和变量\n",
    "        gradsAndVars = optimizer.compute_gradients(cnn.loss)\n",
    "        # 将梯度应用到变量下，生成训练器\n",
    "        trainOp = optimizer.apply_gradients(gradsAndVars, global_step=globalStep)\n",
    "\n",
    "        # 用summary绘制tensorBoard\n",
    "        gradSummaries = []\n",
    "        for g, v in gradsAndVars:\n",
    "            if g is not None:\n",
    "                tf.summary.histogram(\"{}/grad/hist\".format(v.name), g)\n",
    "                tf.summary.scalar(\"{}/grad/sparsity\".format(v.name), tf.nn.zero_fraction(g))\n",
    "\n",
    "        outDir = os.path.abspath(os.path.join(os.path.curdir, \"summarys\"))\n",
    "        print(\"Writing to {}\\n\".format(outDir))\n",
    "\n",
    "        lossSummary = tf.summary.scalar(\"loss\", cnn.loss)\n",
    "        summaryOp = tf.summary.merge_all()\n",
    "\n",
    "        trainSummaryDir = os.path.join(outDir, \"train\")\n",
    "        trainSummaryWriter = tf.summary.FileWriter(trainSummaryDir, sess.graph)\n",
    "\n",
    "        evalSummaryDir = os.path.join(outDir, \"eval\")\n",
    "        evalSummaryWriter = tf.summary.FileWriter(evalSummaryDir, sess.graph)\n",
    "\n",
    "        # 初始化所有变量\n",
    "        saver = tf.train.Saver(tf.global_variables(), max_to_keep=5)\n",
    "\n",
    "        # 保存模型的一种方式，保存为pb文件\n",
    "        savedModelPath = \"../model/textCNN/savedModel\"\n",
    "        if os.path.exists(savedModelPath):\n",
    "            os.rmdir(savedModelPath)\n",
    "        builder = tf.saved_model.builder.SavedModelBuilder(savedModelPath)\n",
    "\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "\n",
    "\n",
    "        def trainStep(batchX, batchY):\n",
    "            \"\"\"\n",
    "            训练函数\n",
    "            \"\"\"\n",
    "            feed_dict = {\n",
    "                cnn.inputX: batchX,\n",
    "                cnn.inputY: batchY,\n",
    "                cnn.dropoutKeepProb: config.model.dropoutKeepProb\n",
    "            }\n",
    "            _, summary, step, loss, predictions = sess.run(\n",
    "                [trainOp, summaryOp, globalStep, cnn.loss, cnn.predictions],\n",
    "                feed_dict)\n",
    "            timeStr = datetime.datetime.now().isoformat()\n",
    "\n",
    "            if config.numClasses == 1:\n",
    "                acc, recall, prec, f_beta = get_binary_metrics(pred_y=predictions, true_y=batchY)\n",
    "\n",
    "\n",
    "            elif config.numClasses > 1:\n",
    "                acc, recall, prec, f_beta = get_multi_metrics(pred_y=predictions, true_y=batchY,\n",
    "                                                              labels=labelList)\n",
    "\n",
    "            trainSummaryWriter.add_summary(summary, step)\n",
    "\n",
    "            return loss, acc, prec, recall, f_beta\n",
    "\n",
    "\n",
    "        def devStep(batchX, batchY):\n",
    "            \"\"\"\n",
    "            验证函数\n",
    "            \"\"\"\n",
    "            feed_dict = {\n",
    "                cnn.inputX: batchX,\n",
    "                cnn.inputY: batchY,\n",
    "                cnn.dropoutKeepProb: 1.0\n",
    "            }\n",
    "            summary, step, loss, predictions = sess.run(\n",
    "                [summaryOp, globalStep, cnn.loss, cnn.predictions],\n",
    "                feed_dict)\n",
    "\n",
    "            if config.numClasses == 1:\n",
    "\n",
    "                acc, precision, recall, f_beta = get_binary_metrics(pred_y=predictions, true_y=batchY)\n",
    "            elif config.numClasses > 1:\n",
    "                acc, precision, recall, f_beta = get_multi_metrics(pred_y=predictions, true_y=batchY, labels=labelList)\n",
    "\n",
    "            evalSummaryWriter.add_summary(summary, step)\n",
    "\n",
    "            return loss, acc, precision, recall, f_beta\n",
    "\n",
    "\n",
    "        for i in range(config.training.epoches):\n",
    "            # 训练模型\n",
    "            print(\"start training model\")\n",
    "            for batchTrain in nextBatch(trainReviews, trainLabels, config.batchSize):\n",
    "                loss, acc, prec, recall, f_beta = trainStep(batchTrain[0], batchTrain[1])\n",
    "\n",
    "                currentStep = tf.train.global_step(sess, globalStep)\n",
    "                print(\"train: step: {}, loss: {}, acc: {}, recall: {}, precision: {}, f_beta: {}\".format(\n",
    "                    currentStep, loss, acc, recall, prec, f_beta))\n",
    "                if currentStep % config.training.evaluateEvery == 0:\n",
    "                    print(\"\\nEvaluation:\")\n",
    "\n",
    "                    losses = []\n",
    "                    accs = []\n",
    "                    f_betas = []\n",
    "                    precisions = []\n",
    "                    recalls = []\n",
    "\n",
    "                    for batchEval in nextBatch(evalReviews, evalLabels, config.batchSize):\n",
    "                        loss, acc, precision, recall, f_beta = devStep(batchEval[0], batchEval[1])\n",
    "                        losses.append(loss)\n",
    "                        accs.append(acc)\n",
    "                        f_betas.append(f_beta)\n",
    "                        precisions.append(precision)\n",
    "                        recalls.append(recall)\n",
    "\n",
    "                    time_str = datetime.datetime.now().isoformat()\n",
    "                    print(\"{}, step: {}, loss: {}, acc: {},precision: {}, recall: {}, f_beta: {}\".format(time_str,\n",
    "                                                                                                         currentStep,\n",
    "                                                                                                         mean(losses),\n",
    "                                                                                                         mean(accs),\n",
    "                                                                                                         mean(\n",
    "                                                                                                             precisions),\n",
    "                                                                                                         mean(recalls),\n",
    "                                                                                                         mean(f_betas)))\n",
    "\n",
    "                if currentStep % config.training.checkpointEvery == 0:\n",
    "                    # 保存模型的另一种方法，保存checkpoint文件\n",
    "                    path = saver.save(sess, \"../model/textCNN/model/my-model\", global_step=currentStep)\n",
    "                    print(\"Saved model checkpoint to {}\\n\".format(path))\n",
    "\n",
    "        inputs = {\"inputX\": tf.saved_model.utils.build_tensor_info(cnn.inputX),\n",
    "                  \"keepProb\": tf.saved_model.utils.build_tensor_info(cnn.dropoutKeepProb)}\n",
    "\n",
    "        outputs = {\"predictions\": tf.saved_model.utils.build_tensor_info(cnn.predictions)}\n",
    "\n",
    "        prediction_signature = tf.saved_model.signature_def_utils.build_signature_def(inputs=inputs, outputs=outputs,\n",
    "                                                                                      method_name=tf.saved_model.signature_constants.PREDICT_METHOD_NAME)\n",
    "        legacy_init_op = tf.group(tf.tables_initializer(), name=\"legacy_init_op\")\n",
    "        builder.add_meta_graph_and_variables(sess, [tf.saved_model.tag_constants.SERVING],\n",
    "                                             signature_def_map={\"predict\": prediction_signature},\n",
    "                                             legacy_init_op=legacy_init_op)\n",
    "\n",
    "        builder.save()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
