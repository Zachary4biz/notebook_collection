{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-21T08:17:02.465177Z",
     "start_time": "2019-10-21T08:17:02.199887Z"
    },
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "%matplotlib inline\n",
    "from tqdm.auto import tqdm\n",
    "import concurrent.futures\n",
    "from multiprocessing import Pool\n",
    "import copy,os,sys,psutil\n",
    "from collections import Counter,deque\n",
    "import numpy as np\n",
    "from zac_pyutils.ExqUtils import zprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-21T08:17:03.740406Z",
     "start_time": "2019-10-21T08:17:02.466837Z"
    },
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import json\n",
    "import re\n",
    "import itertools\n",
    "import pickle\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-21T08:17:06.728539Z",
     "start_time": "2019-10-21T08:17:03.742199Z"
    },
    "init_cell": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tensorflow.python.client.session.Session object at 0x7fd412caebe0>\n"
     ]
    }
   ],
   "source": [
    "# ÂÖÅËÆ∏GPUÊ∏êËøõÂç†Áî®\n",
    "sess_conf = tf.ConfigProto()\n",
    "sess_conf.gpu_options.allow_growth = True  # ÂÖÅËÆ∏GPUÊ∏êËøõÂç†Áî®\n",
    "sess_conf.allow_soft_placement = True  # Êää‰∏çÈÄÇÂêàGPUÁöÑÊîæÂà∞CPU‰∏äË∑ë\n",
    "with tf.Session(config=sess_conf) as sess:\n",
    "    print(sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-21T08:17:06.732370Z",
     "start_time": "2019-10-21T08:17:06.730082Z"
    },
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "# sess.run(.. options=run_opt)ÂèØ‰ª•Âú®OOMÁöÑÊó∂ÂÄôÊèê‰æõÂΩìÂâçÂ∑≤ÁªèÂ£∞Êòé‰∫ÜÁöÑÂèòÈáè\n",
    "run_opt = tf.RunOptions()\n",
    "run_opt.report_tensor_allocations_upon_oom = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ËØïËØïÁúãËÉΩ‰∏çËÉΩyieldÊñπÂºèÊûÑÈÄ†Âá∫ÂçïËØçÁ¥¢Âºï\n",
    "- Ë¶ÅË∑üÂêéÈù¢ÂéªÊ≠£Êñá‰ΩøÁî®Áõ∏ÂêåÁöÑ `load_f` Âä†ËΩΩÊñπÂºèÔºàÁõ∏ÂêåÁöÑÈ¢ÑÂ§ÑÁêÜÔºâ\n",
    "üëåÂ∑≤ÂÆåÊàê"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-21T07:05:12.609004Z",
     "start_time": "2019-10-21T07:05:12.599500Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('fp: ',\n",
       " '/home/zhoutong/NLP/data/labeled_timeliness_region_taste_emotion_sample.json.bak')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "('result_set_fp: ',\n",
       " '/home/zhoutong/NLP/data/labeled_timeliness_region_taste_emotion_sample.json.bak_char2idx')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "('coded_article_fp: ',\n",
       " '/home/zhoutong/NLP/data/labeled_timeliness_region_taste_emotion_sample.json.bak_encoded_article.pkl')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# data_name = \"labeled_timeliness_region_taste_emotion_sample.json.bak.head1k\"\n",
    "data_name = \"labeled_timeliness_region_taste_emotion_sample.json.bak\"\n",
    "fp = \"/home/zhoutong/NLP/data/{}\".format(data_name)\n",
    "result_set_fp = \"/home/zhoutong/NLP/data/{}_char2idx\".format(data_name)\n",
    "coded_article_fp = \"/home/zhoutong/NLP/data/{}_encoded_article.pkl\".format(data_name)\n",
    "\n",
    "\"fp: \",fp\n",
    "\"result_set_fp: \", result_set_fp\n",
    "\"coded_article_fp: \", coded_article_fp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-21T07:05:12.781665Z",
     "start_time": "2019-10-21T07:05:12.778393Z"
    }
   },
   "outputs": [],
   "source": [
    "def load_f(fp_inp):\n",
    "    with open(fp_inp,\"r\") as f:\n",
    "        for line in f:\n",
    "            title = json.loads(line)['title']\n",
    "            text = json.loads(line)['text']\n",
    "            text = re.sub(\"[\\\\n]+\", \"\\\\n\",text)\n",
    "            yield text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-23T10:43:39.584552Z",
     "start_time": "2019-09-23T10:42:12.760043Z"
    },
    "code_folding": [
     10,
     24
    ],
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "   \n",
    "def transform(text_inp):\n",
    "    \"\"\"\n",
    "    ËøôÈáåÊòØÊääÂêÑ‰∏™Ê†áÁÇπÁ¨¶Âè∑ÈÉΩÂâçÂêéÂä†‰∏äÁ©∫Ê†ºÂàÜÂºÄÔºå‰∏çÁ°ÆÂÆöËøôÊ†∑ÊòØÂê¶ÂèØ‰ª•Â¢ûÂä†ÊñáÊú¨ÁîüÊàêÊó∂ÂØπÊ†áÁÇπÁöÑÂáÜÁ°ÆË°®Á§∫\n",
    "    ÁêÜËÆ∫‰∏äÂú®Âª∫Á´ãÁ¥¢ÂºïÁöÑÊó∂ÂÄôË°®ÂæÅËøáÁöÑÂÖÉÁ¥†Ôºà‰æãÂ¶Ç\"\\n\"Á¥¢Âºï‰∏∫0ÔºâÂ∞±ÊúâÂèØËÉΩÊÄß\n",
    "    ‰ΩÜÊòØ‰∏çÂàÜÂºÄÔºåÁõ¥Êé•Êää \"you!\"(idx=11) ÂΩì‰Ωú‰∏Ä‰∏™Êñ∞ÁöÑÊï¥‰ΩìËÄå‰∏çÊòØ \"you\"(idx=9) Âíå \"!\"(idx=10) ÂèØËÉΩ‰πüË°å\n",
    "    \"\"\"\n",
    "    for t in [\"\\\\n\",\", \"]:\n",
    "        text_inp = re.sub(t, \" \"+t+\" \",text_inp)\n",
    "    text_inp = re.sub(\"\\. \",\" . \",text_inp) # \".\" ‰∏çÂ•ΩÁõ¥Êé•ÊîæÂú®Âæ™ÁéØ‰∏≠‰∏ÄËµ∑ÂÅöÔºåËßÑÁü©‰∏çÂ§™‰∏ÄÊ†∑ÂçïÁã¨ÂÅö‰∫Ü \n",
    "    return text_inp\n",
    "\n",
    "\n",
    "text_g = load_f(fp)\n",
    "result_set = set()\n",
    "while True:\n",
    "    chunk = list(itertools.islice(text_g,10000))\n",
    "    if len(chunk) > 0:\n",
    "        for text in chunk:\n",
    "            # ‰∏ç‰ΩøÁî®transform\n",
    "            # text = transform(text)\n",
    "            result_set.update(text.replace(\"\\n\",\" \\n \").strip().split(\" \"))\n",
    "    else:\n",
    "        result_set = [i for i in result_set if i != \"\"]\n",
    "        break\n",
    "\n",
    "\n",
    "import pickle\n",
    "result_set_d = dict([(word,idx) for idx,word in enumerate(result_set)])\n",
    "with open(result_set_fp+\".pickle\",\"wb+\") as f:\n",
    "    pickle.dump(result_set_d,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-14T10:38:56.502159Z",
     "start_time": "2019-10-14T10:38:55.022817Z"
    },
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "with open(result_set_fp+\".pickle\",\"rb+\") as f:\n",
    "    word2idx_dict = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-14T10:39:01.439408Z",
     "start_time": "2019-10-14T10:39:01.436223Z"
    },
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "list(itertools.islice(word2idx_dict.items(),10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ÂÆûÈ™åÊÄßË¥® | ÁúãÁúãÂá∫Êù•ÁöÑÁªìÊûúÂØπ‰∏çÂØπ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-11T06:25:01.374999Z",
     "start_time": "2019-10-11T06:25:00.633773Z"
    },
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "\n",
    "text_g = load_f(fp)\n",
    "wordsIdx = deque()\n",
    "stopCnt = 0\n",
    "while True:\n",
    "    chunk = list(itertools.islice(text_g,10000))\n",
    "    if len(chunk) > 0:\n",
    "        for text in chunk:\n",
    "            words = text.replace(\"\\n\",\" \\n \").strip().split(\" \")\n",
    "            words = [i for i in words if i != \"\"]\n",
    "            wordsIdx.append([word2idx_dict[w] for w in words])\n",
    "            print(\">>>\", words[:10])\n",
    "            for i in list(itertools.islice(wordsIdx,10)):\n",
    "                print(i[:10])  # ÊØèÊ¨°ÈÉΩÊâìÂç∞wordsIdxÁöÑtop10ÊÆµËêΩÁöÑtop10‰∏™ËØç\n",
    "            stopCnt += 1\n",
    "            assert stopCnt<=5\n",
    "    else:\n",
    "        result_set = [i for i in result_set if i != \"\"]\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ÊñáÁ´†ÊõøÊç¢ÊàêwordÁ¥¢Âºï\n",
    "- ËøôÈáåÊØèÁØáÊñáÁ´†ÈÉΩÊòØ‰∏Ä‰∏™ÂçïÁã¨ÁöÑÊï∞ÁªÑ`append`Âà∞`wordsIdx`Èáå\n",
    "- Ëøô‰∏™‰∫åÁª¥Êï∞ÁªÑÂ≠ònpyÊñá‰ª∂Â§™Â§ß‰∫ÜÔºåËΩ¨Êàê‰∫åÁª¥listÂ≠ò\n",
    "    - npy: 5.1G | deque_pkl: 3.2G | list_pkl: 3.2G\n",
    "    - Áõ¥Êé•‰ª•dequeÂ≠òÂíåËΩ¨ÊàêlistÂ≠òÂç†Áî®Á©∫Èó¥Áõ∏Âêå\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-15T01:33:25.945343Z",
     "start_time": "2019-10-15T01:30:43.909912Z"
    },
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "\n",
    "text_g = load_f(fp)\n",
    "wordsIdx = deque()\n",
    "with tqdm() as pbar:\n",
    "    while True:\n",
    "        chunk = list(itertools.islice(text_g,10000))\n",
    "        if len(chunk) > 0:\n",
    "            for text in chunk:\n",
    "                words = text.replace(\"\\n\",\" \\n \").strip().split(\" \")\n",
    "                words = [i for i in words if i != \"\"]\n",
    "                words_idx = ([word2idx_dict[w] for w in words]+[-1]*1024)[:300]  # ÊØèÁØáÊñáÁ´†ÊúÄÂ§öÂèñ1024‰∏™ËØç\n",
    "                wordsIdx.append(words_idx)\n",
    "                pbar.update(1)\n",
    "        else:\n",
    "            break\n",
    "with open(coded_article_fp,\"wb+\") as fwb:\n",
    "    pickle.dump(list(wordsIdx),fwb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-15T01:40:51.737070Z",
     "start_time": "2019-10-15T01:40:33.488827Z"
    },
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "with open(coded_article_fp,\"rb+\") as frb:\n",
    "    coded_article = pickle.load(frb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CharRNN Âü∫‰∫éÂ≠óÁ¨¶"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Êê≠Âª∫Ê®°Âûã"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ËæìÂÖ•Â±Ç"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-21T08:17:06.744970Z",
     "start_time": "2019-10-21T08:17:06.733593Z"
    },
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "def build_inputs(batch_size, num_steps):\n",
    "    '''\n",
    "    ÊûÑÂª∫ËæìÂÖ•Â±Ç\n",
    "    \n",
    "    batch_size: ÊØè‰∏™batch‰∏≠ÁöÑÂ∫èÂàó‰∏™Êï∞\n",
    "    num_steps: ÊØè‰∏™Â∫èÂàóÂåÖÂê´ÁöÑÂ≠óÁ¨¶Êï∞\n",
    "    '''\n",
    "    inputs = tf.placeholder(tf.int32, shape=(batch_size, num_steps), name='inputs')\n",
    "    targets = tf.placeholder(tf.int32, shape=(batch_size, num_steps), name='targets')\n",
    "    \n",
    "    # Âä†ÂÖ•keep_prob\n",
    "    keep_prob = tf.placeholder(tf.float32, name='keep_prob')\n",
    "    \n",
    "    return inputs, targets, keep_prob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM\n",
    "- `BasicLSTMCell` ÊõøÊç¢‰∏∫ `LSTMCell` \n",
    "- LSTMÈúÄË¶ÅÁü•ÈÅì `batch_size` Âè™ÊòØÁî®Êù•ÂÅöÂÖ®Èõ∂ÂàùÂßãÂåñÊó∂ÈúÄË¶ÅÁü•ÈÅìÁª¥Â∫¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-21T08:17:06.751913Z",
     "start_time": "2019-10-21T08:17:06.746512Z"
    },
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "def build_lstm(lstm_size, num_layers, batch_size, keep_prob):\n",
    "    ''' \n",
    "    ÊûÑÂª∫lstmÂ±Ç\n",
    "        \n",
    "    keep_prob\n",
    "    lstm_size: lstmÈöêÂ±Ç‰∏≠ÁªìÁÇπÊï∞ÁõÆ\n",
    "    num_layers: lstmÁöÑÈöêÂ±ÇÊï∞ÁõÆ\n",
    "    batch_size: batch_size\n",
    "\n",
    "    '''\n",
    "    def construct_cell(node_size):\n",
    "        # ÊûÑÂª∫‰∏Ä‰∏™Âü∫Êú¨lstmÂçïÂÖÉ\n",
    "        lstm = tf.nn.rnn_cell.LSTMCell(node_size)\n",
    "        # Ê∑ªÂä†dropout\n",
    "        drop = tf.nn.rnn_cell.DropoutWrapper(lstm, output_keep_prob=keep_prob)\n",
    "        return drop\n",
    "    \n",
    "    # Â†ÜÂè†\n",
    "    cell = tf.nn.rnn_cell.MultiRNNCell([construct_cell(lstm_size) for _ in range(num_layers)])\n",
    "    initial_state = cell.zero_state(batch_size, tf.float32)\n",
    "    \n",
    "    return cell, initial_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ËæìÂá∫Â±Ç\n",
    "- `tf.concat(1,lstm_output)` ÊõøÊç¢‰∏∫ `tf.concat(lstm_output,1)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-21T08:17:06.758630Z",
     "start_time": "2019-10-21T08:17:06.753287Z"
    },
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "def build_output(lstm_output, in_size, out_size):\n",
    "    ''' \n",
    "    ÊûÑÈÄ†ËæìÂá∫Â±Ç\n",
    "        \n",
    "    lstm_output: lstmÂ±ÇÁöÑËæìÂá∫ÁªìÊûú\n",
    "    in_size: lstmËæìÂá∫Â±ÇÈáçÂ°ëÂêéÁöÑsize\n",
    "    out_size: softmaxÂ±ÇÁöÑsize\n",
    "    \n",
    "    '''\n",
    "\n",
    "    # Â∞ÜlstmÁöÑËæìÂá∫ÊåâÁÖßÂàóconcateÔºå‰æãÂ¶Ç[[1,2,3],[7,8,9]],\n",
    "    # tf.concatÁöÑÁªìÊûúÊòØ[1,2,3,7,8,9]\n",
    "    seq_output = tf.concat(lstm_output, 1) # tf.concat(concat_dim, values)\n",
    "    # reshape\n",
    "    x = tf.reshape(seq_output, [-1, in_size])\n",
    "    tf.summary.histogram('seq_output_reshape',x)\n",
    "    \n",
    "    # Â∞ÜlstmÂ±Ç‰∏ésoftmaxÂ±ÇÂÖ®ËøûÊé•\n",
    "    with tf.variable_scope('softmax'):\n",
    "        softmax_w = tf.Variable(tf.truncated_normal([in_size, out_size], stddev=0.1))\n",
    "        softmax_b = tf.Variable(tf.zeros(out_size))\n",
    "    tf.summary.histogram(\"softmax_w\",softmax_w)\n",
    "    tf.summary.histogram(\"softmax_b\",softmax_b)\n",
    "    \n",
    "    # ËÆ°ÁÆólogits\n",
    "    logits = tf.matmul(x, softmax_w) + softmax_b\n",
    "    \n",
    "    # softmaxÂ±ÇËøîÂõûÊ¶ÇÁéáÂàÜÂ∏É\n",
    "    out = tf.nn.softmax(logits, name='predictions')\n",
    "    tf.summary.histogram('pred',out)\n",
    "    \n",
    "    return out, logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ËØØÂ∑Æ\n",
    "- `softmax_cross_entropy_with_logits` ÊõøÊç¢‰∏∫ `softmax_cross_entropy_with_logits_v2`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-21T08:17:06.765299Z",
     "start_time": "2019-10-21T08:17:06.759989Z"
    },
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "def build_loss(logits, targets, lstm_size, num_classes):\n",
    "    '''\n",
    "    Ê†πÊçÆlogitsÂíåtargetsËÆ°ÁÆóÊçüÂ§±\n",
    "    \n",
    "    logits: ÂÖ®ËøûÊé•Â±ÇÁöÑËæìÂá∫ÁªìÊûúÔºà‰∏çÁªèËøásoftmaxÔºâ\n",
    "    targets: targets\n",
    "    lstm_size\n",
    "    num_classes: vocab_size\n",
    "        \n",
    "    '''\n",
    "    \n",
    "    # One-hotÁºñÁ†Å\n",
    "    y_one_hot = tf.one_hot(targets, num_classes)\n",
    "    y_reshaped = tf.reshape(y_one_hot, logits.get_shape())\n",
    "    \n",
    "    # Softmax cross entropy loss\n",
    "    loss = tf.nn.softmax_cross_entropy_with_logits_v2(logits=logits, labels=y_reshaped)\n",
    "    loss = tf.reduce_mean(loss)\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ‰ºòÂåñÂô®"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-21T08:17:06.772109Z",
     "start_time": "2019-10-21T08:17:06.766639Z"
    },
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "def build_optimizer(loss, learning_rate, grad_clip):\n",
    "    ''' \n",
    "    ÊûÑÈÄ†Optimizer\n",
    "   \n",
    "    loss: ÊçüÂ§±\n",
    "    learning_rate: Â≠¶‰π†Áéá\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    # ‰ΩøÁî®clipping gradients\n",
    "    tvars = tf.trainable_variables()\n",
    "    grads, _ = tf.clip_by_global_norm(tf.gradients(loss, tvars), grad_clip)\n",
    "    for g in grads:\n",
    "        tf.summary.histogram(g.name, g)\n",
    "    train_op = tf.train.AdamOptimizer(learning_rate)\n",
    "    optimizer = train_op.apply_gradients(zip(grads, tvars))\n",
    "    \n",
    "    return optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ê®°Âûã\n",
    "- ‰ΩøÁî® `placeholder` Êõø‰ª£Âõ∫ÂÆöÁöÑsizeÂíåsteps\n",
    "- ÂÜÖÈÉ®Êñ∞Âª∫‰∏ÄÂº†ËÆ°ÁÆóÂõæËÄå‰∏çÊòØ‰ΩøÁî®resetÂêéÁöÑÈªòËÆ§ËÆ°ÁÆóÂõæ\n",
    "- Â¢ûÂä†summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-22T03:50:13.488719Z",
     "start_time": "2019-10-22T03:50:13.474524Z"
    },
    "code_folding": [
     38
    ],
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "class CharRNN:\n",
    "    def __init__(self, num_classes, batch_size=64, num_steps=50, \n",
    "                       lstm_size=128, num_layers=2, learning_rate=0.001, \n",
    "                       grad_clip=5, summary_path=None, sampling=False):\n",
    "    \n",
    "        batch_size, num_steps = batch_size, num_steps\n",
    "        \n",
    "        # Êñ∞Âª∫‰∏ÄÂº†Âõæ\n",
    "        self.graph = tf.Graph()\n",
    "        \n",
    "        with self.graph.as_default():\n",
    "            # ËæìÂÖ•Â±Ç\n",
    "            self.inputs, self.targets, self.keep_prob = build_inputs(batch_size, num_steps)\n",
    "\n",
    "            # LSTMÂ±Ç\n",
    "            cell, self.initial_state = build_lstm(lstm_size, num_layers, batch_size, self.keep_prob)\n",
    "\n",
    "            # ÂØπËæìÂÖ•ËøõË°åone-hotÁºñÁ†Å\n",
    "            x_one_hot = tf.one_hot(self.inputs, num_classes)\n",
    "\n",
    "            # ËøêË°åRNN\n",
    "            outputs, state = tf.nn.dynamic_rnn(cell, x_one_hot, initial_state=self.initial_state)\n",
    "            self.final_state = state\n",
    "\n",
    "            # È¢ÑÊµãÁªìÊûú\n",
    "            self.prediction, self.logits = build_output(outputs, lstm_size, num_classes)\n",
    "\n",
    "            # Loss Âíå optimizer (with gradient clipping)\n",
    "            self.loss = build_loss(self.logits, self.targets, lstm_size, num_classes)\n",
    "            self.optimizer = build_optimizer(self.loss, learning_rate, grad_clip)\n",
    "\n",
    "            # summary\n",
    "            tf.summary.scalar(\"loss\", self.loss)\n",
    "            #    lstmÁöÑvariablesÂú®dynamic_run‰πãÂêéÊâç‰ºöÊúâÂÄº‰∏çÁÑ∂ÊòØÁ©∫ÁöÑlist\n",
    "            for idx,tensor in enumerate(cell.variables):\n",
    "                if idx % 2 == 0:\n",
    "                    _ = tf.summary.histogram(f\"lstm_kernel_{idx}\",tensor)\n",
    "                else:\n",
    "                    _ = tf.summary.histogram(f\"lstm_bias_{idx}\",tensor)\n",
    "            self.merge_summary = tf.summary.merge_all()\n",
    "            self.writer = tf.summary.FileWriter(summary_path, self.graph) if summary_path is not None else None\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ÊñáÊú¨ÁºñÁ†Å"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Êï∞ÊçÆÊåáÂÆö"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-21T08:17:06.793582Z",
     "start_time": "2019-10-21T08:17:06.781464Z"
    },
    "init_cell": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('fp: ', '/home/zhoutong/NLP/data/anna.txt')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "('result_set_fp: ', '/home/zhoutong/NLP/data/anna.txt_char2idx')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "('coded_article_fp: ', '/home/zhoutong/NLP/data/anna.txt_encoded_article.npy')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def load_f(fp_inp):\n",
    "    with open(fp_inp,\"r\") as f:\n",
    "        for line in f:\n",
    "            yield line\n",
    "\n",
    "data_name = \"anna.txt\"\n",
    "fp = \"/home/zhoutong/NLP/data/{}\".format(data_name)\n",
    "result_set_fp = \"/home/zhoutong/NLP/data/{}_char2idx\".format(data_name)\n",
    "coded_article_fp = \"/home/zhoutong/NLP/data/{}_encoded_article.npy\".format(data_name)\n",
    "\n",
    "\"fp: \",fp\n",
    "\"result_set_fp: \", result_set_fp\n",
    "\"coded_article_fp: \", coded_article_fp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### char2idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-15T06:50:26.055470Z",
     "start_time": "2019-10-15T06:50:25.903639Z"
    },
    "code_folding": [],
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "text_g = load_f(fp)\n",
    "result_set = set()\n",
    "while True:\n",
    "    chunk = list(itertools.islice(text_g,10000))\n",
    "    if len(chunk) > 0:\n",
    "        for text in chunk:\n",
    "            # ‰∏ç‰ΩøÁî®transform\n",
    "            # text = transform(text)\n",
    "            result_set.update(list(text))\n",
    "    else:\n",
    "        result_set = [i for i in result_set if i != \"\"]\n",
    "        break\n",
    "\n",
    "\n",
    "import pickle\n",
    "result_set_d = dict([(word,idx) for idx,word in enumerate(result_set)])\n",
    "with open(result_set_fp+\".pickle\",\"wb+\") as f:\n",
    "    pickle.dump(result_set_d,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-21T08:17:06.801609Z",
     "start_time": "2019-10-21T08:17:06.794942Z"
    },
    "init_cell": true,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('3', 0),\n",
       " ('2', 1),\n",
       " ('V', 2),\n",
       " (';', 3),\n",
       " ('i', 4),\n",
       " ('o', 5),\n",
       " ('&', 6),\n",
       " ('P', 7),\n",
       " ('Y', 8),\n",
       " ('/', 9)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "83"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(result_set_fp+\".pickle\",\"rb+\") as frb:\n",
    "    char2idx = pickle.load(frb)\n",
    "list(itertools.islice(char2idx.items(),10))\n",
    "len(char2idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### idx2char"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-21T08:17:06.812093Z",
     "start_time": "2019-10-21T08:17:06.802909Z"
    },
    "init_cell": true,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, '3'),\n",
       " (1, '2'),\n",
       " (2, 'V'),\n",
       " (3, ';'),\n",
       " (4, 'i'),\n",
       " (5, 'o'),\n",
       " (6, '&'),\n",
       " (7, 'P'),\n",
       " (8, 'Y'),\n",
       " (9, '/')]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "83"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx2char={v:k for k,v in char2idx.items()}\n",
    "list(itertools.islice(idx2char.items(),10))\n",
    "len(idx2char)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### encoded (doc2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-21T07:11:02.202881Z",
     "start_time": "2019-10-21T07:11:01.364139Z"
    },
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e29a99672654b12b27a77296eb07577",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1985223), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "with open(fp,\"r+\") as fr:\n",
    "    text = fr.read()\n",
    "\n",
    "encoded = np.array([char2idx[c] for c in tqdm(text)])\n",
    "np.save(coded_article_fp,encoded)  # 14G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-21T08:17:06.828330Z",
     "start_time": "2019-10-21T08:17:06.813395Z"
    },
    "init_cell": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1985223,)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "array([29., 25., 63., 62., 42., 56., 48., 49., 64., 28.], dtype=float32)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded = np.load(coded_article_fp).astype(np.float32)\n",
    "encoded.shape\n",
    "encoded[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### get_batchesÂáΩÊï∞"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-21T08:17:06.832677Z",
     "start_time": "2019-10-21T08:17:06.829695Z"
    },
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "def get_batches(encoded, batch_size, n_steps, verbose=False):\n",
    "    chunk_len = batch_size*n_steps \n",
    "    n_chunk = int(len(encoded)/chunk_len)\n",
    "    arr = encoded[:chunk_len*n_chunk]  # Êà™ÂèñÊï¥Êï∞ÂÄçÁöÑbatch_size\n",
    "    arr = arr.reshape((batch_size,-1))\n",
    "\n",
    "    for n in range(0, arr.shape[1], n_steps):\n",
    "        x = arr[:, n:n+n_steps]\n",
    "        y = np.zeros_like(x)\n",
    "        y[:, :-1], y[:, -1] = x[:,1:], y[:, 0]  # ËøôÈáåÂ∫îËØ•ÊúâÈóÆÈ¢òÔºåÊúÄÂêéy[:, 0]Â∫îËØ•ÊîπÊàê‰ªéÂèñÂêé‰∏Ä‰∏™ÊâçÂØπ‰∏∫‰ªÄ‰πàÊòØÂèà‰ªé0ÂºÄÂßãÂèñ\n",
    "        yield x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ‰ª•‰∏ãÊòØÂØπget_batchesÂáΩÊï∞ÁöÑ‰∏Ä‰∏™È™åËØÅ\n",
    "ËøôÈáåÂÖ∂ÂÆûÊòØÊääÊï¥‰∏™ÊñáÊú¨ËØ≠ÊñôÊåâ„ÄåÂ≠óÁ¨¶„Äç‰Ωú‰∏∫Âçï‰ΩçÂàáÂàÜbatchÔºåÂÆåÂÖ®ËàçÂºÉ‰∫Ü„ÄåËØç„ÄçÁöÑÊ¶ÇÂøµ\n",
    "\n",
    "‰æãÂ¶Ç\"I come from China\"ËøõË°åget_batches\n",
    "- `batch_size=3,n_steps=4` ËØ¥ÊòéËøô‰∏™batchÈáå**Êúâ3‰∏™Ê†∑Êú¨ÔºàÂè•Â≠êÔºâÔºåÊØè‰∏™Ê†∑Êú¨Êó∂Èó¥Ê≠•ÈïøÔºàÂ≠óÁ¨¶Êï∞ÔºâÊòØ4**\n",
    "- ËøôÊó∂‰ºöËÆ°ÁÆóËøô‰∏ÄÂÖ±ÊòØÂ§öÂ∞ë‰∏™Â≠óÁ¨¶Ôºö3x4=12\n",
    "- ÂÜçËÆ°ÁÆóÊï¥‰∏™Âè•Â≠êÊîØÊåÅÂ§öÂ∞ë‰∏™batch`n_chunk = int(len(encoded)/chunk_len)`ÔºåÊää‰ΩôÊï∞ÂéªÊéâ\n",
    "- Ê≠§ÂêéÊØèÊ¨°ÈÉΩÁî®`[:, n:n+n_steps]`Êù•Ëø≠‰ª£Âèñ‰∏Ä‰∏™batchÁöÑÊï∞ÊçÆ\n",
    "- Ëøô‰∏™‰æãÂè•‰∏≠ÂàöÂ•ΩÂà∞'I come from 'ÊòØ12ÔºåÂêéÈù¢ÁöÑÂ∞±Ë¢´ÂΩì‰ΩôÊï∞ÂéªÊéâ‰∫Ü\n",
    "- ÂæóÂà∞ÁöÑbatchÂ¶Ç‰∏ãÁ§∫‰æã"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-17T06:45:45.677211Z",
     "start_time": "2019-10-17T06:45:45.667209Z"
    },
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> ÊµãËØïÊñáÊú¨Â§ü„Äå2„Äç‰∏™chunkÔºå‰Ωô‰∏ãË¢´Êà™Êñ≠‰∏¢ÂºÉ‰∫Ü\n",
      "  -ÊâÄ‰ª•ÂÆûÈôÖ‰ΩøÁî®ÁöÑÊµãËØïÊñáÊú¨ÊòØ:'I am from Chaoyang Beiji'\n",
      "  -‰∏¢ÂºÉÁöÑÈÉ®ÂàÜÊòØ          :'ng China'\n",
      ">>> get_batchesÂáΩÊï∞ÈáåÂØπÊà™Êñ≠ÂêéÁöÑarrËøòÂÅö‰∫Ü‰∏™`reshape((batchSize,-1))`ÔºåÊïàÊûúÊòØ:(3, 8)\n",
      " [['I' ' ' 'a' 'm' ' ' 'f' 'r' 'o']\n",
      " ['m' ' ' 'C' 'h' 'a' 'o' 'y' 'a']\n",
      " ['n' 'g' ' ' 'B' 'e' 'i' 'j' 'i']]\n",
      "ËøôÊ†∑ÂêéÈù¢Âú®ÂØπarrÂèñÁ¥¢Âºï [:, n:n+n_steps] Êó∂ÔºåÂÖ∂ÂÆûÊòØÔºöÁ¨¨0‰∏™batchÊòØ‰ªéÊØèË°åÈÉΩÂèñÁ¨¨0ÊâπÁöÑ n_steps ‰∏™ÂÖÉÁ¥†\n",
      "ËøôÊ†∑ÁúãËµ∑Êù•‰∏Ä‰∏™batchÈáåÁöÑÂá†‰∏™ËÆ≠ÁªÉÊ†∑Êú¨ÔºàËÆ≠ÁªÉsequenceÔºâ‰πãÈó¥Âπ∂‰∏çÊòØËøûÁª≠ÁöÑÔºå‰ΩÜÊòØÂπ∂‰∏çÂΩ±ÂìçÔºåÊ†∑Êú¨ÂÜÖÁöÑsequenceÊòØËøûÁª≠ÁöÑÂ∞±Ë°åÔºàÂç≥Ê†∑Êú¨ËøòÊòØÊ≠£Á°ÆÈ°∫Â∫èÁöÑÂ≠óÁ¨¶Ôºâ\n",
      "\n",
      "*****ËøôÈáåyÂèñÁöÑÂ∫îÊúâÈóÆÈ¢òÔºåÊØè‰∏™ËÆ≠ÁªÉÊ†∑Êú¨ÁöÑÊúÄÂêé‰∏Ä‰∏™yÂ•ΩÂÉèÊòØÈîôÁöÑ*****\n",
      "\n",
      ">>> Âú®Á¨¨0‰∏™batchÈáå\n",
      "x:\n",
      " [['I' ' ' 'a' 'm']\n",
      " ['m' ' ' 'C' 'h']\n",
      " ['n' 'g' ' ' 'B']]\n",
      "y (xÁöÑÂ≠óÁ¨¶ÂæÄÂêéÂª∂‰∏Ä‰∏™):\n",
      " [[' ' 'a' 'm' ' ']\n",
      " [' ' 'C' 'h' ' ']\n",
      " ['g' ' ' 'B' 'g']]\n",
      "\n",
      ">>> Âú®Á¨¨1‰∏™batchÈáå\n",
      "x:\n",
      " [[' ' 'f' 'r' 'o']\n",
      " ['a' 'o' 'y' 'a']\n",
      " ['e' 'i' 'j' 'i']]\n",
      "y (xÁöÑÂ≠óÁ¨¶ÂæÄÂêéÂª∂‰∏Ä‰∏™):\n",
      " [['f' 'r' 'o' 'f']\n",
      " ['o' 'y' 'a' 'o']\n",
      " ['i' 'j' 'i' 'i']]\n"
     ]
    }
   ],
   "source": [
    "testStr = \"I am from Chaoyang Beijing China\"\n",
    "batchSize=3\n",
    "nSteps=4\n",
    "print(f\">>> ÊµãËØïÊñáÊú¨Â§ü„Äå{len(testStr)//(batchSize*nSteps)}„Äç‰∏™chunkÔºå‰Ωô‰∏ãË¢´Êà™Êñ≠‰∏¢ÂºÉ‰∫Ü\")\n",
    "actual_used = testStr[:len(testStr)//(batchSize*nSteps)*(batchSize*nSteps)]  # ÂÆûÈôÖ‰ΩøÁî®ÁöÑÊñáÊú¨ÈÉ®ÂàÜ\n",
    "print(f\"  -ÊâÄ‰ª•ÂÆûÈôÖ‰ΩøÁî®ÁöÑÊµãËØïÊñáÊú¨ÊòØ:'{actual_used}'\")\n",
    "print(f\"  -‰∏¢ÂºÉÁöÑÈÉ®ÂàÜÊòØ          :'{testStr[len(testStr)//(batchSize*nSteps)*(batchSize*nSteps):]}'\")\n",
    "actual_used_reshaped = np.array(list(actual_used)).reshape((batchSize,-1))\n",
    "print(f\">>> get_batchesÂáΩÊï∞ÈáåÂØπÊà™Êñ≠ÂêéÁöÑarrËøòÂÅö‰∫Ü‰∏™`reshape((batchSize,-1))`ÔºåÊïàÊûúÊòØ:{actual_used_reshaped.shape}\\n\",actual_used_reshaped)\n",
    "print(\"ËøôÊ†∑ÂêéÈù¢Âú®ÂØπarrÂèñÁ¥¢Âºï [:, n:n+n_steps] Êó∂ÔºåÂÖ∂ÂÆûÊòØÔºöÁ¨¨0‰∏™batchÊòØ‰ªéÊØèË°åÈÉΩÂèñÁ¨¨0ÊâπÁöÑ n_steps ‰∏™ÂÖÉÁ¥†\")\n",
    "print(\"ËøôÊ†∑ÁúãËµ∑Êù•‰∏Ä‰∏™batchÈáåÁöÑÂá†‰∏™ËÆ≠ÁªÉÊ†∑Êú¨ÔºàËÆ≠ÁªÉsequenceÔºâ‰πãÈó¥Âπ∂‰∏çÊòØËøûÁª≠ÁöÑÔºå‰ΩÜÊòØÂπ∂‰∏çÂΩ±ÂìçÔºåÊ†∑Êú¨ÂÜÖÁöÑsequenceÊòØËøûÁª≠ÁöÑÂ∞±Ë°åÔºàÂç≥Ê†∑Êú¨ËøòÊòØÊ≠£Á°ÆÈ°∫Â∫èÁöÑÂ≠óÁ¨¶Ôºâ\")\n",
    "\n",
    "print(\"\\n*****ËøôÈáåyÂèñÁöÑÂ∫îÊúâÈóÆÈ¢òÔºåÊØè‰∏™ËÆ≠ÁªÉÊ†∑Êú¨ÁöÑÊúÄÂêé‰∏Ä‰∏™yÂ•ΩÂÉèÊòØÈîôÁöÑ*****\")\n",
    "for idx,(x,y) in enumerate(get_batches(np.array(list(testStr)),batch_size=3,n_steps=4)):\n",
    "    print(f\"\\n>>> Âú®Á¨¨{idx}‰∏™batchÈáå\")\n",
    "    print(f\"x:\\n\",x)\n",
    "    print(f\"y (xÁöÑÂ≠óÁ¨¶ÂæÄÂêéÂª∂‰∏Ä‰∏™):\\n\",y)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-15T06:50:35.883583Z",
     "start_time": "2019-10-15T06:50:35.828385Z"
    },
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "text_g = load_f(fp)\n",
    "def get_batches(text_generator, batch_size, time_step, verbose=False):\n",
    "    X_batch,Y_batch = [],[]\n",
    "    X_verbose,Y_verbose = [],[]\n",
    "    chunk = list(itertools.islice(text_generator, batch_size))\n",
    "    for text in chunk:\n",
    "        # ÊØèÊ¨°ÁîüÊàê‰∏ÄÁØáÊñáÁ´†ÁöÑÊ†∑Êú¨ÈÉΩ‰ªéfrom_idxÂºÄÂßãÂèñtime_step‰∏™Â≠óÁ¨¶\n",
    "        from_idx=np.random.randint(len(text)-time_step-1) # from_idxÁî®ÈöèÊú∫Êï∞,ÊúÄÂêéÁöÑ-1ÊòØ‰∏∫‰∫ÜÊääÊúÄÂêé‰∏Ä‰∏™Â≠óÁ¨¶ÁïôÁªôY\n",
    "        text_X = text[from_idx:from_idx+time_step]\n",
    "        text_Y = text[from_idx+1:from_idx+time_step+1]\n",
    "        if verbose:\n",
    "            X_verbose.append(text_X)\n",
    "            Y_verbose.append(text_Y)\n",
    "        X_batch.append([char2idx[char] for char in text_X])\n",
    "        Y_batch.append([char2idx[char] for char in text_Y])\n",
    "    X_batch = np.array(X_batch)\n",
    "    Y_batch = np.array(Y_batch)\n",
    "    if verbose:\n",
    "        return X_batch, Y_batch, np.array(X_verbose), np.array(Y_verbose)\n",
    "    else:\n",
    "        return X_batch, Y_batch\n",
    "\n",
    "get_batches(text_g,20,5,True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ë∂ÖÂèÇ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-21T11:20:17.552786Z",
     "start_time": "2019-10-21T11:20:17.547501Z"
    },
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "batch_size = 100         # Sequences per batch\n",
    "num_steps = 100          # Number of sequence steps per batch\n",
    "lstm_size = 512         # Size of hidden layers in LSTMs\n",
    "num_layers = 2          # Number of LSTM layers\n",
    "learning_rate = 0.01    # Learning rate\n",
    "keep_prob = 0.5         # Dropout keep probability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ËÆ≠ÁªÉ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-21T12:03:04.584685Z",
     "start_time": "2019-10-21T11:20:19.138190Z"
    },
    "code_folding": [
     6,
     11,
     15,
     34
    ],
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Summary name clip_by_global_norm/clip_by_global_norm/_0:0 is illegal; using clip_by_global_norm/clip_by_global_norm/_0_0 instead.\n",
      "INFO:tensorflow:Summary name clip_by_global_norm/clip_by_global_norm/_1:0 is illegal; using clip_by_global_norm/clip_by_global_norm/_1_0 instead.\n",
      "INFO:tensorflow:Summary name clip_by_global_norm/clip_by_global_norm/_2:0 is illegal; using clip_by_global_norm/clip_by_global_norm/_2_0 instead.\n",
      "INFO:tensorflow:Summary name clip_by_global_norm/clip_by_global_norm/_3:0 is illegal; using clip_by_global_norm/clip_by_global_norm/_3_0 instead.\n",
      "INFO:tensorflow:Summary name clip_by_global_norm/clip_by_global_norm/_4:0 is illegal; using clip_by_global_norm/clip_by_global_norm/_4_0 instead.\n",
      "INFO:tensorflow:Summary name clip_by_global_norm/clip_by_global_norm/_5:0 is illegal; using clip_by_global_norm/clip_by_global_norm/_5_0 instead.\n",
      "|2019-10-21 19:20:52| epoch: 1/40 batch: 100/198 err: 2.8633\n",
      "|2019-10-21 19:21:25| epoch: 2/40 batch: 200/198 err: 2.3506\n",
      "|2019-10-21 19:21:57| epoch: 2/40 batch: 300/198 err: 2.1507\n",
      "|2019-10-21 19:22:29| epoch: 3/40 batch: 400/198 err: 1.9443\n",
      "|2019-10-21 19:23:01| epoch: 3/40 batch: 500/198 err: 1.8190\n",
      "|2019-10-21 19:23:33| epoch: 4/40 batch: 600/198 err: 1.6809\n",
      "|2019-10-21 19:24:05| epoch: 4/40 batch: 700/198 err: 1.6413\n",
      "|2019-10-21 19:24:37| epoch: 5/40 batch: 800/198 err: 1.5791\n",
      "|2019-10-21 19:25:10| epoch: 5/40 batch: 900/198 err: 1.5391\n",
      "|2019-10-21 19:25:42| epoch: 6/40 batch: 1000/198 err: 1.5016\n",
      "|2019-10-21 19:26:14| epoch: 6/40 batch: 1100/198 err: 1.4745\n",
      "|2019-10-21 19:26:46| epoch: 7/40 batch: 1200/198 err: 1.4121\n",
      "|2019-10-21 19:27:19| epoch: 7/40 batch: 1300/198 err: 1.4064\n",
      "|2019-10-21 19:27:51| epoch: 8/40 batch: 1400/198 err: 1.4121\n",
      "|2019-10-21 19:28:24| epoch: 8/40 batch: 1500/198 err: 1.3581\n",
      "|2019-10-21 19:28:56| epoch: 9/40 batch: 1600/198 err: 1.3404\n",
      "|2019-10-21 19:29:28| epoch: 9/40 batch: 1700/198 err: 1.3103\n",
      "|2019-10-21 19:30:00| epoch: 10/40 batch: 1800/198 err: 1.3541\n",
      "|2019-10-21 19:30:32| epoch: 10/40 batch: 1900/198 err: 1.3166\n",
      "|2019-10-21 19:31:05| epoch: 11/40 batch: 2000/198 err: 1.3431\n",
      "|2019-10-21 19:31:38| epoch: 11/40 batch: 2100/198 err: 1.2898\n",
      "|2019-10-21 19:32:10| epoch: 12/40 batch: 2200/198 err: 1.3035\n",
      "|2019-10-21 19:32:42| epoch: 12/40 batch: 2300/198 err: 1.2429\n",
      "|2019-10-21 19:33:13| epoch: 13/40 batch: 2400/198 err: 1.2975\n",
      "|2019-10-21 19:33:46| epoch: 13/40 batch: 2500/198 err: 1.2632\n",
      "|2019-10-21 19:34:18| epoch: 14/40 batch: 2600/198 err: 1.2262\n",
      "|2019-10-21 19:34:50| epoch: 14/40 batch: 2700/198 err: 1.2135\n",
      "|2019-10-21 19:35:22| epoch: 15/40 batch: 2800/198 err: 1.2775\n",
      "|2019-10-21 19:35:55| epoch: 15/40 batch: 2900/198 err: 1.2475\n",
      "|2019-10-21 19:36:27| epoch: 16/40 batch: 3000/198 err: 1.2605\n",
      "|2019-10-21 19:37:01| epoch: 16/40 batch: 3100/198 err: 1.1921\n",
      "|2019-10-21 19:37:32| epoch: 17/40 batch: 3200/198 err: 1.2033\n",
      "|2019-10-21 19:38:05| epoch: 17/40 batch: 3300/198 err: 1.2045\n",
      "|2019-10-21 19:38:37| epoch: 18/40 batch: 3400/198 err: 1.2458\n",
      "|2019-10-21 19:39:09| epoch: 18/40 batch: 3500/198 err: 1.2395\n",
      "|2019-10-21 19:39:41| epoch: 19/40 batch: 3600/198 err: 1.2339\n",
      "|2019-10-21 19:40:14| epoch: 19/40 batch: 3700/198 err: 1.2134\n",
      "|2019-10-21 19:40:46| epoch: 20/40 batch: 3800/198 err: 1.1771\n",
      "|2019-10-21 19:41:19| epoch: 20/40 batch: 3900/198 err: 1.2386\n",
      "|2019-10-21 19:41:50| epoch: 21/40 batch: 4000/198 err: 1.2171\n",
      "|2019-10-21 19:42:23| epoch: 21/40 batch: 4100/198 err: 1.2277\n",
      "|2019-10-21 19:42:54| epoch: 22/40 batch: 4200/198 err: 1.2539\n",
      "|2019-10-21 19:43:27| epoch: 22/40 batch: 4300/198 err: 1.2031\n",
      "|2019-10-21 19:43:59| epoch: 23/40 batch: 4400/198 err: 1.1850\n",
      "|2019-10-21 19:44:32| epoch: 23/40 batch: 4500/198 err: 1.2439\n",
      "|2019-10-21 19:45:04| epoch: 24/40 batch: 4600/198 err: 1.1885\n",
      "|2019-10-21 19:45:38| epoch: 24/40 batch: 4700/198 err: 1.2208\n",
      "|2019-10-21 19:46:10| epoch: 25/40 batch: 4800/198 err: 1.1779\n",
      "|2019-10-21 19:46:42| epoch: 25/40 batch: 4900/198 err: 1.2362\n",
      "|2019-10-21 19:47:15| epoch: 26/40 batch: 5000/198 err: 1.2149\n",
      "|2019-10-21 19:47:48| epoch: 26/40 batch: 5100/198 err: 1.1962\n",
      "|2019-10-21 19:48:20| epoch: 27/40 batch: 5200/198 err: 1.2219\n",
      "|2019-10-21 19:48:53| epoch: 27/40 batch: 5300/198 err: 1.1788\n",
      "|2019-10-21 19:49:25| epoch: 28/40 batch: 5400/198 err: 1.2013\n",
      "|2019-10-21 19:49:58| epoch: 28/40 batch: 5500/198 err: 1.1762\n",
      "|2019-10-21 19:50:29| epoch: 29/40 batch: 5600/198 err: 1.1834\n",
      "|2019-10-21 19:51:02| epoch: 29/40 batch: 5700/198 err: 1.1720\n",
      "|2019-10-21 19:51:35| epoch: 30/40 batch: 5800/198 err: 1.1639\n",
      "|2019-10-21 19:52:07| epoch: 30/40 batch: 5900/198 err: 1.1595\n",
      "|2019-10-21 19:52:39| epoch: 31/40 batch: 6000/198 err: 1.2131\n",
      "|2019-10-21 19:53:12| epoch: 31/40 batch: 6100/198 err: 1.1958\n",
      "|2019-10-21 19:53:44| epoch: 32/40 batch: 6200/198 err: 1.2206\n",
      "|2019-10-21 19:54:17| epoch: 32/40 batch: 6300/198 err: 1.1891\n",
      "|2019-10-21 19:54:49| epoch: 33/40 batch: 6400/198 err: 1.1737\n",
      "|2019-10-21 19:55:23| epoch: 33/40 batch: 6500/198 err: 1.1731\n",
      "|2019-10-21 19:55:54| epoch: 34/40 batch: 6600/198 err: 1.1781\n",
      "|2019-10-21 19:56:27| epoch: 34/40 batch: 6700/198 err: 1.1775\n",
      "|2019-10-21 19:56:59| epoch: 35/40 batch: 6800/198 err: 1.1540\n",
      "|2019-10-21 19:57:33| epoch: 35/40 batch: 6900/198 err: 1.2193\n",
      "|2019-10-21 19:58:05| epoch: 36/40 batch: 7000/198 err: 1.1478\n",
      "|2019-10-21 19:58:38| epoch: 36/40 batch: 7100/198 err: 1.1843\n",
      "|2019-10-21 19:59:10| epoch: 37/40 batch: 7200/198 err: 1.1748\n",
      "|2019-10-21 19:59:42| epoch: 37/40 batch: 7300/198 err: 1.1525\n",
      "|2019-10-21 20:00:15| epoch: 38/40 batch: 7400/198 err: 1.1342\n",
      "|2019-10-21 20:00:47| epoch: 38/40 batch: 7500/198 err: 1.1779\n",
      "|2019-10-21 20:01:19| epoch: 39/40 batch: 7600/198 err: 1.1644\n",
      "|2019-10-21 20:01:52| epoch: 39/40 batch: 7700/198 err: 1.1320\n",
      "|2019-10-21 20:02:24| epoch: 40/40 batch: 7800/198 err: 1.1567\n",
      "|2019-10-21 20:02:56| epoch: 40/40 batch: 7900/198 err: 1.1945\n"
     ]
    }
   ],
   "source": [
    "epochs = 40\n",
    "# ÊØènËΩÆËøõË°å‰∏ÄÊ¨°ÂèòÈáè‰øùÂ≠ò\n",
    "save_every_n = 200\n",
    "summary_path = './tmp/tensorboard_anna'\n",
    "base_model_path = \"./tmp/lstm_anna/i{}_l{}.ckpt\"\n",
    "\n",
    "model = CharRNN(len(char2idx), batch_size=batch_size, num_steps=num_steps,\n",
    "                lstm_size=lstm_size, num_layers=num_layers, \n",
    "                learning_rate=learning_rate, summary_path=summary_path)\n",
    "\n",
    "# Á¥¢ÂºïËΩ¨ÊàêÂ≠óÁ¨¶\n",
    "def _tochar(i):\n",
    "        return idx2char[i]\n",
    "_tochar_vec = np.vectorize(_tochar)\n",
    "# ÂèñËæìÂÖ•ÁöÑx y predsÁöÑÂ≠óÁ¨¶Êò†Â∞ÑÁªìÊûúÁöÑÁ¨¨‰∏Ä‰∏™Ê†∑Êú¨\n",
    "def get_sample_char(x,y,preds,verbose=False):\n",
    "    # predsÂÖàreshape‰∏Ä‰∏ã\n",
    "    preds_reshape = preds.reshape(batch_size,num_steps,len(char2idx))\n",
    "    preds_argmax = np.array([[np.argmax(each_seq) for each_seq in each_batch] for each_batch in preds_reshape])\n",
    "    x_char,y_char,preds_char = [_tochar_vec(i) for i in [x,y,preds_argmax]]\n",
    "    if verbose:\n",
    "        print(f\"\"\"\n",
    "        >>>preds: {preds.shape}\n",
    "           |_reshape ==> {preds_reshape.shape}\n",
    "             |_argmax ==> {preds_argmax.shape}\n",
    "        \"\"\")\n",
    "\n",
    "        print(f\">>>x:{x.shape}\\n\",x,\"\\n\",x_char)\n",
    "        print(f\">>>y:{y.shape}\\n\",y,\"\\n\",y_char)\n",
    "        print(f\">>>preds_argmax:{preds_argmax.shape}\\n\",preds_argmax,\"\\n\",preds_char)\n",
    "    # ËøôÊ†∑ÂÜô‰πüÊòØ‰∏∫‰∫ÜÈò≤Ê≠¢\\nÂú®printÁöÑÊó∂ÂÄôËá™Âä®ËΩ¨‰πâÊç¢Ë°å | ÊîæÂà∞Êï∞ÁªÑ„ÄÅÂ≠óÂÖ∏ÈáåÂ∞±‰∏ç‰ºöprintÂá∫Êù•Êç¢Ë°å‰∫Ü\n",
    "    res = {\"x\":\"\".join(x_char[0]),\"y\":\"\".join(y_char[0]),\"preds\":\"\".join(preds_char[0])}\n",
    "    return res\n",
    "                \n",
    "def print_control(cnt,info):\n",
    "    if cnt % 100 == 0:\n",
    "        zprint(info)\n",
    "#         if cnt <= 1000:\n",
    "#             # 0~1k ÊØè100ËæìÂá∫‰∏ÄÊ¨°\n",
    "#             zprint(info)\n",
    "#         elif cnt <= 10000:\n",
    "#             # 1k~10kÊØè1kËæìÂá∫‰∏ÄÊ¨°\n",
    "#             if cnt % 1000 == 0:\n",
    "#                 zprint(info)\n",
    "#         else:\n",
    "#             # 1w‰ª•ÂêéÊØè5kËæìÂá∫‰∏ÄÊ¨°\n",
    "#             if cnt % 5000 == 0:\n",
    "#                 zprint(info)\n",
    "\n",
    "\n",
    "with model.graph.as_default():\n",
    "    saver = tf.train.Saver(max_to_keep=100)\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "\n",
    "        counter = 0\n",
    "        for e in range(epochs):\n",
    "            # Train network\n",
    "            new_state = sess.run(model.initial_state)\n",
    "            loss = 0\n",
    "            et_batch_cnt = len(encoded) // (batch_size*num_steps)\n",
    "            all_batch_data = get_batches(encoded, batch_size, num_steps)\n",
    "            for x, y in all_batch_data:\n",
    "                counter += 1\n",
    "                feed = {model.inputs: x,\n",
    "                        model.targets: y,\n",
    "                        model.keep_prob: keep_prob,\n",
    "                        model.initial_state: new_state}\n",
    "                preds,batch_loss, new_state, _, merged_summary = sess.run([model.prediction,\n",
    "                                                                     model.loss, \n",
    "                                                                     model.final_state, \n",
    "                                                                     model.optimizer,\n",
    "                                                                     model.merge_summary,], \n",
    "                                                                     feed_dict=feed)\n",
    "                \n",
    "                # ‰øùÂ≠òËøõÂ±ï\n",
    "                model.writer.add_summary(merged_summary,counter)\n",
    "                # ËæìÂá∫print\n",
    "                info = f\"epoch: {e+1}/{epochs} batch: {counter:0>3d}/{et_batch_cnt} err: {batch_loss:.4f}\"\n",
    "                print_control(counter,info)\n",
    "                # È¢ùÂ§ñËæìÂá∫‰∏Ä‰∏™ÂÆåÊï¥ÁöÑÂ≠óÁ¨¶‰∏≤print\n",
    "                if counter % et_batch_cnt ==0 or counter == 1:\n",
    "                    text_summary_list = [tf.summary.text(k, tf.convert_to_tensor(v)) \n",
    "                                         for k,v in get_sample_char(x,y,preds).items()]\n",
    "                    text_summary = tf.summary.merge(text_summary_list)\n",
    "                    text_summary_ = sess.run(text_summary)\n",
    "                    model.writer.add_summary(text_summary_,counter)\n",
    "                # save model graph\n",
    "                if (counter % save_every_n == 0):\n",
    "                    _=saver.save(sess, base_model_path.format(counter, lstm_size))\n",
    "\n",
    "        _=saver.save(sess, base_model_path.format(counter, lstm_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-21T12:03:05.115280Z",
     "start_time": "2019-10-21T12:03:05.107606Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "model_checkpoint_path: \"./tmp/lstm_anna/i7920_l512.ckpt\"\n",
       "all_model_checkpoint_paths: \"./tmp/lstm_anna/i200_l512.ckpt\"\n",
       "all_model_checkpoint_paths: \"./tmp/lstm_anna/i400_l512.ckpt\"\n",
       "all_model_checkpoint_paths: \"./tmp/lstm_anna/i600_l512.ckpt\"\n",
       "all_model_checkpoint_paths: \"./tmp/lstm_anna/i800_l512.ckpt\"\n",
       "all_model_checkpoint_paths: \"./tmp/lstm_anna/i1000_l512.ckpt\"\n",
       "all_model_checkpoint_paths: \"./tmp/lstm_anna/i1200_l512.ckpt\"\n",
       "all_model_checkpoint_paths: \"./tmp/lstm_anna/i1400_l512.ckpt\"\n",
       "all_model_checkpoint_paths: \"./tmp/lstm_anna/i1600_l512.ckpt\"\n",
       "all_model_checkpoint_paths: \"./tmp/lstm_anna/i1800_l512.ckpt\"\n",
       "all_model_checkpoint_paths: \"./tmp/lstm_anna/i2000_l512.ckpt\"\n",
       "all_model_checkpoint_paths: \"./tmp/lstm_anna/i2200_l512.ckpt\"\n",
       "all_model_checkpoint_paths: \"./tmp/lstm_anna/i2400_l512.ckpt\"\n",
       "all_model_checkpoint_paths: \"./tmp/lstm_anna/i2600_l512.ckpt\"\n",
       "all_model_checkpoint_paths: \"./tmp/lstm_anna/i2800_l512.ckpt\"\n",
       "all_model_checkpoint_paths: \"./tmp/lstm_anna/i3000_l512.ckpt\"\n",
       "all_model_checkpoint_paths: \"./tmp/lstm_anna/i3200_l512.ckpt\"\n",
       "all_model_checkpoint_paths: \"./tmp/lstm_anna/i3400_l512.ckpt\"\n",
       "all_model_checkpoint_paths: \"./tmp/lstm_anna/i3600_l512.ckpt\"\n",
       "all_model_checkpoint_paths: \"./tmp/lstm_anna/i3800_l512.ckpt\"\n",
       "all_model_checkpoint_paths: \"./tmp/lstm_anna/i4000_l512.ckpt\"\n",
       "all_model_checkpoint_paths: \"./tmp/lstm_anna/i4200_l512.ckpt\"\n",
       "all_model_checkpoint_paths: \"./tmp/lstm_anna/i4400_l512.ckpt\"\n",
       "all_model_checkpoint_paths: \"./tmp/lstm_anna/i4600_l512.ckpt\"\n",
       "all_model_checkpoint_paths: \"./tmp/lstm_anna/i4800_l512.ckpt\"\n",
       "all_model_checkpoint_paths: \"./tmp/lstm_anna/i5000_l512.ckpt\"\n",
       "all_model_checkpoint_paths: \"./tmp/lstm_anna/i5200_l512.ckpt\"\n",
       "all_model_checkpoint_paths: \"./tmp/lstm_anna/i5400_l512.ckpt\"\n",
       "all_model_checkpoint_paths: \"./tmp/lstm_anna/i5600_l512.ckpt\"\n",
       "all_model_checkpoint_paths: \"./tmp/lstm_anna/i5800_l512.ckpt\"\n",
       "all_model_checkpoint_paths: \"./tmp/lstm_anna/i6000_l512.ckpt\"\n",
       "all_model_checkpoint_paths: \"./tmp/lstm_anna/i6200_l512.ckpt\"\n",
       "all_model_checkpoint_paths: \"./tmp/lstm_anna/i6400_l512.ckpt\"\n",
       "all_model_checkpoint_paths: \"./tmp/lstm_anna/i6600_l512.ckpt\"\n",
       "all_model_checkpoint_paths: \"./tmp/lstm_anna/i6800_l512.ckpt\"\n",
       "all_model_checkpoint_paths: \"./tmp/lstm_anna/i7000_l512.ckpt\"\n",
       "all_model_checkpoint_paths: \"./tmp/lstm_anna/i7200_l512.ckpt\"\n",
       "all_model_checkpoint_paths: \"./tmp/lstm_anna/i7400_l512.ckpt\"\n",
       "all_model_checkpoint_paths: \"./tmp/lstm_anna/i7600_l512.ckpt\"\n",
       "all_model_checkpoint_paths: \"./tmp/lstm_anna/i7800_l512.ckpt\"\n",
       "all_model_checkpoint_paths: \"./tmp/lstm_anna/i7920_l512.ckpt\""
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tf.train.get_checkpoint_state(\"./tmp/lstm_anna\").all_model_checkpoint_paths)\n",
    "tf.train.get_checkpoint_state(\"./tmp/lstm_anna\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ÁîüÊàê\n",
    "- ÈúÄË¶ÅÊåáÂÆö`n_samples`ÔºöÈúÄË¶ÅÁîüÊàêÂ§öÈïøÁöÑÂ≠óÁ¨¶‰∏≤\n",
    "- Â∞ÜËæìÂÖ•ÁöÑÂçïËØçËΩ¨Êç¢‰∏∫Âçï‰∏™Â≠óÁ¨¶ÁªÑÊàêÁöÑlist\n",
    "- ‰ªéÁ¨¨‰∏Ä‰∏™Â≠óÁ¨¶ÂºÄÂßãËæìÂÖ•CharRNN\n",
    "- ‰ªéÈ¢ÑÊµãÁªìÊûú‰∏≠ÈÄâÂèñÂâçtop_n‰∏™ÊúÄÂèØËÉΩÁöÑÂ≠óÁ¨¶ÔºåÊåâÈ¢ÑÊµãÁªìÊûúÊèê‰æõÁöÑÂêÑ‰∏™Â≠óÁ¨¶ÁöÑÊ¶ÇÁéáËøõË°ånp.random.choice\n",
    " - `pick_top_n`ÈáåÊ∑ªÂä†‰∫Ü`copy()`ÊñπÊ≥ïÔºåÈÅøÂÖçÁõ¥Êé•Êõ¥ÊîπÂèÇÊï∞"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-21T12:03:05.566569Z",
     "start_time": "2019-10-21T12:03:05.558599Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def pick_top_n(preds_, vocab_size, top_n=5, random=False):\n",
    "    \"\"\"\n",
    "    ‰ªéÈ¢ÑÊµãÁªìÊûú‰∏≠ÈÄâÂèñÂâçtop_n‰∏™ÊúÄÂèØËÉΩÁöÑÂ≠óÁ¨¶ÔºåÊåâÈ¢ÑÊµãÁªìÊûúÊèê‰æõÁöÑÂêÑ‰∏™Â≠óÁ¨¶ÁöÑÊ¶ÇÁéáËøõË°ånp.random.choice\n",
    "    \n",
    "    preds_: È¢ÑÊµãÁªìÊûú\n",
    "    vocab_size\n",
    "    top_n\n",
    "    \"\"\"\n",
    "    preds = preds_.copy()  # ÈÅøÂÖçÊîπÂèòÂéüpreds\n",
    "    p = np.squeeze(preds)\n",
    "    # Â∞ÜÈô§‰∫Ütop_n‰∏™È¢ÑÊµãÂÄºÁöÑ‰ΩçÁΩÆÈÉΩÁΩÆ‰∏∫0\n",
    "    p[np.argsort(p)[-top_n:]] = 0\n",
    "    # ÂΩí‰∏ÄÂåñÊ¶ÇÁéá\n",
    "    p = p / np.sum(p)\n",
    "    # ÈöèÊú∫ÈÄâÂèñ‰∏Ä‰∏™Â≠óÁ¨¶ / ÊàñËÄÖÂèñÊ¶ÇÁéáÊúÄÂ§ßÁöÑÂ≠óÁ¨¶\n",
    "    c = np.random.choice(vocab_size, 1, p=p)[0] if random else np.argmax(preds)\n",
    "    return c\n",
    "\n",
    "def sample(checkpoint, n_samples, lstm_size,num_layers, vocab_size, prime=\"The \", random=False):\n",
    "    \"\"\"\n",
    "    ÁîüÊàêÊñ∞ÊñáÊú¨\n",
    "    \n",
    "    checkpoint: Êüê‰∏ÄËΩÆËø≠‰ª£ÁöÑÂèÇÊï∞Êñá‰ª∂\n",
    "    n_sample: Êñ∞ÈóªÊú¨ÁöÑÂ≠óÁ¨¶ÈïøÂ∫¶\n",
    "    lstm_size: ÈöêÂ±ÇÁªìÁÇπÊï∞\n",
    "    vocab_size\n",
    "    prime: Ëµ∑ÂßãÊñáÊú¨\n",
    "    \"\"\"\n",
    "    # Â∞ÜËæìÂÖ•ÁöÑÂçïËØçËΩ¨Êç¢‰∏∫Âçï‰∏™Â≠óÁ¨¶ÁªÑÊàêÁöÑlist\n",
    "    samples = [c for c in prime]\n",
    "    print(f\">>> samples: {samples}\")\n",
    "    # sampling=TrueÊÑèÂë≥ÁùÄbatchÁöÑsize=1 x 1\n",
    "    model = CharRNN(len(char2idx), batch_size=1, num_steps=len(prime),\n",
    "                    lstm_size=lstm_size, num_layers=num_layers, \n",
    "                    learning_rate=learning_rate)\n",
    "    with model.graph.as_default():\n",
    "        saver = tf.train.Saver()\n",
    "\n",
    "        with tf.Session(config=sess_conf) as sess:\n",
    "            # Âä†ËΩΩÊ®°ÂûãÂèÇÊï∞ÔºåÊÅ¢Â§çËÆ≠ÁªÉ\n",
    "            saver.restore(sess, checkpoint)\n",
    "            feed = {model.inputs: np.array([char2idx[c] for c in prime]),\n",
    "                    model.keep_prob: 1.,}\n",
    "            preds, new_state = sess.run([model.prediction, model.final_state], \n",
    "                                        feed_dict=feed,\n",
    "                                        options=run_opt)\n",
    "            top5_prob=preds[0][np.argsort(preds[0])[-5:]]\n",
    "            top5_idx = np.argsort(preds[0])[-5:]\n",
    "            print(f\">>> ÂØπÊï¥‰∏™prime: {prime} ÁöÑÈ¢ÑÊµãÁªìÊûú  [shape]:{preds.shape}\")\n",
    "            print(f\"    top5ÊòØ:{top5_prob}<==>{[idx2char[i] for i in top5_idx]}\")\n",
    "            next_char = pick_top_n(preds, vocab_size, random=random)\n",
    "            print(f\"    Â¶ÇÊûúÊ≠§Êó∂ÈÄâÂèñtopNÁîüÊàêÂ≠óÁ¨¶(ÊòØÂê¶ÈöèÊú∫:{random})Ôºå‰ºöÊòØ: [idx]:'{next_char}' [char]:'{idx2char[next_char]}'\")\n",
    "            \n",
    "            \n",
    "            # Ê∑ªÂä†Â≠óÁ¨¶Âà∞samples‰∏≠\n",
    "            samples.append(idx2char[c])\n",
    "            \n",
    "            inp = np.array([[c]])\n",
    "            # ‰∏çÊñ≠ÁîüÊàêÂ≠óÁ¨¶ÔºåÁõ¥Âà∞ËææÂà∞ÊåáÂÆöÊï∞ÁõÆ\n",
    "            for _ in range(n_samples):\n",
    "                feed = {model.inputs: [[c]],\n",
    "                        model.keep_prob: 1.}\n",
    "                preds, new_state = sess.run([model.prediction, model.final_state], \n",
    "                                            feed_dict=feed,\n",
    "                                            options=run_opt)\n",
    "\n",
    "                c = pick_top_n(preds, vocab_size, random=random)\n",
    "                samples.append(idx2char[c])\n",
    "\n",
    "    return ''.join(samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-22T02:12:41.986369Z",
     "start_time": "2019-10-22T02:12:41.902747Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> samples: ['F', 'a', 'r']\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Error converting shape to a TensorShape: int() argument must be a string, a bytes-like object or a number, not 'Tensor'.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m~/python3/lib/python3.6/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mmake_shape\u001b[0;34m(v, arg_name)\u001b[0m\n\u001b[1;32m    140\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 141\u001b[0;31m     \u001b[0mshape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtensor_shape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    142\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/python3/lib/python3.6/site-packages/tensorflow/python/framework/tensor_shape.py\u001b[0m in \u001b[0;36mas_shape\u001b[0;34m(shape)\u001b[0m\n\u001b[1;32m    946\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 947\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mTensorShape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    948\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/python3/lib/python3.6/site-packages/tensorflow/python/framework/tensor_shape.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, dims)\u001b[0m\n\u001b[1;32m    541\u001b[0m         \u001b[0;31m# Got a list of dimensions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 542\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dims\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mas_dimension\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0md\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdims_iter\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    543\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ndims\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/python3/lib/python3.6/site-packages/tensorflow/python/framework/tensor_shape.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    541\u001b[0m         \u001b[0;31m# Got a list of dimensions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 542\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dims\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mas_dimension\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0md\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdims_iter\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    543\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ndims\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/python3/lib/python3.6/site-packages/tensorflow/python/framework/tensor_shape.py\u001b[0m in \u001b[0;36mas_dimension\u001b[0;34m(value)\u001b[0m\n\u001b[1;32m    481\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 482\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mDimension\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    483\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/python3/lib/python3.6/site-packages/tensorflow/python/framework/tensor_shape.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, value)\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m       if (not isinstance(value, compat.bytes_or_text_types) and\n",
      "\u001b[0;31mTypeError\u001b[0m: int() argument must be a string, a bytes-like object or a number, not 'Tensor'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-44-ceff95474e60>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_default_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mckpt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlatest_checkpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./tmp/lstm_anna'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mckpt\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mn_samples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2000\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlstm_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlstm_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnum_layers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_layers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvocab_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchar2idx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mprime\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Far\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-40-576aa10da45e>\u001b[0m in \u001b[0;36msample\u001b[0;34m(checkpoint, n_samples, lstm_size, num_layers, vocab_size, prime, random)\u001b[0m\n\u001b[1;32m     34\u001b[0m                     \u001b[0mlstm_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlstm_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_layers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_layers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m                     \u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m                     sampling=True)\n\u001b[0m\u001b[1;32m     37\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0msaver\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSaver\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-43-dd85fccbe90c>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, num_classes, batch_size, num_steps, lstm_size, num_layers, learning_rate, grad_clip, summary_path, sampling)\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m             \u001b[0;31m# ËæìÂÖ•Â±Ç\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtargets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeep_prob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_steps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m             \u001b[0;31m# LSTMÂ±Ç\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-1b2d3ea953ee>\u001b[0m in \u001b[0;36mbuild_inputs\u001b[0;34m(num_seqs, num_steps)\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mnum_steps\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mÊØè‰∏™Â∫èÂàóÂåÖÂê´ÁöÑÂ≠óÁ¨¶Êï∞\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     '''\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplaceholder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_seqs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_steps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'inputs'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0mtargets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplaceholder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_seqs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_steps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'targets'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/python3/lib/python3.6/site-packages/tensorflow/python/ops/array_ops.py\u001b[0m in \u001b[0;36mplaceholder\u001b[0;34m(dtype, shape, name)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                        \"eager execution.\")\n\u001b[1;32m   1746\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1747\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mgen_array_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplaceholder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/python3/lib/python3.6/site-packages/tensorflow/python/ops/gen_array_ops.py\u001b[0m in \u001b[0;36mplaceholder\u001b[0;34m(dtype, shape, name)\u001b[0m\n\u001b[1;32m   5202\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mshape\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5203\u001b[0m       \u001b[0mshape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5204\u001b[0;31m     \u001b[0mshape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_execute\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"shape\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5205\u001b[0m     _, _, _op = _op_def_lib._apply_op_helper(\n\u001b[1;32m   5206\u001b[0m         \"Placeholder\", dtype=dtype, shape=shape, name=name)\n",
      "\u001b[0;32m~/python3/lib/python3.6/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mmake_shape\u001b[0;34m(v, arg_name)\u001b[0m\n\u001b[1;32m    141\u001b[0m     \u001b[0mshape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtensor_shape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 143\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Error converting %s to a TensorShape: %s.\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0marg_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    144\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m     raise ValueError(\"Error converting %s to a TensorShape: %s.\" % (arg_name,\n",
      "\u001b[0;31mTypeError\u001b[0m: Error converting shape to a TensorShape: int() argument must be a string, a bytes-like object or a number, not 'Tensor'."
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "ckpt = tf.train.latest_checkpoint('./tmp/lstm_anna')\n",
    "sample(ckpt,n_samples=2000,lstm_size=lstm_size,num_layers=num_layers,vocab_size=len(char2idx),prime=\"Far\", random=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CharRNN ÂÜÖÈÉ®ÁªÜËäÇÁöÑÊµãËØï"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-16T09:36:25.792303Z",
     "start_time": "2019-10-16T09:36:21.664666Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with open(result_set_fp+\".pickle\",\"rb+\") as frb:\n",
    "    char2idx = pickle.load(frb)\n",
    "\n",
    "encoded = np.load(coded_article_fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-16T09:38:08.801334Z",
     "start_time": "2019-10-16T09:38:08.793462Z"
    },
    "code_folding": [
     0
    ],
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_batches_as_iter(encoded, batch_size, time_steps, verbose=False):\n",
    "    chunk_len = batch_size*time_steps \n",
    "    n_chunk = int(len(encoded)/chunk_len)\n",
    "    arr = encoded[:chunk_len*n_chunk]  # Êà™ÂèñÊï¥Êï∞ÂÄçÁöÑbatch_size\n",
    "    arr = arr.reshape((batch_size,-1))\n",
    "\n",
    "    for n in range(0, arr.shape[1], time_steps):\n",
    "        x = arr[:, n:n+time_steps]\n",
    "        y = np.zeros_like(x)\n",
    "        y[:, :-1], y[:, -1] = x[:,1:], y[:, 0] \n",
    "        yield x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-16T09:57:25.505075Z",
     "start_time": "2019-10-16T09:57:25.495281Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> default_BS: 20\n",
      ">>> default_x: (20, 10)\n",
      " [[1915 1991 1096   49 1991 1117 1991 1099  189  189]\n",
      " [1991   33  328  184 2154 2154 1925  328 1220   33]\n",
      " [1220  151 1925 1220 1544 2208 2154 1220 1176  151]]\n",
      ">>> default_y: (20, 10)\n",
      " [[1991 1096   49 1991 1117 1991 1099  189  189 1991]\n",
      " [  33  328  184 2154 2154 1925  328 1220   33   33]\n",
      " [ 151 1925 1220 1544 2208 2154 1220 1176  151  151]]\n"
     ]
    }
   ],
   "source": [
    "time_steps = 10\n",
    "lstm_layers = [256]*2\n",
    "lstm_size = lstm_layers[0]\n",
    "num_classes = len(char2idx)\n",
    "default_BS = 20\n",
    "default_x,default_y=list(itertools.islice(get_batches_as_iter(encoded, batch_size=default_BS, time_steps=time_steps),1))[0]\n",
    "print(f\">>> default_BS: {default_BS}\")\n",
    "print(f\">>> default_x: {default_x.shape}\\n\",default_x[:3,:10])\n",
    "print(f\">>> default_y: {default_y.shape}\\n\",default_y[:3,:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-17T04:29:28.374632Z",
     "start_time": "2019-10-17T04:29:26.787047Z"
    },
    "code_folding": [
     9
    ],
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ">>> ÊµÅÁ®ãÂ¶Ç‰∏ã\n",
      "    inpX: (20, 10)\n",
      "    +onehot=> X: (20, 10, 83)\n",
      "    +mlstm=> lstm_output: (20, 10, 256)\n",
      "    +reshape=> softmax_x: (200, 256)\n",
      "    +softmax(just matmul)=> logits: (200, 83)\n",
      "    \n",
      "    inpY: (20, 10)\n",
      "    +onehot=> Y: (20, 10, 83)\n",
      "    +reshape=> y_reshaped: (200, 83)\n",
      "    \n",
      "    CE(logits,y_reshaped): (200,)\n",
      "    +reduce_mean=> loss: (),scalar:0.1547\n",
      "    \n",
      ">>> X_: (20, 10, 83)\n",
      "\n",
      ">>> Y_: (20, 10, 83)\n",
      "\n",
      ">>> lstm_final_state:\n",
      "    >>> [layer]:0 [c_state:]: (20, 256)\n",
      "\n",
      "    >>> [layer]:0 [h_state:]: (20, 256)\n",
      "\n",
      "    >>> [layer]:1 [c_state:]: (20, 256)\n",
      "\n",
      "    >>> [layer]:1 [h_state:]: (20, 256)\n",
      "\n",
      ">>> lstm_output: (20, 10, 256)\n",
      "\n",
      ">>> seq_output_: (20, 10, 256)\n",
      "\n",
      "seq_output ÁöÑÁ°ÆÊ≤°ÊúâËµ∑Âà∞‰ΩúÁî®,tf‰∏≠ÂØπ‰∏Ä‰∏™tensor‰ΩøÁî®concat‰ªÄ‰πàÈÉΩ‰∏ç‰ºöÊîπÂèòÔºå‰∏ÄËà¨ÊòØÂØπ‰∏Ä‰∏™ÂÜÖÈÉ®ÂÖÉÁ¥†ÊòØtensorÁöÑlistÂÅöconcat\n",
      ">>> sf_x: (200, 256) sf_w: (256, 83) sf_b: (83,)\n",
      ">>> logits_: (200, 83) pred_: (200, 83)\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "# placeholder\n",
    "inpBS = tf.placeholder(tf.int32, [], name=\"batch_size\")\n",
    "inpX = tf.placeholder(tf.int32, shape=(None, time_steps), name=\"inpX\")\n",
    "inpY = tf.placeholder(tf.int32, shape=(None), name=\"inpY\")\n",
    "X = tf.one_hot(inpX, depth=len(char2idx))\n",
    "Y = tf.one_hot(inpY, depth=len(char2idx))\n",
    "# LSTM ÊûÑÂª∫\n",
    "lstm_cell_list = []\n",
    "for nodes_size in lstm_layers:\n",
    "    lstm = tf.contrib.rnn.BasicLSTMCell(nodes_size)\n",
    "    lstm_dropout = tf.contrib.rnn.DropoutWrapper(lstm, output_keep_prob=0.8)\n",
    "    lstm_cell_list.append(lstm_dropout)\n",
    "mlstm_cell = tf.contrib.rnn.MultiRNNCell(lstm_cell_list)\n",
    "initial_state = mlstm_cell.zero_state(inpBS, tf.float32)\n",
    "lstm_output, lstm_final_state = tf.nn.dynamic_rnn(mlstm_cell, X, initial_state = initial_state)\n",
    "\n",
    "# formt output\n",
    "seq_output = tf.concat(lstm_output, axis=1) \n",
    "softmax_x = tf.reshape(seq_output, [-1, lstm_size])\n",
    "softmax_w = tf.Variable(tf.truncated_normal([lstm_size, num_classes], stddev=0.1))\n",
    "softmax_b = tf.Variable(tf.zeros(num_classes))\n",
    "logits = tf.matmul(softmax_x, softmax_w) + softmax_b\n",
    "pred = tf.nn.softmax(logits, name='predictions')\n",
    "\n",
    "# ËÆ°ÁÆóloss\n",
    "y_reshaped = tf.reshape(Y, [-1, num_classes])\n",
    "loss_ce = tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y_reshaped)\n",
    "loss = tf.reduce_mean(loss_ce)\n",
    "\n",
    "# optimize\n",
    "tvars = tf.trainable_variables()\n",
    "grad_clip = 5\n",
    "grads, _ = tf.clip_by_global_norm(tf.gradients(loss, tvars), grad_clip)\n",
    "train_op = tf.train.AdamOptimizer(0.01)\n",
    "optimizer = train_op.apply_gradients(zip(grads, tvars))\n",
    "\n",
    "default_feed = {inpBS:default_BS, inpX:default_x, inpY:default_y}\n",
    "with tf.Session(config=sess_conf) as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    inpX_,X_,inpY_,Y_,is_,lo_,lfs_each_layer = sess.run([inpX,X,inpY,Y,initial_state,lstm_output,lstm_final_state], feed_dict=default_feed)\n",
    "    seq_output_,sf_x,sf_w,sf_b = sess.run([seq_output,softmax_x,softmax_w,softmax_b], feed_dict=default_feed)\n",
    "    logits_,pred_,y_reshaped_ = sess.run([logits,pred,y_reshaped], feed_dict=default_feed)\n",
    "    loss_ce_,loss_,_ = sess.run([loss_ce,loss,optimizer],feed_dict=default_feed)\n",
    "    print(f\"\"\"\\n>>> ÊµÅÁ®ãÂ¶Ç‰∏ã\n",
    "    inpX: {inpX_.shape}\n",
    "    +onehot=> X: {X_.shape}\n",
    "    +mlstm=> lstm_output: {lo_.shape}\n",
    "    +reshape=> softmax_x: {sf_x.shape}\n",
    "    +softmax(just matmul)=> logits: {logits_.shape}\n",
    "    \n",
    "    inpY: {inpY_.shape}\n",
    "    +onehot=> Y: {Y_.shape}\n",
    "    +reshape=> y_reshaped: {y_reshaped_.shape}\n",
    "    \n",
    "    CE(logits,y_reshaped): {loss_ce_.shape}\n",
    "    +reduce_mean=> loss: {loss_.shape},scalar:{loss_:.4f}\n",
    "    \"\"\"\n",
    "    )\n",
    "    print(f\">>> X_: {X_.shape}\\n\")\n",
    "    print(f\">>> Y_: {Y_.shape}\\n\")\n",
    "    print(\">>> lstm_final_state:\")\n",
    "    for idx,lfs in enumerate(lfs_each_layer):\n",
    "        print(f\"    >>> [layer]:{idx} [c_state:]: {lfs.c.shape}\\n\")\n",
    "        print(f\"    >>> [layer]:{idx} [h_state:]: {lfs.h.shape}\\n\")\n",
    "    print(f\">>> lstm_output: {lo_.shape}\\n\")\n",
    "    print(f\">>> seq_output_: {seq_output_.shape}\\n\")\n",
    "    print(\"seq_output ÁöÑÁ°ÆÊ≤°ÊúâËµ∑Âà∞‰ΩúÁî®,tf‰∏≠ÂØπ‰∏Ä‰∏™tensor‰ΩøÁî®concat‰ªÄ‰πàÈÉΩ‰∏ç‰ºöÊîπÂèòÔºå‰∏ÄËà¨ÊòØÂØπ‰∏Ä‰∏™ÂÜÖÈÉ®ÂÖÉÁ¥†ÊòØtensorÁöÑlistÂÅöconcat\")\n",
    "    print(f\">>> sf_x: {sf_x.shape} sf_w: {sf_w.shape} sf_b: {sf_b.shape}\")\n",
    "    print(f\">>> logits_: {logits_.shape} pred_: {pred_.shape}\")\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Initialization Cell",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "205.355px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
