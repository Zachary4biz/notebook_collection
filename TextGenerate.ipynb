{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-21T08:17:02.465177Z",
     "start_time": "2019-10-21T08:17:02.199887Z"
    },
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "%matplotlib inline\n",
    "from tqdm.auto import tqdm\n",
    "import concurrent.futures\n",
    "from multiprocessing import Pool\n",
    "import copy,os,sys,psutil\n",
    "from collections import Counter,deque\n",
    "import numpy as np\n",
    "from zac_pyutils.ExqUtils import zprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-21T08:17:03.740406Z",
     "start_time": "2019-10-21T08:17:02.466837Z"
    },
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import json\n",
    "import re\n",
    "import itertools\n",
    "import pickle\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-21T08:17:06.728539Z",
     "start_time": "2019-10-21T08:17:03.742199Z"
    },
    "init_cell": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tensorflow.python.client.session.Session object at 0x7fd412caebe0>\n"
     ]
    }
   ],
   "source": [
    "# å…è®¸GPUæ¸è¿›å ç”¨\n",
    "sess_conf = tf.ConfigProto()\n",
    "sess_conf.gpu_options.allow_growth = True  # å…è®¸GPUæ¸è¿›å ç”¨\n",
    "sess_conf.allow_soft_placement = True  # æŠŠä¸é€‚åˆGPUçš„æ”¾åˆ°CPUä¸Šè·‘\n",
    "with tf.Session(config=sess_conf) as sess:\n",
    "    print(sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-21T08:17:06.732370Z",
     "start_time": "2019-10-21T08:17:06.730082Z"
    },
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "# sess.run(.. options=run_opt)å¯ä»¥åœ¨OOMçš„æ—¶å€™æä¾›å½“å‰å·²ç»å£°æ˜äº†çš„å˜é‡\n",
    "run_opt = tf.RunOptions()\n",
    "run_opt.report_tensor_allocations_upon_oom = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# è¯•è¯•çœ‹èƒ½ä¸èƒ½yieldæ–¹å¼æ„é€ å‡ºå•è¯ç´¢å¼•\n",
    "- è¦è·Ÿåé¢å»æ­£æ–‡ä½¿ç”¨ç›¸åŒçš„ `load_f` åŠ è½½æ–¹å¼ï¼ˆç›¸åŒçš„é¢„å¤„ç†ï¼‰\n",
    "ğŸ‘Œå·²å®Œæˆ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-21T07:05:12.609004Z",
     "start_time": "2019-10-21T07:05:12.599500Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('fp: ',\n",
       " '/home/zhoutong/NLP/data/labeled_timeliness_region_taste_emotion_sample.json.bak')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "('result_set_fp: ',\n",
       " '/home/zhoutong/NLP/data/labeled_timeliness_region_taste_emotion_sample.json.bak_char2idx')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "('coded_article_fp: ',\n",
       " '/home/zhoutong/NLP/data/labeled_timeliness_region_taste_emotion_sample.json.bak_encoded_article.pkl')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# data_name = \"labeled_timeliness_region_taste_emotion_sample.json.bak.head1k\"\n",
    "data_name = \"labeled_timeliness_region_taste_emotion_sample.json.bak\"\n",
    "fp = \"/home/zhoutong/NLP/data/{}\".format(data_name)\n",
    "result_set_fp = \"/home/zhoutong/NLP/data/{}_char2idx\".format(data_name)\n",
    "coded_article_fp = \"/home/zhoutong/NLP/data/{}_encoded_article.pkl\".format(data_name)\n",
    "\n",
    "\"fp: \",fp\n",
    "\"result_set_fp: \", result_set_fp\n",
    "\"coded_article_fp: \", coded_article_fp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-21T07:05:12.781665Z",
     "start_time": "2019-10-21T07:05:12.778393Z"
    }
   },
   "outputs": [],
   "source": [
    "def load_f(fp_inp):\n",
    "    with open(fp_inp,\"r\") as f:\n",
    "        for line in f:\n",
    "            title = json.loads(line)['title']\n",
    "            text = json.loads(line)['text']\n",
    "            text = re.sub(\"[\\\\n]+\", \"\\\\n\",text)\n",
    "            yield text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-23T10:43:39.584552Z",
     "start_time": "2019-09-23T10:42:12.760043Z"
    },
    "code_folding": [
     10,
     24
    ],
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "   \n",
    "def transform(text_inp):\n",
    "    \"\"\"\n",
    "    è¿™é‡Œæ˜¯æŠŠå„ä¸ªæ ‡ç‚¹ç¬¦å·éƒ½å‰ååŠ ä¸Šç©ºæ ¼åˆ†å¼€ï¼Œä¸ç¡®å®šè¿™æ ·æ˜¯å¦å¯ä»¥å¢åŠ æ–‡æœ¬ç”Ÿæˆæ—¶å¯¹æ ‡ç‚¹çš„å‡†ç¡®è¡¨ç¤º\n",
    "    ç†è®ºä¸Šåœ¨å»ºç«‹ç´¢å¼•çš„æ—¶å€™è¡¨å¾è¿‡çš„å…ƒç´ ï¼ˆä¾‹å¦‚\"\\n\"ç´¢å¼•ä¸º0ï¼‰å°±æœ‰å¯èƒ½æ€§\n",
    "    ä½†æ˜¯ä¸åˆ†å¼€ï¼Œç›´æ¥æŠŠ \"you!\"(idx=11) å½“ä½œä¸€ä¸ªæ–°çš„æ•´ä½“è€Œä¸æ˜¯ \"you\"(idx=9) å’Œ \"!\"(idx=10) å¯èƒ½ä¹Ÿè¡Œ\n",
    "    \"\"\"\n",
    "    for t in [\"\\\\n\",\", \"]:\n",
    "        text_inp = re.sub(t, \" \"+t+\" \",text_inp)\n",
    "    text_inp = re.sub(\"\\. \",\" . \",text_inp) # \".\" ä¸å¥½ç›´æ¥æ”¾åœ¨å¾ªç¯ä¸­ä¸€èµ·åšï¼Œè§„çŸ©ä¸å¤ªä¸€æ ·å•ç‹¬åšäº† \n",
    "    return text_inp\n",
    "\n",
    "\n",
    "text_g = load_f(fp)\n",
    "result_set = set()\n",
    "while True:\n",
    "    chunk = list(itertools.islice(text_g,10000))\n",
    "    if len(chunk) > 0:\n",
    "        for text in chunk:\n",
    "            # ä¸ä½¿ç”¨transform\n",
    "            # text = transform(text)\n",
    "            result_set.update(text.replace(\"\\n\",\" \\n \").strip().split(\" \"))\n",
    "    else:\n",
    "        result_set = [i for i in result_set if i != \"\"]\n",
    "        break\n",
    "\n",
    "\n",
    "import pickle\n",
    "result_set_d = dict([(word,idx) for idx,word in enumerate(result_set)])\n",
    "with open(result_set_fp+\".pickle\",\"wb+\") as f:\n",
    "    pickle.dump(result_set_d,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-14T10:38:56.502159Z",
     "start_time": "2019-10-14T10:38:55.022817Z"
    },
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "with open(result_set_fp+\".pickle\",\"rb+\") as f:\n",
    "    word2idx_dict = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-14T10:39:01.439408Z",
     "start_time": "2019-10-14T10:39:01.436223Z"
    },
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "list(itertools.islice(word2idx_dict.items(),10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# å®éªŒæ€§è´¨ | çœ‹çœ‹å‡ºæ¥çš„ç»“æœå¯¹ä¸å¯¹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-11T06:25:01.374999Z",
     "start_time": "2019-10-11T06:25:00.633773Z"
    },
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "\n",
    "text_g = load_f(fp)\n",
    "wordsIdx = deque()\n",
    "stopCnt = 0\n",
    "while True:\n",
    "    chunk = list(itertools.islice(text_g,10000))\n",
    "    if len(chunk) > 0:\n",
    "        for text in chunk:\n",
    "            words = text.replace(\"\\n\",\" \\n \").strip().split(\" \")\n",
    "            words = [i for i in words if i != \"\"]\n",
    "            wordsIdx.append([word2idx_dict[w] for w in words])\n",
    "            print(\">>>\", words[:10])\n",
    "            for i in list(itertools.islice(wordsIdx,10)):\n",
    "                print(i[:10])  # æ¯æ¬¡éƒ½æ‰“å°wordsIdxçš„top10æ®µè½çš„top10ä¸ªè¯\n",
    "            stopCnt += 1\n",
    "            assert stopCnt<=5\n",
    "    else:\n",
    "        result_set = [i for i in result_set if i != \"\"]\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# æ–‡ç« æ›¿æ¢æˆwordç´¢å¼•\n",
    "- è¿™é‡Œæ¯ç¯‡æ–‡ç« éƒ½æ˜¯ä¸€ä¸ªå•ç‹¬çš„æ•°ç»„`append`åˆ°`wordsIdx`é‡Œ\n",
    "- è¿™ä¸ªäºŒç»´æ•°ç»„å­˜npyæ–‡ä»¶å¤ªå¤§äº†ï¼Œè½¬æˆäºŒç»´listå­˜\n",
    "    - npy: 5.1G | deque_pkl: 3.2G | list_pkl: 3.2G\n",
    "    - ç›´æ¥ä»¥dequeå­˜å’Œè½¬æˆlistå­˜å ç”¨ç©ºé—´ç›¸åŒ\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-15T01:33:25.945343Z",
     "start_time": "2019-10-15T01:30:43.909912Z"
    },
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "\n",
    "text_g = load_f(fp)\n",
    "wordsIdx = deque()\n",
    "with tqdm() as pbar:\n",
    "    while True:\n",
    "        chunk = list(itertools.islice(text_g,10000))\n",
    "        if len(chunk) > 0:\n",
    "            for text in chunk:\n",
    "                words = text.replace(\"\\n\",\" \\n \").strip().split(\" \")\n",
    "                words = [i for i in words if i != \"\"]\n",
    "                words_idx = ([word2idx_dict[w] for w in words]+[-1]*1024)[:300]  # æ¯ç¯‡æ–‡ç« æœ€å¤šå–1024ä¸ªè¯\n",
    "                wordsIdx.append(words_idx)\n",
    "                pbar.update(1)\n",
    "        else:\n",
    "            break\n",
    "with open(coded_article_fp,\"wb+\") as fwb:\n",
    "    pickle.dump(list(wordsIdx),fwb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-15T01:40:51.737070Z",
     "start_time": "2019-10-15T01:40:33.488827Z"
    },
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "with open(coded_article_fp,\"rb+\") as frb:\n",
    "    coded_article = pickle.load(frb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CharRNN åŸºäºå­—ç¬¦"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## æ­å»ºæ¨¡å‹"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### è¾“å…¥å±‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-21T08:17:06.744970Z",
     "start_time": "2019-10-21T08:17:06.733593Z"
    },
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "def build_inputs(batch_size, num_steps):\n",
    "    '''\n",
    "    æ„å»ºè¾“å…¥å±‚\n",
    "    \n",
    "    batch_size: æ¯ä¸ªbatchä¸­çš„åºåˆ—ä¸ªæ•°\n",
    "    num_steps: æ¯ä¸ªåºåˆ—åŒ…å«çš„å­—ç¬¦æ•°\n",
    "    '''\n",
    "    inputs = tf.placeholder(tf.int32, shape=(batch_size, num_steps), name='inputs')\n",
    "    targets = tf.placeholder(tf.int32, shape=(batch_size, num_steps), name='targets')\n",
    "    \n",
    "    # åŠ å…¥keep_prob\n",
    "    keep_prob = tf.placeholder(tf.float32, name='keep_prob')\n",
    "    \n",
    "    return inputs, targets, keep_prob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM\n",
    "- `BasicLSTMCell` æ›¿æ¢ä¸º `LSTMCell` \n",
    "- LSTMéœ€è¦çŸ¥é“ `batch_size` åªæ˜¯ç”¨æ¥åšå…¨é›¶åˆå§‹åŒ–æ—¶éœ€è¦çŸ¥é“ç»´åº¦"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-21T08:17:06.751913Z",
     "start_time": "2019-10-21T08:17:06.746512Z"
    },
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "def build_lstm(lstm_size, num_layers, batch_size, keep_prob):\n",
    "    ''' \n",
    "    æ„å»ºlstmå±‚\n",
    "        \n",
    "    keep_prob\n",
    "    lstm_size: lstméšå±‚ä¸­ç»“ç‚¹æ•°ç›®\n",
    "    num_layers: lstmçš„éšå±‚æ•°ç›®\n",
    "    batch_size: batch_size\n",
    "\n",
    "    '''\n",
    "    def construct_cell(node_size):\n",
    "        # æ„å»ºä¸€ä¸ªåŸºæœ¬lstmå•å…ƒ\n",
    "        lstm = tf.nn.rnn_cell.LSTMCell(node_size)\n",
    "        # æ·»åŠ dropout\n",
    "        drop = tf.nn.rnn_cell.DropoutWrapper(lstm, output_keep_prob=keep_prob)\n",
    "        return drop\n",
    "    \n",
    "    # å †å \n",
    "    cell = tf.nn.rnn_cell.MultiRNNCell([construct_cell(lstm_size) for _ in range(num_layers)])\n",
    "    initial_state = cell.zero_state(batch_size, tf.float32)\n",
    "    \n",
    "    return cell, initial_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### è¾“å‡ºå±‚\n",
    "- `tf.concat(1,lstm_output)` æ›¿æ¢ä¸º `tf.concat(lstm_output,1)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-21T08:17:06.758630Z",
     "start_time": "2019-10-21T08:17:06.753287Z"
    },
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "def build_output(lstm_output, in_size, out_size):\n",
    "    ''' \n",
    "    æ„é€ è¾“å‡ºå±‚\n",
    "        \n",
    "    lstm_output: lstmå±‚çš„è¾“å‡ºç»“æœ\n",
    "    in_size: lstmè¾“å‡ºå±‚é‡å¡‘åçš„size\n",
    "    out_size: softmaxå±‚çš„size\n",
    "    \n",
    "    '''\n",
    "\n",
    "    # å°†lstmçš„è¾“å‡ºæŒ‰ç…§åˆ—concateï¼Œä¾‹å¦‚[[1,2,3],[7,8,9]],\n",
    "    # tf.concatçš„ç»“æœæ˜¯[1,2,3,7,8,9]\n",
    "    seq_output = tf.concat(lstm_output, 1) # tf.concat(concat_dim, values)\n",
    "    # reshape\n",
    "    x = tf.reshape(seq_output, [-1, in_size])\n",
    "    tf.summary.histogram('seq_output_reshape',x)\n",
    "    \n",
    "    # å°†lstmå±‚ä¸softmaxå±‚å…¨è¿æ¥\n",
    "    with tf.variable_scope('softmax'):\n",
    "        softmax_w = tf.Variable(tf.truncated_normal([in_size, out_size], stddev=0.1))\n",
    "        softmax_b = tf.Variable(tf.zeros(out_size))\n",
    "    tf.summary.histogram(\"softmax_w\",softmax_w)\n",
    "    tf.summary.histogram(\"softmax_b\",softmax_b)\n",
    "    \n",
    "    # è®¡ç®—logits\n",
    "    logits = tf.matmul(x, softmax_w) + softmax_b\n",
    "    \n",
    "    # softmaxå±‚è¿”å›æ¦‚ç‡åˆ†å¸ƒ\n",
    "    out = tf.nn.softmax(logits, name='predictions')\n",
    "    tf.summary.histogram('pred',out)\n",
    "    \n",
    "    return out, logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### è¯¯å·®\n",
    "- `softmax_cross_entropy_with_logits` æ›¿æ¢ä¸º `softmax_cross_entropy_with_logits_v2`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-21T08:17:06.765299Z",
     "start_time": "2019-10-21T08:17:06.759989Z"
    },
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "def build_loss(logits, targets, lstm_size, num_classes):\n",
    "    '''\n",
    "    æ ¹æ®logitså’Œtargetsè®¡ç®—æŸå¤±\n",
    "    \n",
    "    logits: å…¨è¿æ¥å±‚çš„è¾“å‡ºç»“æœï¼ˆä¸ç»è¿‡softmaxï¼‰\n",
    "    targets: targets\n",
    "    lstm_size\n",
    "    num_classes: vocab_size\n",
    "        \n",
    "    '''\n",
    "    \n",
    "    # One-hotç¼–ç \n",
    "    y_one_hot = tf.one_hot(targets, num_classes)\n",
    "    y_reshaped = tf.reshape(y_one_hot, logits.get_shape())\n",
    "    \n",
    "    # Softmax cross entropy loss\n",
    "    loss = tf.nn.softmax_cross_entropy_with_logits_v2(logits=logits, labels=y_reshaped)\n",
    "    loss = tf.reduce_mean(loss)\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ä¼˜åŒ–å™¨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-21T08:17:06.772109Z",
     "start_time": "2019-10-21T08:17:06.766639Z"
    },
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "def build_optimizer(loss, learning_rate, grad_clip):\n",
    "    ''' \n",
    "    æ„é€ Optimizer\n",
    "   \n",
    "    loss: æŸå¤±\n",
    "    learning_rate: å­¦ä¹ ç‡\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    # ä½¿ç”¨clipping gradients\n",
    "    tvars = tf.trainable_variables()\n",
    "    grads, _ = tf.clip_by_global_norm(tf.gradients(loss, tvars), grad_clip)\n",
    "    for g in grads:\n",
    "        tf.summary.histogram(g.name, g)\n",
    "    train_op = tf.train.AdamOptimizer(learning_rate)\n",
    "    optimizer = train_op.apply_gradients(zip(grads, tvars))\n",
    "    \n",
    "    return optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### æ¨¡å‹\n",
    "- ä½¿ç”¨ `placeholder` æ›¿ä»£å›ºå®šçš„sizeå’Œsteps\n",
    "- å†…éƒ¨æ–°å»ºä¸€å¼ è®¡ç®—å›¾è€Œä¸æ˜¯ä½¿ç”¨resetåçš„é»˜è®¤è®¡ç®—å›¾\n",
    "- å¢åŠ summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-22T03:50:13.488719Z",
     "start_time": "2019-10-22T03:50:13.474524Z"
    },
    "code_folding": [
     38
    ],
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "class CharRNN:\n",
    "    def __init__(self, num_classes, batch_size=64, num_steps=50, \n",
    "                       lstm_size=128, num_layers=2, learning_rate=0.001, \n",
    "                       grad_clip=5, summary_path=None, sampling=False):\n",
    "    \n",
    "        batch_size, num_steps = batch_size, num_steps\n",
    "        \n",
    "        # æ–°å»ºä¸€å¼ å›¾\n",
    "        self.graph = tf.Graph()\n",
    "        \n",
    "        with self.graph.as_default():\n",
    "            # è¾“å…¥å±‚\n",
    "            self.inputs, self.targets, self.keep_prob = build_inputs(batch_size, num_steps)\n",
    "\n",
    "            # LSTMå±‚\n",
    "            cell, self.initial_state = build_lstm(lstm_size, num_layers, batch_size, self.keep_prob)\n",
    "\n",
    "            # å¯¹è¾“å…¥è¿›è¡Œone-hotç¼–ç \n",
    "            x_one_hot = tf.one_hot(self.inputs, num_classes)\n",
    "\n",
    "            # è¿è¡ŒRNN\n",
    "            outputs, state = tf.nn.dynamic_rnn(cell, x_one_hot, initial_state=self.initial_state)\n",
    "            self.final_state = state\n",
    "\n",
    "            # é¢„æµ‹ç»“æœ\n",
    "            self.prediction, self.logits = build_output(outputs, lstm_size, num_classes)\n",
    "\n",
    "            # Loss å’Œ optimizer (with gradient clipping)\n",
    "            self.loss = build_loss(self.logits, self.targets, lstm_size, num_classes)\n",
    "            self.optimizer = build_optimizer(self.loss, learning_rate, grad_clip)\n",
    "\n",
    "            # summary\n",
    "            tf.summary.scalar(\"loss\", self.loss)\n",
    "            #    lstmçš„variablesåœ¨dynamic_runä¹‹åæ‰ä¼šæœ‰å€¼ä¸ç„¶æ˜¯ç©ºçš„list\n",
    "            for idx,tensor in enumerate(cell.variables):\n",
    "                if idx % 2 == 0:\n",
    "                    _ = tf.summary.histogram(f\"lstm_kernel_{idx}\",tensor)\n",
    "                else:\n",
    "                    _ = tf.summary.histogram(f\"lstm_bias_{idx}\",tensor)\n",
    "            self.merge_summary = tf.summary.merge_all()\n",
    "            self.writer = tf.summary.FileWriter(summary_path, self.graph) if summary_path is not None else None\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## æ–‡æœ¬ç¼–ç "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### æ•°æ®æŒ‡å®š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-21T08:17:06.793582Z",
     "start_time": "2019-10-21T08:17:06.781464Z"
    },
    "init_cell": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('fp: ', '/home/zhoutong/NLP/data/anna.txt')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "('result_set_fp: ', '/home/zhoutong/NLP/data/anna.txt_char2idx')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "('coded_article_fp: ', '/home/zhoutong/NLP/data/anna.txt_encoded_article.npy')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def load_f(fp_inp):\n",
    "    with open(fp_inp,\"r\") as f:\n",
    "        for line in f:\n",
    "            yield line\n",
    "\n",
    "data_name = \"anna.txt\"\n",
    "fp = \"/home/zhoutong/NLP/data/{}\".format(data_name)\n",
    "result_set_fp = \"/home/zhoutong/NLP/data/{}_char2idx\".format(data_name)\n",
    "coded_article_fp = \"/home/zhoutong/NLP/data/{}_encoded_article.npy\".format(data_name)\n",
    "\n",
    "\"fp: \",fp\n",
    "\"result_set_fp: \", result_set_fp\n",
    "\"coded_article_fp: \", coded_article_fp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### char2idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-15T06:50:26.055470Z",
     "start_time": "2019-10-15T06:50:25.903639Z"
    },
    "code_folding": [],
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "text_g = load_f(fp)\n",
    "result_set = set()\n",
    "while True:\n",
    "    chunk = list(itertools.islice(text_g,10000))\n",
    "    if len(chunk) > 0:\n",
    "        for text in chunk:\n",
    "            # ä¸ä½¿ç”¨transform\n",
    "            # text = transform(text)\n",
    "            result_set.update(list(text))\n",
    "    else:\n",
    "        result_set = [i for i in result_set if i != \"\"]\n",
    "        break\n",
    "\n",
    "\n",
    "import pickle\n",
    "result_set_d = dict([(word,idx) for idx,word in enumerate(result_set)])\n",
    "with open(result_set_fp+\".pickle\",\"wb+\") as f:\n",
    "    pickle.dump(result_set_d,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-21T08:17:06.801609Z",
     "start_time": "2019-10-21T08:17:06.794942Z"
    },
    "init_cell": true,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('3', 0),\n",
       " ('2', 1),\n",
       " ('V', 2),\n",
       " (';', 3),\n",
       " ('i', 4),\n",
       " ('o', 5),\n",
       " ('&', 6),\n",
       " ('P', 7),\n",
       " ('Y', 8),\n",
       " ('/', 9)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "83"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(result_set_fp+\".pickle\",\"rb+\") as frb:\n",
    "    char2idx = pickle.load(frb)\n",
    "list(itertools.islice(char2idx.items(),10))\n",
    "len(char2idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### idx2char"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-21T08:17:06.812093Z",
     "start_time": "2019-10-21T08:17:06.802909Z"
    },
    "init_cell": true,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, '3'),\n",
       " (1, '2'),\n",
       " (2, 'V'),\n",
       " (3, ';'),\n",
       " (4, 'i'),\n",
       " (5, 'o'),\n",
       " (6, '&'),\n",
       " (7, 'P'),\n",
       " (8, 'Y'),\n",
       " (9, '/')]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "83"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx2char={v:k for k,v in char2idx.items()}\n",
    "list(itertools.islice(idx2char.items(),10))\n",
    "len(idx2char)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### encoded (doc2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-21T07:11:02.202881Z",
     "start_time": "2019-10-21T07:11:01.364139Z"
    },
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e29a99672654b12b27a77296eb07577",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1985223), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "with open(fp,\"r+\") as fr:\n",
    "    text = fr.read()\n",
    "\n",
    "encoded = np.array([char2idx[c] for c in tqdm(text)])\n",
    "np.save(coded_article_fp,encoded)  # 14G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-21T08:17:06.828330Z",
     "start_time": "2019-10-21T08:17:06.813395Z"
    },
    "init_cell": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1985223,)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "array([29., 25., 63., 62., 42., 56., 48., 49., 64., 28.], dtype=float32)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded = np.load(coded_article_fp).astype(np.float32)\n",
    "encoded.shape\n",
    "encoded[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### get_batcheså‡½æ•°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-21T08:17:06.832677Z",
     "start_time": "2019-10-21T08:17:06.829695Z"
    },
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "def get_batches(encoded, batch_size, n_steps, verbose=False):\n",
    "    chunk_len = batch_size*n_steps \n",
    "    n_chunk = int(len(encoded)/chunk_len)\n",
    "    arr = encoded[:chunk_len*n_chunk]  # æˆªå–æ•´æ•°å€çš„batch_size\n",
    "    arr = arr.reshape((batch_size,-1))\n",
    "\n",
    "    for n in range(0, arr.shape[1], n_steps):\n",
    "        x = arr[:, n:n+n_steps]\n",
    "        y = np.zeros_like(x)\n",
    "        y[:, :-1], y[:, -1] = x[:,1:], y[:, 0]  # è¿™é‡Œåº”è¯¥æœ‰é—®é¢˜ï¼Œæœ€åy[:, 0]åº”è¯¥æ”¹æˆä»å–åä¸€ä¸ªæ‰å¯¹ä¸ºä»€ä¹ˆæ˜¯åˆä»0å¼€å§‹å–\n",
    "        yield x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ä»¥ä¸‹æ˜¯å¯¹get_batcheså‡½æ•°çš„ä¸€ä¸ªéªŒè¯\n",
    "è¿™é‡Œå…¶å®æ˜¯æŠŠæ•´ä¸ªæ–‡æœ¬è¯­æ–™æŒ‰ã€Œå­—ç¬¦ã€ä½œä¸ºå•ä½åˆ‡åˆ†batchï¼Œå®Œå…¨èˆå¼ƒäº†ã€Œè¯ã€çš„æ¦‚å¿µ\n",
    "\n",
    "ä¾‹å¦‚\"I come from China\"è¿›è¡Œget_batches\n",
    "- `batch_size=3,n_steps=4` è¯´æ˜è¿™ä¸ªbatché‡Œ**æœ‰3ä¸ªæ ·æœ¬ï¼ˆå¥å­ï¼‰ï¼Œæ¯ä¸ªæ ·æœ¬æ—¶é—´æ­¥é•¿ï¼ˆå­—ç¬¦æ•°ï¼‰æ˜¯4**\n",
    "- è¿™æ—¶ä¼šè®¡ç®—è¿™ä¸€å…±æ˜¯å¤šå°‘ä¸ªå­—ç¬¦ï¼š3x4=12\n",
    "- å†è®¡ç®—æ•´ä¸ªå¥å­æ”¯æŒå¤šå°‘ä¸ªbatch`n_chunk = int(len(encoded)/chunk_len)`ï¼ŒæŠŠä½™æ•°å»æ‰\n",
    "- æ­¤åæ¯æ¬¡éƒ½ç”¨`[:, n:n+n_steps]`æ¥è¿­ä»£å–ä¸€ä¸ªbatchçš„æ•°æ®\n",
    "- è¿™ä¸ªä¾‹å¥ä¸­åˆšå¥½åˆ°'I come from 'æ˜¯12ï¼Œåé¢çš„å°±è¢«å½“ä½™æ•°å»æ‰äº†\n",
    "- å¾—åˆ°çš„batchå¦‚ä¸‹ç¤ºä¾‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-17T06:45:45.677211Z",
     "start_time": "2019-10-17T06:45:45.667209Z"
    },
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> æµ‹è¯•æ–‡æœ¬å¤Ÿã€Œ2ã€ä¸ªchunkï¼Œä½™ä¸‹è¢«æˆªæ–­ä¸¢å¼ƒäº†\n",
      "  -æ‰€ä»¥å®é™…ä½¿ç”¨çš„æµ‹è¯•æ–‡æœ¬æ˜¯:'I am from Chaoyang Beiji'\n",
      "  -ä¸¢å¼ƒçš„éƒ¨åˆ†æ˜¯          :'ng China'\n",
      ">>> get_batcheså‡½æ•°é‡Œå¯¹æˆªæ–­åçš„arrè¿˜åšäº†ä¸ª`reshape((batchSize,-1))`ï¼Œæ•ˆæœæ˜¯:(3, 8)\n",
      " [['I' ' ' 'a' 'm' ' ' 'f' 'r' 'o']\n",
      " ['m' ' ' 'C' 'h' 'a' 'o' 'y' 'a']\n",
      " ['n' 'g' ' ' 'B' 'e' 'i' 'j' 'i']]\n",
      "è¿™æ ·åé¢åœ¨å¯¹arrå–ç´¢å¼• [:, n:n+n_steps] æ—¶ï¼Œå…¶å®æ˜¯ï¼šç¬¬0ä¸ªbatchæ˜¯ä»æ¯è¡Œéƒ½å–ç¬¬0æ‰¹çš„ n_steps ä¸ªå…ƒç´ \n",
      "è¿™æ ·çœ‹èµ·æ¥ä¸€ä¸ªbatché‡Œçš„å‡ ä¸ªè®­ç»ƒæ ·æœ¬ï¼ˆè®­ç»ƒsequenceï¼‰ä¹‹é—´å¹¶ä¸æ˜¯è¿ç»­çš„ï¼Œä½†æ˜¯å¹¶ä¸å½±å“ï¼Œæ ·æœ¬å†…çš„sequenceæ˜¯è¿ç»­çš„å°±è¡Œï¼ˆå³æ ·æœ¬è¿˜æ˜¯æ­£ç¡®é¡ºåºçš„å­—ç¬¦ï¼‰\n",
      "\n",
      "*****è¿™é‡Œyå–çš„åº”æœ‰é—®é¢˜ï¼Œæ¯ä¸ªè®­ç»ƒæ ·æœ¬çš„æœ€åä¸€ä¸ªyå¥½åƒæ˜¯é”™çš„*****\n",
      "\n",
      ">>> åœ¨ç¬¬0ä¸ªbatché‡Œ\n",
      "x:\n",
      " [['I' ' ' 'a' 'm']\n",
      " ['m' ' ' 'C' 'h']\n",
      " ['n' 'g' ' ' 'B']]\n",
      "y (xçš„å­—ç¬¦å¾€åå»¶ä¸€ä¸ª):\n",
      " [[' ' 'a' 'm' ' ']\n",
      " [' ' 'C' 'h' ' ']\n",
      " ['g' ' ' 'B' 'g']]\n",
      "\n",
      ">>> åœ¨ç¬¬1ä¸ªbatché‡Œ\n",
      "x:\n",
      " [[' ' 'f' 'r' 'o']\n",
      " ['a' 'o' 'y' 'a']\n",
      " ['e' 'i' 'j' 'i']]\n",
      "y (xçš„å­—ç¬¦å¾€åå»¶ä¸€ä¸ª):\n",
      " [['f' 'r' 'o' 'f']\n",
      " ['o' 'y' 'a' 'o']\n",
      " ['i' 'j' 'i' 'i']]\n"
     ]
    }
   ],
   "source": [
    "testStr = \"I am from Chaoyang Beijing China\"\n",
    "batchSize=3\n",
    "nSteps=4\n",
    "print(f\">>> æµ‹è¯•æ–‡æœ¬å¤Ÿã€Œ{len(testStr)//(batchSize*nSteps)}ã€ä¸ªchunkï¼Œä½™ä¸‹è¢«æˆªæ–­ä¸¢å¼ƒäº†\")\n",
    "actual_used = testStr[:len(testStr)//(batchSize*nSteps)*(batchSize*nSteps)]  # å®é™…ä½¿ç”¨çš„æ–‡æœ¬éƒ¨åˆ†\n",
    "print(f\"  -æ‰€ä»¥å®é™…ä½¿ç”¨çš„æµ‹è¯•æ–‡æœ¬æ˜¯:'{actual_used}'\")\n",
    "print(f\"  -ä¸¢å¼ƒçš„éƒ¨åˆ†æ˜¯          :'{testStr[len(testStr)//(batchSize*nSteps)*(batchSize*nSteps):]}'\")\n",
    "actual_used_reshaped = np.array(list(actual_used)).reshape((batchSize,-1))\n",
    "print(f\">>> get_batcheså‡½æ•°é‡Œå¯¹æˆªæ–­åçš„arrè¿˜åšäº†ä¸ª`reshape((batchSize,-1))`ï¼Œæ•ˆæœæ˜¯:{actual_used_reshaped.shape}\\n\",actual_used_reshaped)\n",
    "print(\"è¿™æ ·åé¢åœ¨å¯¹arrå–ç´¢å¼• [:, n:n+n_steps] æ—¶ï¼Œå…¶å®æ˜¯ï¼šç¬¬0ä¸ªbatchæ˜¯ä»æ¯è¡Œéƒ½å–ç¬¬0æ‰¹çš„ n_steps ä¸ªå…ƒç´ \")\n",
    "print(\"è¿™æ ·çœ‹èµ·æ¥ä¸€ä¸ªbatché‡Œçš„å‡ ä¸ªè®­ç»ƒæ ·æœ¬ï¼ˆè®­ç»ƒsequenceï¼‰ä¹‹é—´å¹¶ä¸æ˜¯è¿ç»­çš„ï¼Œä½†æ˜¯å¹¶ä¸å½±å“ï¼Œæ ·æœ¬å†…çš„sequenceæ˜¯è¿ç»­çš„å°±è¡Œï¼ˆå³æ ·æœ¬è¿˜æ˜¯æ­£ç¡®é¡ºåºçš„å­—ç¬¦ï¼‰\")\n",
    "\n",
    "print(\"\\n*****è¿™é‡Œyå–çš„åº”æœ‰é—®é¢˜ï¼Œæ¯ä¸ªè®­ç»ƒæ ·æœ¬çš„æœ€åä¸€ä¸ªyå¥½åƒæ˜¯é”™çš„*****\")\n",
    "for idx,(x,y) in enumerate(get_batches(np.array(list(testStr)),batch_size=3,n_steps=4)):\n",
    "    print(f\"\\n>>> åœ¨ç¬¬{idx}ä¸ªbatché‡Œ\")\n",
    "    print(f\"x:\\n\",x)\n",
    "    print(f\"y (xçš„å­—ç¬¦å¾€åå»¶ä¸€ä¸ª):\\n\",y)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-15T06:50:35.883583Z",
     "start_time": "2019-10-15T06:50:35.828385Z"
    },
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "text_g = load_f(fp)\n",
    "def get_batches(text_generator, batch_size, time_step, verbose=False):\n",
    "    X_batch,Y_batch = [],[]\n",
    "    X_verbose,Y_verbose = [],[]\n",
    "    chunk = list(itertools.islice(text_generator, batch_size))\n",
    "    for text in chunk:\n",
    "        # æ¯æ¬¡ç”Ÿæˆä¸€ç¯‡æ–‡ç« çš„æ ·æœ¬éƒ½ä»from_idxå¼€å§‹å–time_stepä¸ªå­—ç¬¦\n",
    "        from_idx=np.random.randint(len(text)-time_step-1) # from_idxç”¨éšæœºæ•°,æœ€åçš„-1æ˜¯ä¸ºäº†æŠŠæœ€åä¸€ä¸ªå­—ç¬¦ç•™ç»™Y\n",
    "        text_X = text[from_idx:from_idx+time_step]\n",
    "        text_Y = text[from_idx+1:from_idx+time_step+1]\n",
    "        if verbose:\n",
    "            X_verbose.append(text_X)\n",
    "            Y_verbose.append(text_Y)\n",
    "        X_batch.append([char2idx[char] for char in text_X])\n",
    "        Y_batch.append([char2idx[char] for char in text_Y])\n",
    "    X_batch = np.array(X_batch)\n",
    "    Y_batch = np.array(Y_batch)\n",
    "    if verbose:\n",
    "        return X_batch, Y_batch, np.array(X_verbose), np.array(Y_verbose)\n",
    "    else:\n",
    "        return X_batch, Y_batch\n",
    "\n",
    "get_batches(text_g,20,5,True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## è¶…å‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-21T11:20:17.552786Z",
     "start_time": "2019-10-21T11:20:17.547501Z"
    },
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "batch_size = 100         # Sequences per batch\n",
    "num_steps = 100          # Number of sequence steps per batch\n",
    "lstm_size = 512         # Size of hidden layers in LSTMs\n",
    "num_layers = 2          # Number of LSTM layers\n",
    "learning_rate = 0.01    # Learning rate\n",
    "keep_prob = 0.5         # Dropout keep probability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## è®­ç»ƒ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-21T12:03:04.584685Z",
     "start_time": "2019-10-21T11:20:19.138190Z"
    },
    "code_folding": [
     6,
     11,
     15,
     34
    ],
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Summary name clip_by_global_norm/clip_by_global_norm/_0:0 is illegal; using clip_by_global_norm/clip_by_global_norm/_0_0 instead.\n",
      "INFO:tensorflow:Summary name clip_by_global_norm/clip_by_global_norm/_1:0 is illegal; using clip_by_global_norm/clip_by_global_norm/_1_0 instead.\n",
      "INFO:tensorflow:Summary name clip_by_global_norm/clip_by_global_norm/_2:0 is illegal; using clip_by_global_norm/clip_by_global_norm/_2_0 instead.\n",
      "INFO:tensorflow:Summary name clip_by_global_norm/clip_by_global_norm/_3:0 is illegal; using clip_by_global_norm/clip_by_global_norm/_3_0 instead.\n",
      "INFO:tensorflow:Summary name clip_by_global_norm/clip_by_global_norm/_4:0 is illegal; using clip_by_global_norm/clip_by_global_norm/_4_0 instead.\n",
      "INFO:tensorflow:Summary name clip_by_global_norm/clip_by_global_norm/_5:0 is illegal; using clip_by_global_norm/clip_by_global_norm/_5_0 instead.\n",
      "|2019-10-21 19:20:52| epoch: 1/40 batch: 100/198 err: 2.8633\n",
      "|2019-10-21 19:21:25| epoch: 2/40 batch: 200/198 err: 2.3506\n",
      "|2019-10-21 19:21:57| epoch: 2/40 batch: 300/198 err: 2.1507\n",
      "|2019-10-21 19:22:29| epoch: 3/40 batch: 400/198 err: 1.9443\n",
      "|2019-10-21 19:23:01| epoch: 3/40 batch: 500/198 err: 1.8190\n",
      "|2019-10-21 19:23:33| epoch: 4/40 batch: 600/198 err: 1.6809\n",
      "|2019-10-21 19:24:05| epoch: 4/40 batch: 700/198 err: 1.6413\n",
      "|2019-10-21 19:24:37| epoch: 5/40 batch: 800/198 err: 1.5791\n",
      "|2019-10-21 19:25:10| epoch: 5/40 batch: 900/198 err: 1.5391\n",
      "|2019-10-21 19:25:42| epoch: 6/40 batch: 1000/198 err: 1.5016\n",
      "|2019-10-21 19:26:14| epoch: 6/40 batch: 1100/198 err: 1.4745\n",
      "|2019-10-21 19:26:46| epoch: 7/40 batch: 1200/198 err: 1.4121\n",
      "|2019-10-21 19:27:19| epoch: 7/40 batch: 1300/198 err: 1.4064\n",
      "|2019-10-21 19:27:51| epoch: 8/40 batch: 1400/198 err: 1.4121\n",
      "|2019-10-21 19:28:24| epoch: 8/40 batch: 1500/198 err: 1.3581\n",
      "|2019-10-21 19:28:56| epoch: 9/40 batch: 1600/198 err: 1.3404\n",
      "|2019-10-21 19:29:28| epoch: 9/40 batch: 1700/198 err: 1.3103\n",
      "|2019-10-21 19:30:00| epoch: 10/40 batch: 1800/198 err: 1.3541\n",
      "|2019-10-21 19:30:32| epoch: 10/40 batch: 1900/198 err: 1.3166\n",
      "|2019-10-21 19:31:05| epoch: 11/40 batch: 2000/198 err: 1.3431\n",
      "|2019-10-21 19:31:38| epoch: 11/40 batch: 2100/198 err: 1.2898\n",
      "|2019-10-21 19:32:10| epoch: 12/40 batch: 2200/198 err: 1.3035\n",
      "|2019-10-21 19:32:42| epoch: 12/40 batch: 2300/198 err: 1.2429\n",
      "|2019-10-21 19:33:13| epoch: 13/40 batch: 2400/198 err: 1.2975\n",
      "|2019-10-21 19:33:46| epoch: 13/40 batch: 2500/198 err: 1.2632\n",
      "|2019-10-21 19:34:18| epoch: 14/40 batch: 2600/198 err: 1.2262\n",
      "|2019-10-21 19:34:50| epoch: 14/40 batch: 2700/198 err: 1.2135\n",
      "|2019-10-21 19:35:22| epoch: 15/40 batch: 2800/198 err: 1.2775\n",
      "|2019-10-21 19:35:55| epoch: 15/40 batch: 2900/198 err: 1.2475\n",
      "|2019-10-21 19:36:27| epoch: 16/40 batch: 3000/198 err: 1.2605\n",
      "|2019-10-21 19:37:01| epoch: 16/40 batch: 3100/198 err: 1.1921\n",
      "|2019-10-21 19:37:32| epoch: 17/40 batch: 3200/198 err: 1.2033\n",
      "|2019-10-21 19:38:05| epoch: 17/40 batch: 3300/198 err: 1.2045\n",
      "|2019-10-21 19:38:37| epoch: 18/40 batch: 3400/198 err: 1.2458\n",
      "|2019-10-21 19:39:09| epoch: 18/40 batch: 3500/198 err: 1.2395\n",
      "|2019-10-21 19:39:41| epoch: 19/40 batch: 3600/198 err: 1.2339\n",
      "|2019-10-21 19:40:14| epoch: 19/40 batch: 3700/198 err: 1.2134\n",
      "|2019-10-21 19:40:46| epoch: 20/40 batch: 3800/198 err: 1.1771\n",
      "|2019-10-21 19:41:19| epoch: 20/40 batch: 3900/198 err: 1.2386\n",
      "|2019-10-21 19:41:50| epoch: 21/40 batch: 4000/198 err: 1.2171\n",
      "|2019-10-21 19:42:23| epoch: 21/40 batch: 4100/198 err: 1.2277\n",
      "|2019-10-21 19:42:54| epoch: 22/40 batch: 4200/198 err: 1.2539\n",
      "|2019-10-21 19:43:27| epoch: 22/40 batch: 4300/198 err: 1.2031\n",
      "|2019-10-21 19:43:59| epoch: 23/40 batch: 4400/198 err: 1.1850\n",
      "|2019-10-21 19:44:32| epoch: 23/40 batch: 4500/198 err: 1.2439\n",
      "|2019-10-21 19:45:04| epoch: 24/40 batch: 4600/198 err: 1.1885\n",
      "|2019-10-21 19:45:38| epoch: 24/40 batch: 4700/198 err: 1.2208\n",
      "|2019-10-21 19:46:10| epoch: 25/40 batch: 4800/198 err: 1.1779\n",
      "|2019-10-21 19:46:42| epoch: 25/40 batch: 4900/198 err: 1.2362\n",
      "|2019-10-21 19:47:15| epoch: 26/40 batch: 5000/198 err: 1.2149\n",
      "|2019-10-21 19:47:48| epoch: 26/40 batch: 5100/198 err: 1.1962\n",
      "|2019-10-21 19:48:20| epoch: 27/40 batch: 5200/198 err: 1.2219\n",
      "|2019-10-21 19:48:53| epoch: 27/40 batch: 5300/198 err: 1.1788\n",
      "|2019-10-21 19:49:25| epoch: 28/40 batch: 5400/198 err: 1.2013\n",
      "|2019-10-21 19:49:58| epoch: 28/40 batch: 5500/198 err: 1.1762\n",
      "|2019-10-21 19:50:29| epoch: 29/40 batch: 5600/198 err: 1.1834\n",
      "|2019-10-21 19:51:02| epoch: 29/40 batch: 5700/198 err: 1.1720\n",
      "|2019-10-21 19:51:35| epoch: 30/40 batch: 5800/198 err: 1.1639\n",
      "|2019-10-21 19:52:07| epoch: 30/40 batch: 5900/198 err: 1.1595\n",
      "|2019-10-21 19:52:39| epoch: 31/40 batch: 6000/198 err: 1.2131\n",
      "|2019-10-21 19:53:12| epoch: 31/40 batch: 6100/198 err: 1.1958\n",
      "|2019-10-21 19:53:44| epoch: 32/40 batch: 6200/198 err: 1.2206\n",
      "|2019-10-21 19:54:17| epoch: 32/40 batch: 6300/198 err: 1.1891\n",
      "|2019-10-21 19:54:49| epoch: 33/40 batch: 6400/198 err: 1.1737\n",
      "|2019-10-21 19:55:23| epoch: 33/40 batch: 6500/198 err: 1.1731\n",
      "|2019-10-21 19:55:54| epoch: 34/40 batch: 6600/198 err: 1.1781\n",
      "|2019-10-21 19:56:27| epoch: 34/40 batch: 6700/198 err: 1.1775\n",
      "|2019-10-21 19:56:59| epoch: 35/40 batch: 6800/198 err: 1.1540\n",
      "|2019-10-21 19:57:33| epoch: 35/40 batch: 6900/198 err: 1.2193\n",
      "|2019-10-21 19:58:05| epoch: 36/40 batch: 7000/198 err: 1.1478\n",
      "|2019-10-21 19:58:38| epoch: 36/40 batch: 7100/198 err: 1.1843\n",
      "|2019-10-21 19:59:10| epoch: 37/40 batch: 7200/198 err: 1.1748\n",
      "|2019-10-21 19:59:42| epoch: 37/40 batch: 7300/198 err: 1.1525\n",
      "|2019-10-21 20:00:15| epoch: 38/40 batch: 7400/198 err: 1.1342\n",
      "|2019-10-21 20:00:47| epoch: 38/40 batch: 7500/198 err: 1.1779\n",
      "|2019-10-21 20:01:19| epoch: 39/40 batch: 7600/198 err: 1.1644\n",
      "|2019-10-21 20:01:52| epoch: 39/40 batch: 7700/198 err: 1.1320\n",
      "|2019-10-21 20:02:24| epoch: 40/40 batch: 7800/198 err: 1.1567\n",
      "|2019-10-21 20:02:56| epoch: 40/40 batch: 7900/198 err: 1.1945\n"
     ]
    }
   ],
   "source": [
    "epochs = 40\n",
    "# æ¯nè½®è¿›è¡Œä¸€æ¬¡å˜é‡ä¿å­˜\n",
    "save_every_n = 200\n",
    "summary_path = './tmp/tensorboard_anna'\n",
    "base_model_path = \"./tmp/lstm_anna/i{}_l{}.ckpt\"\n",
    "\n",
    "model = CharRNN(len(char2idx), batch_size=batch_size, num_steps=num_steps,\n",
    "                lstm_size=lstm_size, num_layers=num_layers, \n",
    "                learning_rate=learning_rate, summary_path=summary_path)\n",
    "\n",
    "# ç´¢å¼•è½¬æˆå­—ç¬¦\n",
    "def _tochar(i):\n",
    "        return idx2char[i]\n",
    "_tochar_vec = np.vectorize(_tochar)\n",
    "# å–è¾“å…¥çš„x y predsçš„å­—ç¬¦æ˜ å°„ç»“æœçš„ç¬¬ä¸€ä¸ªæ ·æœ¬\n",
    "def get_sample_char(x,y,preds,verbose=False):\n",
    "    # predså…ˆreshapeä¸€ä¸‹\n",
    "    preds_reshape = preds.reshape(batch_size,num_steps,len(char2idx))\n",
    "    preds_argmax = np.array([[np.argmax(each_seq) for each_seq in each_batch] for each_batch in preds_reshape])\n",
    "    x_char,y_char,preds_char = [_tochar_vec(i) for i in [x,y,preds_argmax]]\n",
    "    if verbose:\n",
    "        print(f\"\"\"\n",
    "        >>>preds: {preds.shape}\n",
    "           |_reshape ==> {preds_reshape.shape}\n",
    "             |_argmax ==> {preds_argmax.shape}\n",
    "        \"\"\")\n",
    "\n",
    "        print(f\">>>x:{x.shape}\\n\",x,\"\\n\",x_char)\n",
    "        print(f\">>>y:{y.shape}\\n\",y,\"\\n\",y_char)\n",
    "        print(f\">>>preds_argmax:{preds_argmax.shape}\\n\",preds_argmax,\"\\n\",preds_char)\n",
    "    # è¿™æ ·å†™ä¹Ÿæ˜¯ä¸ºäº†é˜²æ­¢\\nåœ¨printçš„æ—¶å€™è‡ªåŠ¨è½¬ä¹‰æ¢è¡Œ | æ”¾åˆ°æ•°ç»„ã€å­—å…¸é‡Œå°±ä¸ä¼šprintå‡ºæ¥æ¢è¡Œäº†\n",
    "    res = {\"x\":\"\".join(x_char[0]),\"y\":\"\".join(y_char[0]),\"preds\":\"\".join(preds_char[0])}\n",
    "    return res\n",
    "                \n",
    "def print_control(cnt,info):\n",
    "    if cnt % 100 == 0:\n",
    "        zprint(info)\n",
    "#         if cnt <= 1000:\n",
    "#             # 0~1k æ¯100è¾“å‡ºä¸€æ¬¡\n",
    "#             zprint(info)\n",
    "#         elif cnt <= 10000:\n",
    "#             # 1k~10kæ¯1kè¾“å‡ºä¸€æ¬¡\n",
    "#             if cnt % 1000 == 0:\n",
    "#                 zprint(info)\n",
    "#         else:\n",
    "#             # 1wä»¥åæ¯5kè¾“å‡ºä¸€æ¬¡\n",
    "#             if cnt % 5000 == 0:\n",
    "#                 zprint(info)\n",
    "\n",
    "\n",
    "with model.graph.as_default():\n",
    "    saver = tf.train.Saver(max_to_keep=100)\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "\n",
    "        counter = 0\n",
    "        for e in range(epochs):\n",
    "            # Train network\n",
    "            new_state = sess.run(model.initial_state)\n",
    "            loss = 0\n",
    "            et_batch_cnt = len(encoded) // (batch_size*num_steps)\n",
    "            all_batch_data = get_batches(encoded, batch_size, num_steps)\n",
    "            for x, y in all_batch_data:\n",
    "                counter += 1\n",
    "                feed = {model.inputs: x,\n",
    "                        model.targets: y,\n",
    "                        model.keep_prob: keep_prob,\n",
    "                        model.initial_state: new_state}\n",
    "                preds,batch_loss, new_state, _, merged_summary = sess.run([model.prediction,\n",
    "                                                                     model.loss, \n",
    "                                                                     model.final_state, \n",
    "                                                                     model.optimizer,\n",
    "                                                                     model.merge_summary,], \n",
    "                                                                     feed_dict=feed)\n",
    "                \n",
    "                # ä¿å­˜è¿›å±•\n",
    "                model.writer.add_summary(merged_summary,counter)\n",
    "                # è¾“å‡ºprint\n",
    "                info = f\"epoch: {e+1}/{epochs} batch: {counter:0>3d}/{et_batch_cnt} err: {batch_loss:.4f}\"\n",
    "                print_control(counter,info)\n",
    "                # é¢å¤–è¾“å‡ºä¸€ä¸ªå®Œæ•´çš„å­—ç¬¦ä¸²print\n",
    "                if counter % et_batch_cnt ==0 or counter == 1:\n",
    "                    text_summary_list = [tf.summary.text(k, tf.convert_to_tensor(v)) \n",
    "                                         for k,v in get_sample_char(x,y,preds).items()]\n",
    "                    text_summary = tf.summary.merge(text_summary_list)\n",
    "                    text_summary_ = sess.run(text_summary)\n",
    "                    model.writer.add_summary(text_summary_,counter)\n",
    "                # save model graph\n",
    "                if (counter % save_every_n == 0):\n",
    "                    _=saver.save(sess, base_model_path.format(counter, lstm_size))\n",
    "\n",
    "        _=saver.save(sess, base_model_path.format(counter, lstm_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-21T12:03:05.115280Z",
     "start_time": "2019-10-21T12:03:05.107606Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "model_checkpoint_path: \"./tmp/lstm_anna/i7920_l512.ckpt\"\n",
       "all_model_checkpoint_paths: \"./tmp/lstm_anna/i200_l512.ckpt\"\n",
       "all_model_checkpoint_paths: \"./tmp/lstm_anna/i400_l512.ckpt\"\n",
       "all_model_checkpoint_paths: \"./tmp/lstm_anna/i600_l512.ckpt\"\n",
       "all_model_checkpoint_paths: \"./tmp/lstm_anna/i800_l512.ckpt\"\n",
       "all_model_checkpoint_paths: \"./tmp/lstm_anna/i1000_l512.ckpt\"\n",
       "all_model_checkpoint_paths: \"./tmp/lstm_anna/i1200_l512.ckpt\"\n",
       "all_model_checkpoint_paths: \"./tmp/lstm_anna/i1400_l512.ckpt\"\n",
       "all_model_checkpoint_paths: \"./tmp/lstm_anna/i1600_l512.ckpt\"\n",
       "all_model_checkpoint_paths: \"./tmp/lstm_anna/i1800_l512.ckpt\"\n",
       "all_model_checkpoint_paths: \"./tmp/lstm_anna/i2000_l512.ckpt\"\n",
       "all_model_checkpoint_paths: \"./tmp/lstm_anna/i2200_l512.ckpt\"\n",
       "all_model_checkpoint_paths: \"./tmp/lstm_anna/i2400_l512.ckpt\"\n",
       "all_model_checkpoint_paths: \"./tmp/lstm_anna/i2600_l512.ckpt\"\n",
       "all_model_checkpoint_paths: \"./tmp/lstm_anna/i2800_l512.ckpt\"\n",
       "all_model_checkpoint_paths: \"./tmp/lstm_anna/i3000_l512.ckpt\"\n",
       "all_model_checkpoint_paths: \"./tmp/lstm_anna/i3200_l512.ckpt\"\n",
       "all_model_checkpoint_paths: \"./tmp/lstm_anna/i3400_l512.ckpt\"\n",
       "all_model_checkpoint_paths: \"./tmp/lstm_anna/i3600_l512.ckpt\"\n",
       "all_model_checkpoint_paths: \"./tmp/lstm_anna/i3800_l512.ckpt\"\n",
       "all_model_checkpoint_paths: \"./tmp/lstm_anna/i4000_l512.ckpt\"\n",
       "all_model_checkpoint_paths: \"./tmp/lstm_anna/i4200_l512.ckpt\"\n",
       "all_model_checkpoint_paths: \"./tmp/lstm_anna/i4400_l512.ckpt\"\n",
       "all_model_checkpoint_paths: \"./tmp/lstm_anna/i4600_l512.ckpt\"\n",
       "all_model_checkpoint_paths: \"./tmp/lstm_anna/i4800_l512.ckpt\"\n",
       "all_model_checkpoint_paths: \"./tmp/lstm_anna/i5000_l512.ckpt\"\n",
       "all_model_checkpoint_paths: \"./tmp/lstm_anna/i5200_l512.ckpt\"\n",
       "all_model_checkpoint_paths: \"./tmp/lstm_anna/i5400_l512.ckpt\"\n",
       "all_model_checkpoint_paths: \"./tmp/lstm_anna/i5600_l512.ckpt\"\n",
       "all_model_checkpoint_paths: \"./tmp/lstm_anna/i5800_l512.ckpt\"\n",
       "all_model_checkpoint_paths: \"./tmp/lstm_anna/i6000_l512.ckpt\"\n",
       "all_model_checkpoint_paths: \"./tmp/lstm_anna/i6200_l512.ckpt\"\n",
       "all_model_checkpoint_paths: \"./tmp/lstm_anna/i6400_l512.ckpt\"\n",
       "all_model_checkpoint_paths: \"./tmp/lstm_anna/i6600_l512.ckpt\"\n",
       "all_model_checkpoint_paths: \"./tmp/lstm_anna/i6800_l512.ckpt\"\n",
       "all_model_checkpoint_paths: \"./tmp/lstm_anna/i7000_l512.ckpt\"\n",
       "all_model_checkpoint_paths: \"./tmp/lstm_anna/i7200_l512.ckpt\"\n",
       "all_model_checkpoint_paths: \"./tmp/lstm_anna/i7400_l512.ckpt\"\n",
       "all_model_checkpoint_paths: \"./tmp/lstm_anna/i7600_l512.ckpt\"\n",
       "all_model_checkpoint_paths: \"./tmp/lstm_anna/i7800_l512.ckpt\"\n",
       "all_model_checkpoint_paths: \"./tmp/lstm_anna/i7920_l512.ckpt\""
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tf.train.get_checkpoint_state(\"./tmp/lstm_anna\").all_model_checkpoint_paths)\n",
    "tf.train.get_checkpoint_state(\"./tmp/lstm_anna\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ç”Ÿæˆ\n",
    "- éœ€è¦æŒ‡å®š`n_samples`ï¼šéœ€è¦ç”Ÿæˆå¤šé•¿çš„å­—ç¬¦ä¸²\n",
    "- å°†è¾“å…¥çš„å•è¯è½¬æ¢ä¸ºå•ä¸ªå­—ç¬¦ç»„æˆçš„list\n",
    "- ä»ç¬¬ä¸€ä¸ªå­—ç¬¦å¼€å§‹è¾“å…¥CharRNN\n",
    "- ä»é¢„æµ‹ç»“æœä¸­é€‰å–å‰top_nä¸ªæœ€å¯èƒ½çš„å­—ç¬¦ï¼ŒæŒ‰é¢„æµ‹ç»“æœæä¾›çš„å„ä¸ªå­—ç¬¦çš„æ¦‚ç‡è¿›è¡Œnp.random.choice\n",
    " - `pick_top_n`é‡Œæ·»åŠ äº†`copy()`æ–¹æ³•ï¼Œé¿å…ç›´æ¥æ›´æ”¹å‚æ•°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-21T12:03:05.566569Z",
     "start_time": "2019-10-21T12:03:05.558599Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def pick_top_n(preds_, vocab_size, top_n=5, random=False):\n",
    "    \"\"\"\n",
    "    ä»é¢„æµ‹ç»“æœä¸­é€‰å–å‰top_nä¸ªæœ€å¯èƒ½çš„å­—ç¬¦ï¼ŒæŒ‰é¢„æµ‹ç»“æœæä¾›çš„å„ä¸ªå­—ç¬¦çš„æ¦‚ç‡è¿›è¡Œnp.random.choice\n",
    "    \n",
    "    preds_: é¢„æµ‹ç»“æœ\n",
    "    vocab_size\n",
    "    top_n\n",
    "    \"\"\"\n",
    "    preds = preds_.copy()  # é¿å…æ”¹å˜åŸpreds\n",
    "    p = np.squeeze(preds)\n",
    "    # å°†é™¤äº†top_nä¸ªé¢„æµ‹å€¼çš„ä½ç½®éƒ½ç½®ä¸º0\n",
    "    p[np.argsort(p)[-top_n:]] = 0\n",
    "    # å½’ä¸€åŒ–æ¦‚ç‡\n",
    "    p = p / np.sum(p)\n",
    "    # éšæœºé€‰å–ä¸€ä¸ªå­—ç¬¦ / æˆ–è€…å–æ¦‚ç‡æœ€å¤§çš„å­—ç¬¦\n",
    "    c = np.random.choice(vocab_size, 1, p=p)[0] if random else np.argmax(preds)\n",
    "    return c\n",
    "\n",
    "def sample(checkpoint, n_samples, lstm_size,num_layers, vocab_size, prime=\"The \", random=False):\n",
    "    \"\"\"\n",
    "    ç”Ÿæˆæ–°æ–‡æœ¬\n",
    "    \n",
    "    checkpoint: æŸä¸€è½®è¿­ä»£çš„å‚æ•°æ–‡ä»¶\n",
    "    n_sample: æ–°é—»æœ¬çš„å­—ç¬¦é•¿åº¦\n",
    "    lstm_size: éšå±‚ç»“ç‚¹æ•°\n",
    "    vocab_size\n",
    "    prime: èµ·å§‹æ–‡æœ¬\n",
    "    \"\"\"\n",
    "    # å°†è¾“å…¥çš„å•è¯è½¬æ¢ä¸ºå•ä¸ªå­—ç¬¦ç»„æˆçš„list\n",
    "    samples = [c for c in prime]\n",
    "    print(f\">>> samples: {samples}\")\n",
    "    # sampling=Trueæ„å‘³ç€batchçš„size=1 x 1\n",
    "    model = CharRNN(len(char2idx), batch_size=1, num_steps=len(prime),\n",
    "                    lstm_size=lstm_size, num_layers=num_layers, \n",
    "                    learning_rate=learning_rate)\n",
    "    with model.graph.as_default():\n",
    "        saver = tf.train.Saver()\n",
    "\n",
    "        with tf.Session(config=sess_conf) as sess:\n",
    "            # åŠ è½½æ¨¡å‹å‚æ•°ï¼Œæ¢å¤è®­ç»ƒ\n",
    "            saver.restore(sess, checkpoint)\n",
    "            feed = {model.inputs: np.array([char2idx[c] for c in prime]),\n",
    "                    model.keep_prob: 1.,}\n",
    "            preds, new_state = sess.run([model.prediction, model.final_state], \n",
    "                                        feed_dict=feed,\n",
    "                                        options=run_opt)\n",
    "            top5_prob=preds[0][np.argsort(preds[0])[-5:]]\n",
    "            top5_idx = np.argsort(preds[0])[-5:]\n",
    "            print(f\">>> å¯¹æ•´ä¸ªprime: {prime} çš„é¢„æµ‹ç»“æœ  [shape]:{preds.shape}\")\n",
    "            print(f\"    top5æ˜¯:{top5_prob}<==>{[idx2char[i] for i in top5_idx]}\")\n",
    "            next_char = pick_top_n(preds, vocab_size, random=random)\n",
    "            print(f\"    å¦‚æœæ­¤æ—¶é€‰å–topNç”Ÿæˆå­—ç¬¦(æ˜¯å¦éšæœº:{random})ï¼Œä¼šæ˜¯: [idx]:'{next_char}' [char]:'{idx2char[next_char]}'\")\n",
    "            \n",
    "            \n",
    "            # æ·»åŠ å­—ç¬¦åˆ°samplesä¸­\n",
    "            samples.append(idx2char[c])\n",
    "            \n",
    "            inp = np.array([[c]])\n",
    "            # ä¸æ–­ç”Ÿæˆå­—ç¬¦ï¼Œç›´åˆ°è¾¾åˆ°æŒ‡å®šæ•°ç›®\n",
    "            for _ in range(n_samples):\n",
    "                feed = {model.inputs: [[c]],\n",
    "                        model.keep_prob: 1.}\n",
    "                preds, new_state = sess.run([model.prediction, model.final_state], \n",
    "                                            feed_dict=feed,\n",
    "                                            options=run_opt)\n",
    "\n",
    "                c = pick_top_n(preds, vocab_size, random=random)\n",
    "                samples.append(idx2char[c])\n",
    "\n",
    "    return ''.join(samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-22T02:12:41.986369Z",
     "start_time": "2019-10-22T02:12:41.902747Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> samples: ['F', 'a', 'r']\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Error converting shape to a TensorShape: int() argument must be a string, a bytes-like object or a number, not 'Tensor'.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m~/python3/lib/python3.6/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mmake_shape\u001b[0;34m(v, arg_name)\u001b[0m\n\u001b[1;32m    140\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 141\u001b[0;31m     \u001b[0mshape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtensor_shape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    142\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/python3/lib/python3.6/site-packages/tensorflow/python/framework/tensor_shape.py\u001b[0m in \u001b[0;36mas_shape\u001b[0;34m(shape)\u001b[0m\n\u001b[1;32m    946\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 947\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mTensorShape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    948\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/python3/lib/python3.6/site-packages/tensorflow/python/framework/tensor_shape.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, dims)\u001b[0m\n\u001b[1;32m    541\u001b[0m         \u001b[0;31m# Got a list of dimensions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 542\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dims\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mas_dimension\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0md\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdims_iter\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    543\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ndims\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/python3/lib/python3.6/site-packages/tensorflow/python/framework/tensor_shape.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    541\u001b[0m         \u001b[0;31m# Got a list of dimensions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 542\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dims\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mas_dimension\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0md\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdims_iter\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    543\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ndims\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/python3/lib/python3.6/site-packages/tensorflow/python/framework/tensor_shape.py\u001b[0m in \u001b[0;36mas_dimension\u001b[0;34m(value)\u001b[0m\n\u001b[1;32m    481\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 482\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mDimension\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    483\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/python3/lib/python3.6/site-packages/tensorflow/python/framework/tensor_shape.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, value)\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m       if (not isinstance(value, compat.bytes_or_text_types) and\n",
      "\u001b[0;31mTypeError\u001b[0m: int() argument must be a string, a bytes-like object or a number, not 'Tensor'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-44-ceff95474e60>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_default_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mckpt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlatest_checkpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./tmp/lstm_anna'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mckpt\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mn_samples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2000\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlstm_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlstm_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnum_layers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_layers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvocab_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchar2idx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mprime\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Far\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-40-576aa10da45e>\u001b[0m in \u001b[0;36msample\u001b[0;34m(checkpoint, n_samples, lstm_size, num_layers, vocab_size, prime, random)\u001b[0m\n\u001b[1;32m     34\u001b[0m                     \u001b[0mlstm_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlstm_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_layers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_layers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m                     \u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m                     sampling=True)\n\u001b[0m\u001b[1;32m     37\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0msaver\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSaver\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-43-dd85fccbe90c>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, num_classes, batch_size, num_steps, lstm_size, num_layers, learning_rate, grad_clip, summary_path, sampling)\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m             \u001b[0;31m# è¾“å…¥å±‚\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtargets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeep_prob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_steps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m             \u001b[0;31m# LSTMå±‚\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-1b2d3ea953ee>\u001b[0m in \u001b[0;36mbuild_inputs\u001b[0;34m(num_seqs, num_steps)\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mnum_steps\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mæ¯ä¸ªåºåˆ—åŒ…å«çš„å­—ç¬¦æ•°\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     '''\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplaceholder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_seqs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_steps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'inputs'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0mtargets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplaceholder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_seqs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_steps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'targets'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/python3/lib/python3.6/site-packages/tensorflow/python/ops/array_ops.py\u001b[0m in \u001b[0;36mplaceholder\u001b[0;34m(dtype, shape, name)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                        \"eager execution.\")\n\u001b[1;32m   1746\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1747\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mgen_array_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplaceholder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/python3/lib/python3.6/site-packages/tensorflow/python/ops/gen_array_ops.py\u001b[0m in \u001b[0;36mplaceholder\u001b[0;34m(dtype, shape, name)\u001b[0m\n\u001b[1;32m   5202\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mshape\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5203\u001b[0m       \u001b[0mshape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5204\u001b[0;31m     \u001b[0mshape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_execute\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"shape\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5205\u001b[0m     _, _, _op = _op_def_lib._apply_op_helper(\n\u001b[1;32m   5206\u001b[0m         \"Placeholder\", dtype=dtype, shape=shape, name=name)\n",
      "\u001b[0;32m~/python3/lib/python3.6/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mmake_shape\u001b[0;34m(v, arg_name)\u001b[0m\n\u001b[1;32m    141\u001b[0m     \u001b[0mshape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtensor_shape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 143\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Error converting %s to a TensorShape: %s.\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0marg_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    144\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m     raise ValueError(\"Error converting %s to a TensorShape: %s.\" % (arg_name,\n",
      "\u001b[0;31mTypeError\u001b[0m: Error converting shape to a TensorShape: int() argument must be a string, a bytes-like object or a number, not 'Tensor'."
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "ckpt = tf.train.latest_checkpoint('./tmp/lstm_anna')\n",
    "sample(ckpt,n_samples=2000,lstm_size=lstm_size,num_layers=num_layers,vocab_size=len(char2idx),prime=\"Far\", random=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CharRNN å†…éƒ¨ç»†èŠ‚çš„æµ‹è¯•"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-16T09:36:25.792303Z",
     "start_time": "2019-10-16T09:36:21.664666Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with open(result_set_fp+\".pickle\",\"rb+\") as frb:\n",
    "    char2idx = pickle.load(frb)\n",
    "\n",
    "encoded = np.load(coded_article_fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-16T09:38:08.801334Z",
     "start_time": "2019-10-16T09:38:08.793462Z"
    },
    "code_folding": [
     0
    ],
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_batches_as_iter(encoded, batch_size, time_steps, verbose=False):\n",
    "    chunk_len = batch_size*time_steps \n",
    "    n_chunk = int(len(encoded)/chunk_len)\n",
    "    arr = encoded[:chunk_len*n_chunk]  # æˆªå–æ•´æ•°å€çš„batch_size\n",
    "    arr = arr.reshape((batch_size,-1))\n",
    "\n",
    "    for n in range(0, arr.shape[1], time_steps):\n",
    "        x = arr[:, n:n+time_steps]\n",
    "        y = np.zeros_like(x)\n",
    "        y[:, :-1], y[:, -1] = x[:,1:], y[:, 0] \n",
    "        yield x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-16T09:57:25.505075Z",
     "start_time": "2019-10-16T09:57:25.495281Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> default_BS: 20\n",
      ">>> default_x: (20, 10)\n",
      " [[1915 1991 1096   49 1991 1117 1991 1099  189  189]\n",
      " [1991   33  328  184 2154 2154 1925  328 1220   33]\n",
      " [1220  151 1925 1220 1544 2208 2154 1220 1176  151]]\n",
      ">>> default_y: (20, 10)\n",
      " [[1991 1096   49 1991 1117 1991 1099  189  189 1991]\n",
      " [  33  328  184 2154 2154 1925  328 1220   33   33]\n",
      " [ 151 1925 1220 1544 2208 2154 1220 1176  151  151]]\n"
     ]
    }
   ],
   "source": [
    "time_steps = 10\n",
    "lstm_layers = [256]*2\n",
    "lstm_size = lstm_layers[0]\n",
    "num_classes = len(char2idx)\n",
    "default_BS = 20\n",
    "default_x,default_y=list(itertools.islice(get_batches_as_iter(encoded, batch_size=default_BS, time_steps=time_steps),1))[0]\n",
    "print(f\">>> default_BS: {default_BS}\")\n",
    "print(f\">>> default_x: {default_x.shape}\\n\",default_x[:3,:10])\n",
    "print(f\">>> default_y: {default_y.shape}\\n\",default_y[:3,:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-17T04:29:28.374632Z",
     "start_time": "2019-10-17T04:29:26.787047Z"
    },
    "code_folding": [
     9
    ],
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ">>> æµç¨‹å¦‚ä¸‹\n",
      "    inpX: (20, 10)\n",
      "    +onehot=> X: (20, 10, 83)\n",
      "    +mlstm=> lstm_output: (20, 10, 256)\n",
      "    +reshape=> softmax_x: (200, 256)\n",
      "    +softmax(just matmul)=> logits: (200, 83)\n",
      "    \n",
      "    inpY: (20, 10)\n",
      "    +onehot=> Y: (20, 10, 83)\n",
      "    +reshape=> y_reshaped: (200, 83)\n",
      "    \n",
      "    CE(logits,y_reshaped): (200,)\n",
      "    +reduce_mean=> loss: (),scalar:0.1547\n",
      "    \n",
      ">>> X_: (20, 10, 83)\n",
      "\n",
      ">>> Y_: (20, 10, 83)\n",
      "\n",
      ">>> lstm_final_state:\n",
      "    >>> [layer]:0 [c_state:]: (20, 256)\n",
      "\n",
      "    >>> [layer]:0 [h_state:]: (20, 256)\n",
      "\n",
      "    >>> [layer]:1 [c_state:]: (20, 256)\n",
      "\n",
      "    >>> [layer]:1 [h_state:]: (20, 256)\n",
      "\n",
      ">>> lstm_output: (20, 10, 256)\n",
      "\n",
      ">>> seq_output_: (20, 10, 256)\n",
      "\n",
      "seq_output çš„ç¡®æ²¡æœ‰èµ·åˆ°ä½œç”¨,tfä¸­å¯¹ä¸€ä¸ªtensorä½¿ç”¨concatä»€ä¹ˆéƒ½ä¸ä¼šæ”¹å˜ï¼Œä¸€èˆ¬æ˜¯å¯¹ä¸€ä¸ªå†…éƒ¨å…ƒç´ æ˜¯tensorçš„liståšconcat\n",
      ">>> sf_x: (200, 256) sf_w: (256, 83) sf_b: (83,)\n",
      ">>> logits_: (200, 83) pred_: (200, 83)\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "# placeholder\n",
    "inpBS = tf.placeholder(tf.int32, [], name=\"batch_size\")\n",
    "inpX = tf.placeholder(tf.int32, shape=(None, time_steps), name=\"inpX\")\n",
    "inpY = tf.placeholder(tf.int32, shape=(None), name=\"inpY\")\n",
    "X = tf.one_hot(inpX, depth=len(char2idx))\n",
    "Y = tf.one_hot(inpY, depth=len(char2idx))\n",
    "# LSTM æ„å»º\n",
    "lstm_cell_list = []\n",
    "for nodes_size in lstm_layers:\n",
    "    lstm = tf.contrib.rnn.BasicLSTMCell(nodes_size)\n",
    "    lstm_dropout = tf.contrib.rnn.DropoutWrapper(lstm, output_keep_prob=0.8)\n",
    "    lstm_cell_list.append(lstm_dropout)\n",
    "mlstm_cell = tf.contrib.rnn.MultiRNNCell(lstm_cell_list)\n",
    "initial_state = mlstm_cell.zero_state(inpBS, tf.float32)\n",
    "lstm_output, lstm_final_state = tf.nn.dynamic_rnn(mlstm_cell, X, initial_state = initial_state)\n",
    "\n",
    "# formt output\n",
    "seq_output = tf.concat(lstm_output, axis=1) \n",
    "softmax_x = tf.reshape(seq_output, [-1, lstm_size])\n",
    "softmax_w = tf.Variable(tf.truncated_normal([lstm_size, num_classes], stddev=0.1))\n",
    "softmax_b = tf.Variable(tf.zeros(num_classes))\n",
    "logits = tf.matmul(softmax_x, softmax_w) + softmax_b\n",
    "pred = tf.nn.softmax(logits, name='predictions')\n",
    "\n",
    "# è®¡ç®—loss\n",
    "y_reshaped = tf.reshape(Y, [-1, num_classes])\n",
    "loss_ce = tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y_reshaped)\n",
    "loss = tf.reduce_mean(loss_ce)\n",
    "\n",
    "# optimize\n",
    "tvars = tf.trainable_variables()\n",
    "grad_clip = 5\n",
    "grads, _ = tf.clip_by_global_norm(tf.gradients(loss, tvars), grad_clip)\n",
    "train_op = tf.train.AdamOptimizer(0.01)\n",
    "optimizer = train_op.apply_gradients(zip(grads, tvars))\n",
    "\n",
    "default_feed = {inpBS:default_BS, inpX:default_x, inpY:default_y}\n",
    "with tf.Session(config=sess_conf) as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    inpX_,X_,inpY_,Y_,is_,lo_,lfs_each_layer = sess.run([inpX,X,inpY,Y,initial_state,lstm_output,lstm_final_state], feed_dict=default_feed)\n",
    "    seq_output_,sf_x,sf_w,sf_b = sess.run([seq_output,softmax_x,softmax_w,softmax_b], feed_dict=default_feed)\n",
    "    logits_,pred_,y_reshaped_ = sess.run([logits,pred,y_reshaped], feed_dict=default_feed)\n",
    "    loss_ce_,loss_,_ = sess.run([loss_ce,loss,optimizer],feed_dict=default_feed)\n",
    "    print(f\"\"\"\\n>>> æµç¨‹å¦‚ä¸‹\n",
    "    inpX: {inpX_.shape}\n",
    "    +onehot=> X: {X_.shape}\n",
    "    +mlstm=> lstm_output: {lo_.shape}\n",
    "    +reshape=> softmax_x: {sf_x.shape}\n",
    "    +softmax(just matmul)=> logits: {logits_.shape}\n",
    "    \n",
    "    inpY: {inpY_.shape}\n",
    "    +onehot=> Y: {Y_.shape}\n",
    "    +reshape=> y_reshaped: {y_reshaped_.shape}\n",
    "    \n",
    "    CE(logits,y_reshaped): {loss_ce_.shape}\n",
    "    +reduce_mean=> loss: {loss_.shape},scalar:{loss_:.4f}\n",
    "    \"\"\"\n",
    "    )\n",
    "    print(f\">>> X_: {X_.shape}\\n\")\n",
    "    print(f\">>> Y_: {Y_.shape}\\n\")\n",
    "    print(\">>> lstm_final_state:\")\n",
    "    for idx,lfs in enumerate(lfs_each_layer):\n",
    "        print(f\"    >>> [layer]:{idx} [c_state:]: {lfs.c.shape}\\n\")\n",
    "        print(f\"    >>> [layer]:{idx} [h_state:]: {lfs.h.shape}\\n\")\n",
    "    print(f\">>> lstm_output: {lo_.shape}\\n\")\n",
    "    print(f\">>> seq_output_: {seq_output_.shape}\\n\")\n",
    "    print(\"seq_output çš„ç¡®æ²¡æœ‰èµ·åˆ°ä½œç”¨,tfä¸­å¯¹ä¸€ä¸ªtensorä½¿ç”¨concatä»€ä¹ˆéƒ½ä¸ä¼šæ”¹å˜ï¼Œä¸€èˆ¬æ˜¯å¯¹ä¸€ä¸ªå†…éƒ¨å…ƒç´ æ˜¯tensorçš„liståšconcat\")\n",
    "    print(f\">>> sf_x: {sf_x.shape} sf_w: {sf_w.shape} sf_b: {sf_b.shape}\")\n",
    "    print(f\">>> logits_: {logits_.shape} pred_: {pred_.shape}\")\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Initialization Cell",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "205.355px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
