{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-07T02:52:29.154730Z",
     "start_time": "2020-04-07T02:52:29.143558Z"
    }
   },
   "outputs": [],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "%matplotlib inline\n",
    "from tqdm.auto import tqdm\n",
    "import concurrent.futures\n",
    "from multiprocessing import Pool\n",
    "import copy,os,sys,psutil\n",
    "from collections import Counter,deque\n",
    "import itertools\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-07T02:52:30.103416Z",
     "start_time": "2020-04-07T02:52:30.100809Z"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_addons as tfa\n",
    "import tensorflow_hub as hub\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-27T10:33:38.844898Z",
     "start_time": "2020-03-27T10:33:38.840622Z"
    }
   },
   "outputs": [],
   "source": [
    "baseDir=\"/home/zhoutong/notebook_collection/tmp/NLP_ner/lstmcrf\"\n",
    "checkpoint_dir = baseDir+\"/ckpt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SubclassedM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BiLSTMCRF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-08T08:38:19.661450Z",
     "start_time": "2020-04-08T08:38:19.642934Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# 参考： https://github.com/saiwaiyanyu/bi-lstm-crf-ner-tf2.0/blob/master/train.py\n",
    "class LSTMCRF(tf.keras.Model):\n",
    "    def __init__(self, label_size, lstm_units, vocab_size, emb_dim):\n",
    "        super().__init__()\n",
    "        self.label_size = label_size\n",
    "        self.lstm_units = lstm_units\n",
    "        self.vocab_size = vocab_size\n",
    "        self.emb_dim = emb_dim\n",
    "\n",
    "        self.embedding = tf.keras.layers.Embedding(self.vocab_size, self.emb_dim)\n",
    "        self.bilstm = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(self.lstm_units, return_sequences=True))\n",
    "#         self.dense = tf.keras.layers.Dense(label_size,activation=\"softmax\")\n",
    "        self.dense = tf.keras.layers.Dense(label_size) # 正统做法是没有激活函数没做归一化\n",
    "        self.dropout = tf.keras.layers.Dropout(0.5)\n",
    "\n",
    "        self.transition_params = tf.Variable(tf.random.uniform(shape=(self.label_size, self.label_size)),trainable=False)\n",
    "\n",
    "    # @tf.function\n",
    "    def call(self, text,labels=None,training=None):\n",
    "        seq_lens = tf.math.reduce_sum(tf.cast(tf.math.not_equal(text, 0), dtype=tf.int32), axis=-1)\n",
    "        # -1 change 0·\n",
    "        inputs = self.embedding(text)\n",
    "        inputs = self.dropout(inputs, training)\n",
    "        logits = self.dense(self.bilstm(inputs))\n",
    "\n",
    "        if labels is not None:\n",
    "            label_sequences = tf.convert_to_tensor(labels, dtype=tf.int32)\n",
    "            log_likelihood, self.transition_params = tfa.text.crf_log_likelihood(logits, label_sequences, seq_lens)\n",
    "            self.transition_params = tf.Variable(self.transition_params, trainable=False)\n",
    "            return logits, seq_lens, log_likelihood\n",
    "        else:\n",
    "            return logits, seq_lens\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERTLayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-02T03:18:50.518956Z",
     "start_time": "2020-04-02T03:18:50.514649Z"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow_hub as hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     1,
     3,
     5,
     7
    ]
   },
   "outputs": [],
   "source": [
    "max_seq_length = 128  # Your choice here.\n",
    "input_word_ids = tf.keras.layers.Input(shape=(max_seq_length,), dtype=tf.int32,\n",
    "                                       name=\"input_word_ids\")\n",
    "input_mask = tf.keras.layers.Input(shape=(max_seq_length,), dtype=tf.int32,\n",
    "                                   name=\"input_mask\")\n",
    "segment_ids = tf.keras.layers.Input(shape=(max_seq_length,), dtype=tf.int32,\n",
    "                                    name=\"segment_ids\")\n",
    "bert_layer = hub.KerasLayer(\"https://tfhub.dev/tensorflow/bert_zh_L-12_H-768_A-12/1\",\n",
    "                            trainable=True)\n",
    "pooled_output, sequence_output = bert_layer([input_word_ids, input_mask, segment_ids])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "class BertLayer(tf.layers.Layer):\n",
    "    def __init__(self, n_fine_tune_layers=10, **kwargs):\n",
    "        self.n_fine_tune_layers = n_fine_tune_layers\n",
    "        self.trainable = True\n",
    "        self.output_size = 768\n",
    "        super(BertLayer, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.bert = hub.Module(\n",
    "            bert_path,\n",
    "            trainable=self.trainable,\n",
    "            name=\"{}_module\".format(self.name)\n",
    "        )\n",
    "        trainable_vars = self.bert.variables\n",
    "        \n",
    "        # Remove unused layers\n",
    "        trainable_vars = [var for var in trainable_vars if not \"/cls/\" in var.name]\n",
    "        \n",
    "        # Select how many layers to fine tune\n",
    "        trainable_vars = trainable_vars[-self.n_fine_tune_layers :]\n",
    "        \n",
    "        # Add to trainable weights\n",
    "        for var in trainable_vars:\n",
    "            self._trainable_weights.append(var)\n",
    "        \n",
    "        # Add non-trainable weights\n",
    "        for var in self.bert.variables:\n",
    "            if var not in self._trainable_weights:\n",
    "                self._non_trainable_weights.append(var)\n",
    "        \n",
    "        super(BertLayer, self).build(input_shape)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        inputs = [K.cast(x, dtype=\"int32\") for x in inputs]\n",
    "        input_ids, input_mask, segment_ids = inputs\n",
    "        bert_inputs = dict(\n",
    "            input_ids=input_ids, input_mask=input_mask, segment_ids=segment_ids\n",
    "        )\n",
    "        result = self.bert(inputs=bert_inputs, signature=\"tokens\", as_dict=True)[\n",
    "            \"pooled_output\"\n",
    "        ]\n",
    "        return result\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (input_shape[0], self.output_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LoadData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 489,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-14T12:15:20.588066Z",
     "start_time": "2020-04-14T12:15:15.896114Z"
    }
   },
   "outputs": [],
   "source": [
    "pad_length = 100\n",
    "target_fp=baseDir+\"/data/renminribao/target_BIO_2014_cropus.txt\"\n",
    "source_fp=baseDir+\"/data/renminribao/source_BIO_2014_cropus.txt\"\n",
    "with open(target_fp,\"r\") as fr:\n",
    "    target=[i.strip() for i in fr.readlines()]\n",
    "with open(source_fp,\"r\") as fr:\n",
    "    source=[i.strip() for i in fr.readlines()]\n",
    "\n",
    "# 所有样本\n",
    "sample_iter=zip(source,target)\n",
    "# 词表\n",
    "all_words=set([char for sentence in source for char in sentence.split(\" \") if char != ''])\n",
    "word2idx = dict((word,idx+1) for idx,word in enumerate(all_words))\n",
    "word2idx.update({\"PAD\":0})\n",
    "# 标注label\n",
    "all_tags=set([tag for tags in target for tag in tags.split(\" \") if tag != ''])\n",
    "tag2idx = dict((tag,idx) for idx,tag in enumerate(all_tags))\n",
    "# tag2idx.update({\"PAD\":0})\n",
    "\n",
    "# 训练集测试集\n",
    "def _yield_samples(source_inp,target_inp):\n",
    "    for sen,labels in zip(source_inp,target_inp):\n",
    "        X = ([w for w in sen.split(\" \")]+['PAD']*pad_length)[:pad_length]\n",
    "        X = [word2idx[w] for w in X]\n",
    "        Y = ([w for w in labels.split(\" \")]+['O']*pad_length)[:pad_length]\n",
    "        Y = [tag2idx[l] for l in Y]\n",
    "        yield (X,Y)\n",
    "\n",
    "total_size = len(source)\n",
    "train_size = int(total_size*0.7)\n",
    "train_dataset = _yield_samples(source[:train_size],target[:train_size])\n",
    "test_dataset = _yield_samples(source[train_size:],target[train_size:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 482,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-14T12:12:36.610299Z",
     "start_time": "2020-04-14T12:12:36.527083Z"
    }
   },
   "outputs": [],
   "source": [
    "idx2word=dict([(v,k) for k,v in word2idx.items()])\n",
    "idx2tag=dict([(v,k) for k,v in tag2idx.items()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 475,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-14T12:05:39.794262Z",
     "start_time": "2020-04-14T12:05:39.775839Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('人 民 网 1 月 1 日 讯 据 《 纽 约 时 报 》 报 道 , 美 国 华 尔 街 股 市 在 2 0 1 3 年 的 最 后 一 天 继 续 上 涨 , 和 全 球 股 市 一 样 , 都 以 最 高 纪 录 或 接 近 最 高 纪 录 结 束 本 年 的 交 易 。',\n",
       " 'O O O B_T I_T I_T I_T O O O B_LOC I_LOC O O O O O O B_LOC I_LOC I_LOC I_LOC I_LOC O O O B_T I_T I_T I_T I_T O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O')"
      ]
     },
     "execution_count": 475,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "人 民 网 1 月 1 日 讯 据 《 纽 约 时 报 》 报 道 , 美 国 华 尔 街 股 市 在 2 0 1 3 年 的 最 后 一 天 继 续 上 涨 , 和 全 球 股 市 一 样 , 都 以 最 高 纪 录 或 接 近 最 高 纪 录 结 束 本 年 的 交 易 。 \n",
      " [5235, 3477, 628, 3238, 1756, 3238, 4392, 2325, 1762, 18, 2960, 2119, 2604, 394, 2827, 394, 3983, 1376, 1321, 4679, 6015, 2643, 5759, 4508, 2073, 5981, 1198, 3487, 3238, 4295, 2235, 1129, 1090, 1079, 5964, 1241, 1091, 4083, 3167, 3457, 1376, 5860, 1698, 5248, 4508, 2073, 5964, 3910, 1376, 4979, 843, 1090, 3128, 4074, 5030, 3197, 1537, 4389, 1090, 3128, 4074, 5030, 5425, 4839, 525, 2235, 1129, 377, 2916, 2900]\n",
      "O O O B_T I_T I_T I_T O O O B_LOC I_LOC O O O O O O B_LOC I_LOC I_LOC I_LOC I_LOC O O O B_T I_T I_T I_T I_T O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O \n",
      " [7, 7, 7, 6, 4, 4, 4, 7, 7, 7, 8, 3, 7, 7, 7, 7, 7, 7, 8, 3, 3, 3, 3, 7, 7, 7, 6, 4, 4, 4, 4, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([5235, 3477, 628, 3238], [7, 7, 7, 6])"
      ]
     },
     "execution_count": 475,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample = sample_iter.__next__()\n",
    "sample\n",
    "sen,tag = sample\n",
    "sen_idx=[word2idx[i] for i in sen.split(\" \")]\n",
    "tag_idx=[tag2idx[i] for i in tag.split(\" \")]\n",
    "print(sen,\"\\n\",sen_idx)\n",
    "print(tag,\"\\n\",tag_idx)\n",
    "\n",
    "sen_idx = sen_idx[:4]\n",
    "tag_idx = tag_idx[:4]\n",
    "sen_idx,tag_idx\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# InitModel | Params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 511,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-14T12:19:56.673335Z",
     "start_time": "2020-04-14T12:19:56.080251Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'label_size': 9, 'lstm_units': 8, 'vocab_size': 6364, 'emb_dim': 300}\n",
      "Model: \"lstmcrf_12\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_12 (Embedding)     multiple                  1909200   \n",
      "_________________________________________________________________\n",
      "bidirectional_12 (Bidirectio multiple                  19776     \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             multiple                  153       \n",
      "_________________________________________________________________\n",
      "dropout_12 (Dropout)         multiple                  0         \n",
      "=================================================================\n",
      "Total params: 1,929,210\n",
      "Trainable params: 1,929,129\n",
      "Non-trainable params: 81\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "config={\n",
    "    \"label_size\":len(tag2idx),\n",
    "    \"lstm_units\":8,\n",
    "    \"vocab_size\":len(word2idx),\n",
    "    \"emb_dim\":300\n",
    "}\n",
    "print(config)\n",
    "M = LSTMCRF(**config)\n",
    "\n",
    "M.build((None,20))\n",
    "M.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## pre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 525,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-14T12:31:41.101844Z",
     "start_time": "2020-04-14T12:31:41.083009Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "##################################################\n",
    "# tf.train.Checkpoint 不能存模型内部有Model类变量的模型\n",
    "# tf.train.CheckpointManager 不能在保存时指定prefix \n",
    "# 自行包装一个类，主要使用model.save_weights\n",
    "##################################################\n",
    "class CustomCkpt:\n",
    "    def __init__(self, ckpt_dir, model=None,max_keep=20):\n",
    "        self.ckpt_dir = ckpt_dir\n",
    "        self.best_valid_acc = 0.0\n",
    "        self.best_valid_loss = 1e10\n",
    "        self.total_saved_fps = []\n",
    "        self.history = []\n",
    "        self.model = model\n",
    "        self.max_keep=max_keep\n",
    "         \n",
    "    def save(self,val_acc,val_loss,model=None,fileName=None):\n",
    "        if model is not None:\n",
    "            self.model = model\n",
    "        assert self.model is not None, \"初始化时未指定model则.save()必须提供model\"\n",
    "        if fileName is None:\n",
    "            save_fp = os.path.join(self.ckpt_dir,\"ckpt_\"+len(self.total_saved_fps))\n",
    "        else:\n",
    "            save_fp = os.path.join(self.ckpt_dir,fileName)\n",
    "        saved = False # 因为acc和loss提升时都要打log并更新best，但是保存只存一次\n",
    "        if val_acc > self.best_valid_acc:\n",
    "            print(f\"acc improved [from]:{self.best_valid_acc:.4f} [to]:{val_acc:.4f}.\")\n",
    "            self.best_valid_acc = val_acc\n",
    "            if not saved:\n",
    "                self.model.save_weights(save_fp)\n",
    "                saved=True\n",
    "        if val_loss < self.best_valid_loss:\n",
    "            print(f\"loss improved [from]:{self.best_valid_loss:.4f} [to]:{val_loss:.4f}.\")\n",
    "            self.best_valid_loss = val_loss\n",
    "            if not saved:\n",
    "                self.model.save_weights(save_fp)\n",
    "                saved=True\n",
    "        # 限制最多保存文件个数\n",
    "        if saved:\n",
    "            print(f\"[ckpt-path]: {save_fp}\")\n",
    "            self.total_saved_fps.append(save_fp)\n",
    "            if len(self.total_saved_fps) >= self.max_keep:\n",
    "                toDel_fp = self.total_saved_fps.pop(0)\n",
    "                status,output=subprocess.getstatusoutput(f\"rm {toDel_fp}*\")\n",
    "        \n",
    "        self.history.append({\"val_acc\":val_acc, \"val_loss\":val_loss})\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 526,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-14T12:31:42.232737Z",
     "start_time": "2020-04-14T12:31:42.226493Z"
    },
    "code_folding": [
     5,
     10
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "历史tbd信息已删除\n"
     ]
    }
   ],
   "source": [
    "######\n",
    "# tbd\n",
    "######\n",
    "opt=tf.keras.optimizers.Adam()\n",
    "tbd_dir = baseDir+\"/tensorboard\"\n",
    "if os.path.exists(tbd_dir):\n",
    "    import shutil\n",
    "    shutil.rmtree(tbd_dir)\n",
    "    print(\"历史tbd信息已删除\")\n",
    "summary_writer = tf.summary.create_file_writer(tbd_dir)\n",
    "with summary_writer.as_default():\n",
    "    pass\n",
    "#     _=tf.summary.image(\"Trainning Data\", normal_flow_train[0][0], max_outputs=4, step=0)\n",
    "\n",
    "    \n",
    "#######\n",
    "# ckpt\n",
    "#######\n",
    "ckpt_saver = CustomCkpt(checkpoint_dir)\n",
    "\n",
    "##################\n",
    "# 损失函数和评价指标\n",
    "##################\n",
    "ce_loss_fn = tf.keras.losses.categorical_crossentropy\n",
    "acc_fn = tf.keras.metrics.categorical_accuracy\n",
    "\n",
    "########\n",
    "# 优化器\n",
    "########\n",
    "optimizer = tf.optimizers.Adam(1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 527,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-14T12:31:43.060701Z",
     "start_time": "2020-04-14T12:31:43.054657Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[6316, 1418, 3744, ...,    0,    0,    0],\n",
       "       [5694, 3606, 3837, ...,    0,    0,    0],\n",
       "       [2869,  818, 3837, ..., 2356, 1698, 2900],\n",
       "       ...,\n",
       "       [4692, 5235, 3392, ...,    0,    0,    0],\n",
       "       [2543, 1363, 2604, ..., 1376, 1582,  401],\n",
       "       [1523,  289, 5141, ...,    0,    0,    0]])"
      ]
     },
     "execution_count": 527,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(20, 100)"
      ]
     },
     "execution_count": 527,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_batch = list(itertools.islice(train_dataset,batch_size))\n",
    "text_batch = np.array([i[0] for i in data_batch])\n",
    "label_batch = np.array([i[1] for i in data_batch])\n",
    "text_batch\n",
    "text_batch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 528,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-14T12:35:51.336790Z",
     "start_time": "2020-04-14T12:31:43.888209Z"
    },
    "code_folding": [],
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[e]: 0 [step]: 20 [acc]: 0.9326 [loss]:23.6377\n",
      "acc improved [from]:0.0000 [to]:0.9326.\n",
      "loss improved [from]:10000000000.0000 [to]:23.6377.\n",
      "[ckpt-path]: /home/zhoutong/notebook_collection/tmp/NLP_ner/lstmcrf/ckpt/ckpt_e0_trainLoss23.6377_trainAcc0.9326\n",
      "[e]: 0 [step]: 40 [acc]: 0.9235 [loss]:25.9836\n",
      "[e]: 0 [step]: 60 [acc]: 0.9280 [loss]:18.2528\n",
      "loss improved [from]:23.6377 [to]:18.2528.\n",
      "[ckpt-path]: /home/zhoutong/notebook_collection/tmp/NLP_ner/lstmcrf/ckpt/ckpt_e0_trainLoss18.2528_trainAcc0.9280\n",
      "[e]: 0 [step]: 80 [acc]: 0.8746 [loss]:38.0338\n",
      "[e]: 0 [step]: 100 [acc]: 0.9223 [loss]:21.5593\n",
      "[e]: 0 [step]: 120 [acc]: 0.9270 [loss]:26.2218\n",
      "[e]: 0 [step]: 140 [acc]: 0.8553 [loss]:26.2525\n",
      "[e]: 0 [step]: 160 [acc]: 0.7975 [loss]:46.1039\n",
      "[e]: 0 [step]: 180 [acc]: 0.9197 [loss]:22.3032\n",
      "[e]: 0 [step]: 200 [acc]: 0.9179 [loss]:29.2866\n",
      "[e]: 0 [step]: 220 [acc]: 0.9482 [loss]:15.3642\n",
      "acc improved [from]:0.9326 [to]:0.9482.\n",
      "loss improved [from]:18.2528 [to]:15.3642.\n",
      "[ckpt-path]: /home/zhoutong/notebook_collection/tmp/NLP_ner/lstmcrf/ckpt/ckpt_e0_trainLoss15.3642_trainAcc0.9482\n",
      "[e]: 0 [step]: 240 [acc]: 0.8842 [loss]:34.1563\n",
      "[e]: 0 [step]: 260 [acc]: 0.8630 [loss]:27.7638\n",
      "[e]: 0 [step]: 280 [acc]: 0.9253 [loss]:23.7875\n",
      "[e]: 0 [step]: 300 [acc]: 0.9048 [loss]:29.5885\n",
      "[e]: 0 [step]: 320 [acc]: 0.9295 [loss]:8.9807\n",
      "loss improved [from]:15.3642 [to]:8.9807.\n",
      "[ckpt-path]: /home/zhoutong/notebook_collection/tmp/NLP_ner/lstmcrf/ckpt/ckpt_e0_trainLoss8.9807_trainAcc0.9295\n",
      "[e]: 0 [step]: 340 [acc]: 0.8748 [loss]:25.6806\n",
      "[e]: 0 [step]: 360 [acc]: 0.8983 [loss]:36.6826\n",
      "[e]: 0 [step]: 380 [acc]: 0.8922 [loss]:13.3071\n",
      "[e]: 0 [step]: 400 [acc]: 0.9352 [loss]:19.0652\n",
      "[e]: 0 [step]: 420 [acc]: 0.8748 [loss]:23.0303\n",
      "[e]: 0 [step]: 440 [acc]: 0.8919 [loss]:28.4466\n",
      "[e]: 0 [step]: 460 [acc]: 0.9295 [loss]:17.2406\n",
      "[e]: 0 [step]: 480 [acc]: 0.9492 [loss]:9.5778\n",
      "acc improved [from]:0.9482 [to]:0.9492.\n",
      "[ckpt-path]: /home/zhoutong/notebook_collection/tmp/NLP_ner/lstmcrf/ckpt/ckpt_e0_trainLoss9.5778_trainAcc0.9492\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-528-ca4b52c891df>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0mlabels_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata_batch\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0mstep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseq_len_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_one_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m20\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0macc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_acc_one_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseq_len_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-528-ca4b52c891df>\u001b[0m in \u001b[0;36mtrain_one_step\u001b[0;34m(text_batch, labels_batch)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtrain_one_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGradientTape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtape\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m         \u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseq_len_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_likelihood\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mM\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels_batch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduce_mean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlog_likelihood\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mgradients\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mM\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainable_variables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf2/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    820\u001b[0m           with base_layer_utils.autocast_context_manager(\n\u001b[1;32m    821\u001b[0m               self._compute_dtype):\n\u001b[0;32m--> 822\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcast_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    823\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle_activity_regularization\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    824\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_mask_metadata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_masks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-380-8e9585ec6f12>\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, text, labels, training)\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m         \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbilstm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf2/lib/python3.6/site-packages/tensorflow_core/python/keras/layers/wrappers.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, initial_state, constants, **kwargs)\u001b[0m\n\u001b[1;32m    541\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    542\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0minitial_state\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mconstants\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 543\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBidirectional\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    544\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    545\u001b[0m     \u001b[0;31m# Applies the same workaround as in `RNN.__call__`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf2/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    820\u001b[0m           with base_layer_utils.autocast_context_manager(\n\u001b[1;32m    821\u001b[0m               self._compute_dtype):\n\u001b[0;32m--> 822\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcast_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    823\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle_activity_regularization\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    824\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_mask_metadata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_masks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf2/lib/python3.6/site-packages/tensorflow_core/python/keras/layers/wrappers.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, inputs, training, mask, initial_state, constants)\u001b[0m\n\u001b[1;32m    657\u001b[0m                              initial_state=forward_state, **kwargs)\n\u001b[1;32m    658\u001b[0m       y_rev = self.backward_layer(backward_inputs,\n\u001b[0;32m--> 659\u001b[0;31m                                   initial_state=backward_state, **kwargs)\n\u001b[0m\u001b[1;32m    660\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    661\u001b[0m       \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf2/lib/python3.6/site-packages/tensorflow_core/python/keras/layers/recurrent.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, initial_state, constants, **kwargs)\u001b[0m\n\u001b[1;32m    642\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    643\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0minitial_state\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mconstants\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 644\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mRNN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    645\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    646\u001b[0m     \u001b[0;31m# If any of `initial_state` or `constants` are specified and are Keras\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf2/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    820\u001b[0m           with base_layer_utils.autocast_context_manager(\n\u001b[1;32m    821\u001b[0m               self._compute_dtype):\n\u001b[0;32m--> 822\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcast_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    823\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle_activity_regularization\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    824\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_mask_metadata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_masks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf2/lib/python3.6/site-packages/tensorflow_core/python/keras/layers/recurrent_v2.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, inputs, mask, training, initial_state)\u001b[0m\n\u001b[1;32m   1145\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1146\u001b[0m           last_output, outputs, new_h, new_c, runtime = standard_lstm(\n\u001b[0;32m-> 1147\u001b[0;31m               **normal_lstm_kwargs)\n\u001b[0m\u001b[1;32m   1148\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1149\u001b[0m         (last_output, outputs, new_h, new_c,\n",
      "\u001b[0;32m~/anaconda3/envs/tf2/lib/python3.6/site-packages/tensorflow_core/python/keras/layers/recurrent_v2.py\u001b[0m in \u001b[0;36mstandard_lstm\u001b[0;34m(inputs, init_h, init_c, kernel, recurrent_kernel, bias, activation, recurrent_activation, mask, time_major, go_backwards, sequence_lengths)\u001b[0m\n\u001b[1;32m   1279\u001b[0m       \u001b[0mgo_backwards\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgo_backwards\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1280\u001b[0m       \u001b[0minput_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msequence_lengths\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1281\u001b[0;31m       if sequence_lengths is not None else timesteps)\n\u001b[0m\u001b[1;32m   1282\u001b[0m   return (last_output, outputs, new_states[0], new_states[1],\n\u001b[1;32m   1283\u001b[0m           _runtime(_RUNTIME_CPU))\n",
      "\u001b[0;32m~/anaconda3/envs/tf2/lib/python3.6/site-packages/tensorflow_core/python/keras/backend.py\u001b[0m in \u001b[0;36mrnn\u001b[0;34m(step_function, inputs, initial_states, go_backwards, mask, constants, unroll, input_length, time_major, zero_output_for_mask)\u001b[0m\n\u001b[1;32m   4172\u001b[0m           \u001b[0mbody\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_step\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4173\u001b[0m           \u001b[0mloop_vars\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_ta\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstates\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4174\u001b[0;31m           **while_loop_kwargs)\n\u001b[0m\u001b[1;32m   4175\u001b[0m       \u001b[0mnew_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfinal_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4176\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf2/lib/python3.6/site-packages/tensorflow_core/python/ops/control_flow_ops.py\u001b[0m in \u001b[0;36mwhile_loop\u001b[0;34m(cond, body, loop_vars, shape_invariants, parallel_iterations, back_prop, swap_memory, name, maximum_iterations, return_same_structure)\u001b[0m\n\u001b[1;32m   2712\u001b[0m                                               list(loop_vars))\n\u001b[1;32m   2713\u001b[0m       \u001b[0;32mwhile\u001b[0m \u001b[0mcond\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mloop_vars\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2714\u001b[0;31m         \u001b[0mloop_vars\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbody\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mloop_vars\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2715\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtry_to_pack\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloop_vars\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_basetuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2716\u001b[0m           \u001b[0mpacked\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf2/lib/python3.6/site-packages/tensorflow_core/python/keras/backend.py\u001b[0m in \u001b[0;36m_step\u001b[0;34m(time, output_ta_t, *states)\u001b[0m\n\u001b[1;32m   4157\u001b[0m         output, new_states = step_function(current_input,\n\u001b[1;32m   4158\u001b[0m                                            tuple(states) + tuple(constants))\n\u001b[0;32m-> 4159\u001b[0;31m         \u001b[0mflat_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4160\u001b[0m         \u001b[0mflat_new_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4161\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_state\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflat_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflat_new_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf2/lib/python3.6/site-packages/tensorflow_core/python/util/nest.py\u001b[0m in \u001b[0;36mflatten\u001b[0;34m(structure, expand_composites)\u001b[0m\n\u001b[1;32m    272\u001b[0m     \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mThe\u001b[0m \u001b[0mnest\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mcontains\u001b[0m \u001b[0ma\u001b[0m \u001b[0mdict\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mnon\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0msortable\u001b[0m \u001b[0mkeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    273\u001b[0m   \"\"\"\n\u001b[0;32m--> 274\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0m_pywrap_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFlatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstructure\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpand_composites\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    275\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    276\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def train_one_step(text_batch, labels_batch):\n",
    "    with tf.GradientTape() as tape:\n",
    "        logits, seq_len_batch, log_likelihood = M(text_batch, labels_batch,training=True)\n",
    "        loss = - tf.reduce_mean(log_likelihood)\n",
    "    gradients = tape.gradient(loss, M.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, M.trainable_variables))\n",
    "    return loss,logits, seq_len_batch\n",
    "\n",
    "def get_acc_one_step(logits, seq_len_batch, labels_batch):\n",
    "    paths = []\n",
    "    acc = 0\n",
    "    for logit, seq_len, labels in zip(logits, seq_len_batch, labels_batch):\n",
    "        viterbi_path, _ = tfa.text.viterbi_decode(logit[:seq_len], M.transition_params)\n",
    "        paths.append(viterbi_path)\n",
    "        correct_prediction = tf.equal(\n",
    "            tf.convert_to_tensor(tf.keras.preprocessing.sequence.pad_sequences([viterbi_path], padding='post'),\n",
    "                                 dtype=tf.int32),\n",
    "            tf.convert_to_tensor(tf.keras.preprocessing.sequence.pad_sequences([labels[:seq_len]], padding='post'),\n",
    "                                 dtype=tf.int32)\n",
    "        )\n",
    "        acc = acc + tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "    acc = acc / len(paths)\n",
    "    return acc\n",
    "\n",
    "\n",
    "def get_validation_acc():\n",
    "    while True:\n",
    "        data_batch = list(itertools.islice(test_dataset,batch_size))\n",
    "        if len(data_batch) < batch_size:\n",
    "            break\n",
    "        text_batch = np.array([i[0] for i in data_batch])\n",
    "        labels_batch = np.array([i[1] for i in data_batch])\n",
    "        step = step + 1\n",
    "        acc = get_acc_one_step(logits, seq_len_batch, labels_batch)\n",
    "        \n",
    "\n",
    "\n",
    "best_acc = 0\n",
    "step = 0\n",
    "epoch=10\n",
    "batch_size=20\n",
    "for epoch in range(epoch):\n",
    "    while True:\n",
    "        data_batch = list(itertools.islice(train_dataset,batch_size))\n",
    "        if len(data_batch) < batch_size:\n",
    "            break\n",
    "        text_batch = np.array([i[0] for i in data_batch])\n",
    "        labels_batch = np.array([i[1] for i in data_batch])\n",
    "        step = step + 1\n",
    "        loss, logits, seq_len_batch = train_one_step(text_batch, labels_batch)\n",
    "        if step % 20 == 0:\n",
    "            acc = get_acc_one_step(logits, seq_len_batch, labels_batch)\n",
    "            tqdm.write(f\"[e]: {epoch} [step]: {step} [acc]: {acc:.4f} [loss]:{loss:.4f}\")\n",
    "            ckpt_saver.save(val_loss=loss,val_acc=acc,model=M,fileName=f\"ckpt_e{epoch}_trainLoss{loss:.4f}_trainAcc{acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CRF 计算参数实验\n",
    "\n",
    "```\n",
    "loglikelihood = sequence_score - log_norm\n",
    "= (unary_score + binary_score) - logsumexp(alphas)\n",
    "= (unary_score + binary_score) - logsumexp(crf_forward)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## loglikelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 466,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-14T12:02:42.473921Z",
     "start_time": "2020-04-14T12:02:42.384654Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'inp'"
      ]
     },
     "execution_count": 466,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "array([[500, 501, 502, 503]])"
      ]
     },
     "execution_count": 466,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "('logits', (1, 4, 10))"
      ]
     },
     "execution_count": 466,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "array([[[ 0.02905709, -0.0079308 ,  0.00130202, -0.01221373,\n",
       "          0.00268621,  0.01556253, -0.00820034,  0.01168286,\n",
       "         -0.01324397, -0.02614049],\n",
       "        [ 0.01999625,  0.00837216,  0.02181312, -0.02321985,\n",
       "          0.00284035,  0.02077706,  0.00205434,  0.01210876,\n",
       "          0.00665217, -0.01442756],\n",
       "        [ 0.01611964,  0.008289  ,  0.02945245, -0.01523286,\n",
       "          0.00243924,  0.01745703, -0.0071438 ,  0.00392865,\n",
       "          0.00447142,  0.00135649],\n",
       "        [ 0.00934999, -0.0086754 , -0.01002031, -0.0018477 ,\n",
       "          0.01812736,  0.01125069, -0.00853377,  0.00692784,\n",
       "         -0.01058302, -0.00975374]]], dtype=float32)"
      ]
     },
     "execution_count": 466,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "('label_sequences', (1, 4))"
      ]
     },
     "execution_count": 466,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "array([[1, 2, 3, 9]])"
      ]
     },
     "execution_count": 466,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "'loglikelihood'"
      ]
     },
     "execution_count": 466,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "array([-8.901098], dtype=float32)"
      ]
     },
     "execution_count": 466,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "'seq_score'"
      ]
     },
     "execution_count": 466,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "array([1.7472095], dtype=float32)"
      ]
     },
     "execution_count": 466,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "'log_norm'"
      ]
     },
     "execution_count": 466,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "array([10.648308], dtype=float32)"
      ]
     },
     "execution_count": 466,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sen_idx=[500,501,502,503]\n",
    "# tag_idx=[1,2,3,10]\n",
    "tag_idx=[1,2,3,9]\n",
    "inp = np.array([sen_idx])\n",
    "\"inp\"\n",
    "inp\n",
    "# logits=M.dense(M.dropout(M.bilstm(M.embedding(inp))))\n",
    "logits,seq_lens=M(inp)\n",
    "\"logits\",logits.numpy().shape\n",
    "logits.numpy()\n",
    "label_sequences = np.array([tag_idx])\n",
    "\"label_sequences\",label_sequences.shape\n",
    "label_sequences\n",
    "text_lens=np.array([len(sen_idx)])\n",
    "loglikelihood,trans_params = tfa.text.crf_log_likelihood(logits, label_sequences, text_lens,M.transition_params)\n",
    "\"loglikelihood\"\n",
    "loglikelihood.numpy()\n",
    "\"seq_score\"\n",
    "tfa.text.crf_sequence_score(logits,label_sequences,text_lens,M.transition_params).numpy()\n",
    "\"log_norm\"\n",
    "tfa.text.crf_log_norm(logits, text_lens, M.transition_params).numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## sequence_socre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-08T06:18:49.950137Z",
     "start_time": "2020-04-08T06:18:49.920963Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'crf_sequence_score: [1.0131763]'"
      ]
     },
     "execution_count": 252,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "'== unary_score+bianry_score'"
      ]
     },
     "execution_count": 252,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "'unary_score: [0.3015548]'"
      ]
     },
     "execution_count": 252,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "'== sum(emission of labelSeq): 0.301554799079895'"
      ]
     },
     "execution_count": 252,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "'bianry_score: [0.7116215]'"
      ]
     },
     "execution_count": 252,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "'== sum(transition of labelSeq): 0.7116215229034424'"
      ]
     },
     "execution_count": 252,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f\"crf_sequence_score: {tfa.text.crf_sequence_score(logits,label_sequences,text_lens,M.transition_params)}\"\n",
    "\"== unary_score+bianry_score\"\n",
    "f\"unary_score: {tfa.text.crf_unary_score(label_sequences, text_lens, logits)}\"\n",
    "f\"== sum(emission of labelSeq): {sum([logits.numpy()[0,idx,i] for idx,i in enumerate(label_sequences[0])])}\"\n",
    "f\"bianry_score: {tfa.text.crf_binary_score(label_sequences, text_lens, M.transition_params)}\"\n",
    "f\"== sum(transition of labelSeq): {sum([M.transition_params.numpy()[a,b] for a,b in list(zip(label_sequences[0][:-1],label_sequences[0][1:]))])}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## log_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-08T06:25:56.694831Z",
     "start_time": "2020-04-08T06:25:56.681678Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'log_norm: [8.306995]'"
      ]
     },
     "execution_count": 271,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f\"log_norm: {tfa.text.crf_log_norm(logits, text_lens, M.transition_params).numpy()}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### first_input&rest_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-08T07:16:51.764489Z",
     "start_time": "2020-04-08T07:16:51.736904Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[logits]: (1, 3, 10)'"
      ]
     },
     "execution_count": 297,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "array([[[ 0.00217672,  0.0098424 ,  0.00291217, -0.00717967,\n",
       "          0.01139859, -0.01732886,  0.00449238, -0.0078244 ,\n",
       "         -0.01410343,  0.00459101],\n",
       "        [ 0.01717858,  0.00824076,  0.0123352 ,  0.02027788,\n",
       "          0.01698909, -0.03285388, -0.02621158, -0.01658068,\n",
       "         -0.01112588,  0.01208735],\n",
       "        [-0.0097875 , -0.00283362,  0.01374137,  0.01923978,\n",
       "          0.01765317, -0.01433519, -0.02876953, -0.03505114,\n",
       "         -0.0089741 , -0.01326887]]], dtype=float32)"
      ]
     },
     "execution_count": 297,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "'[first_input]: (1, 10)'"
      ]
     },
     "execution_count": 297,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "array([[ 0.00217672,  0.0098424 ,  0.00291217, -0.00717967,  0.01139859,\n",
       "        -0.01732886,  0.00449238, -0.0078244 , -0.01410343,  0.00459101]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 297,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "'[rest_of_input]: (1, 2, 10)'"
      ]
     },
     "execution_count": 297,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "array([[[ 0.01717858,  0.00824076,  0.0123352 ,  0.02027788,\n",
       "          0.01698909, -0.03285388, -0.02621158, -0.01658068,\n",
       "         -0.01112588,  0.01208735],\n",
       "        [-0.0097875 , -0.00283362,  0.01374137,  0.01923978,\n",
       "          0.01765317, -0.01433519, -0.02876953, -0.03505114,\n",
       "         -0.0089741 , -0.01326887]]], dtype=float32)"
      ]
     },
     "execution_count": 297,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs=logits\n",
    "sequence_lengths=text_lens\n",
    "first_input = tf.slice(inputs, [0, 0, 0], [-1, 1, -1])\n",
    "first_input = tf.squeeze(first_input, [1])\n",
    "rest_of_input = tf.slice(inputs, [0, 1, 0], [-1, -1, -1])\n",
    "f\"[logits]: {logits.numpy().shape}\"\n",
    "logits.numpy()\n",
    "f\"[first_input]: {first_input.numpy().shape}\"\n",
    "first_input.numpy()\n",
    "f\"[rest_of_input]: {rest_of_input.numpy().shape}\"\n",
    "rest_of_input.numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### crf_forward\n",
    "前向计算和viterbi类似，只不过是把求最大改成了求和"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-08T07:37:31.869699Z",
     "start_time": "2020-04-08T07:37:31.852699Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n",
       "array([[5.750242 , 5.76396  , 5.7077093, 5.6722016, 5.826963 , 5.8499684,\n",
       "        5.7189903, 5.7958345, 5.6517015, 5.6982207]], dtype=float32)>"
      ]
     },
     "execution_count": 306,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1,), dtype=float32, numpy=array([8.048121], dtype=float32)>"
      ]
     },
     "execution_count": 306,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alphas = tfa.text.crf_forward(rest_of_input, first_input, M.transition_params, sequence_lengths)\n",
    "alphas\n",
    "tf.reduce_logsumexp(alphas,[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-08T07:34:16.071153Z",
     "start_time": "2020-04-08T07:34:16.021593Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0, 0, 0],\n",
       " [0, 0, 1],\n",
       " [0, 0, 2],\n",
       " [0, 1, 0],\n",
       " [0, 1, 1],\n",
       " [0, 1, 2],\n",
       " [0, 2, 0],\n",
       " [0, 2, 1],\n",
       " [0, 2, 2],\n",
       " [1, 0, 0],\n",
       " [1, 0, 1],\n",
       " [1, 0, 2],\n",
       " [1, 1, 0],\n",
       " [1, 1, 1],\n",
       " [1, 1, 2],\n",
       " [1, 2, 0],\n",
       " [1, 2, 1],\n",
       " [1, 2, 2],\n",
       " [2, 0, 0],\n",
       " [2, 0, 1],\n",
       " [2, 0, 2],\n",
       " [2, 1, 0],\n",
       " [2, 1, 1],\n",
       " [2, 1, 2],\n",
       " [2, 2, 0],\n",
       " [2, 2, 1],\n",
       " [2, 2, 2]]"
      ]
     },
     "execution_count": 305,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "27"
      ]
     },
     "execution_count": 305,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "[(0.009567800909280777, 1.563323736190796),\n",
       " (0.016521684359759092, 1.2504353523254395),\n",
       " (0.03309667017310858, 0.9342750310897827),\n",
       " (0.0006299763917922974, 1.1747034788131714),\n",
       " (0.007583859842270613, 0.6671038866043091),\n",
       " (0.024158845655620098, 1.3209037780761719),\n",
       " (0.004724414087831974, 0.8684554100036621),\n",
       " (0.01167829753831029, 1.0337573289871216),\n",
       " (0.028253283351659775, 0.5662949085235596),\n",
       " (0.017233476042747498, 1.4875918626785278),\n",
       " (0.024187359493225813, 1.1747034788131714),\n",
       " (0.0407623453065753, 0.8585431575775146),\n",
       " (0.008295651525259018, 0.9042603969573975),\n",
       " (0.015249534975737333, 0.39666080474853516),\n",
       " (0.03182452078908682, 1.050460696220398),\n",
       " (0.012390089221298695, 1.5679725408554077),\n",
       " (0.01934397267177701, 1.7332744598388672),\n",
       " (0.035918958485126495, 1.2658120393753052),\n",
       " (0.010303251445293427, 1.4975041151046753),\n",
       " (0.017257134895771742, 1.1846157312393188),\n",
       " (0.03383212070912123, 0.8684554100036621),\n",
       " (0.001365426927804947, 1.5870741605758667),\n",
       " (0.008319310378283262, 1.0794745683670044),\n",
       " (0.024894296191632748, 1.7332744598388672),\n",
       " (0.0054598646238446236, 1.1295239925384521),\n",
       " (0.012413748074322939, 1.2948259115219116),\n",
       " (0.028988733887672424, 0.8273634910583496)]"
      ]
     },
     "execution_count": 305,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 模拟计算所有序列情况各自的sequence_score，并求和\n",
    "# 从结果来看不是这样计算的\n",
    "length=text_lens[0]\n",
    "ps=[]\n",
    "for i in range(length):\n",
    "    for j in range(length): \n",
    "        for m in range(length):\n",
    "                ps.append([i,j,m])\n",
    "ps\n",
    "res=[]\n",
    "for seq in ps:\n",
    "    unary_score=sum([logits.numpy()[0][idx,i] for idx,i in enumerate(seq)])\n",
    "    binary_score = sum([M.transition_params[a][b].numpy() for a,b in list(zip(seq[:-1],seq[1:]))])\n",
    "    res.append((unary_score, binary_score))\n",
    "\n",
    "len(res)\n",
    "res\n",
    "# tf.reduce_logsumexp(res).numpy()\n",
    "# for word_idx,emit_list in enumerate(logits.numpy()[0]):\n",
    "#     for emit_idx,emit in enumerate(emit_list):\n",
    "#         emit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-08T07:16:37.359676Z",
     "start_time": "2020-04-08T07:16:37.351617Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 3, 10), dtype=float32, numpy=\n",
       "array([[[0.10040308, 0.10123939, 0.09852703, 0.09806626, 0.10067997,\n",
       "         0.09858277, 0.10091217, 0.09968802, 0.10062727, 0.10127402],\n",
       "        [0.09987237, 0.10105918, 0.09948634, 0.09686752, 0.09984942,\n",
       "         0.09828502, 0.10078489, 0.10046219, 0.10212169, 0.10121138],\n",
       "        [0.10036152, 0.0987159 , 0.09999987, 0.09854174, 0.09934476,\n",
       "         0.09669184, 0.10196146, 0.10054415, 0.1020008 , 0.10183792]]],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 296,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rest_of_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-08T02:59:07.089800Z",
     "start_time": "2020-04-08T02:59:07.044336Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'shape: rest_of_input,first_input,transition_params'"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "[(1, 3, 10), (1, 10), (10, 10)]"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "'shape(after): rest_of_input,first_input,transition_params'"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "[(3, 1, 10), (1, 10), (1, 10, 10)]"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n",
       "array([[0.1006834 , 0.10105516, 0.09909209, 0.09861839, 0.10069787,\n",
       "        0.09951755, 0.0996674 , 0.10011183, 0.100015  , 0.10054132]],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 10, 1), dtype=float32, numpy=\n",
       "array([[[0.1006834 ],\n",
       "        [0.10105516],\n",
       "        [0.09909209],\n",
       "        [0.09861839],\n",
       "        [0.10069787],\n",
       "        [0.09951755],\n",
       "        [0.0996674 ],\n",
       "        [0.10011183],\n",
       "        [0.100015  ],\n",
       "        [0.10054132]]], dtype=float32)>"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 10, 10), dtype=float32, numpy=\n",
       "array([[[0.44697106, 0.2418617 , 0.8227587 , 0.4104979 , 0.782711  ,\n",
       "         0.01426017, 0.40244222, 0.11317647, 0.1748898 , 0.6239077 ],\n",
       "        [0.20628917, 0.99719846, 0.12566197, 0.8589554 , 0.01937366,\n",
       "         0.8665881 , 0.59559464, 0.00556386, 0.9480275 , 0.35019696],\n",
       "        [0.9995408 , 0.9722867 , 0.5364071 , 0.971385  , 0.3729949 ,\n",
       "         0.6110319 , 0.9596269 , 0.5348549 , 0.06864536, 0.6061748 ],\n",
       "        [0.19580531, 0.8521589 , 0.43386817, 0.62382567, 0.6366831 ,\n",
       "         0.86621857, 0.06924629, 0.8040581 , 0.6113379 , 0.9791328 ],\n",
       "        [0.58327127, 0.62254274, 0.4878497 , 0.5879643 , 0.3653742 ,\n",
       "         0.0238682 , 0.3072735 , 0.34319937, 0.31553054, 0.5738963 ],\n",
       "        [0.342664  , 0.01548791, 0.08265579, 0.3064649 , 0.49686992,\n",
       "         0.83524024, 0.50523055, 0.95652676, 0.46592188, 0.51819336],\n",
       "        [0.63470256, 0.2315464 , 0.24391973, 0.30125952, 0.42490733,\n",
       "         0.63373923, 0.6383817 , 0.35166633, 0.76938987, 0.5649425 ],\n",
       "        [0.5689857 , 0.03791308, 0.46573925, 0.19652998, 0.28090703,\n",
       "         0.72030675, 0.03897691, 0.10826731, 0.26779532, 0.2344141 ],\n",
       "        [0.06005204, 0.8423836 , 0.33140337, 0.07298076, 0.21876395,\n",
       "         0.34242964, 0.7043555 , 0.9701718 , 0.95859563, 0.39573073],\n",
       "        [0.77014065, 0.3962493 , 0.31336284, 0.6322361 , 0.66768086,\n",
       "         0.00951624, 0.30341613, 0.40366566, 0.34672344, 0.00739753]]],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 10, 10), dtype=float32, numpy=\n",
       "array([[[0.54765445, 0.3425451 , 0.92344207, 0.5111813 , 0.8833944 ,\n",
       "         0.11494357, 0.5031256 , 0.21385986, 0.2755732 , 0.7245911 ],\n",
       "        [0.30734432, 1.0982536 , 0.22671713, 0.9600105 , 0.12042882,\n",
       "         0.96764326, 0.6966498 , 0.10661902, 1.0490826 , 0.4512521 ],\n",
       "        [1.0986329 , 1.0713788 , 0.6354992 , 1.0704771 , 0.472087  ,\n",
       "         0.71012396, 1.058719  , 0.63394696, 0.16773745, 0.7052669 ],\n",
       "        [0.2944237 , 0.9507773 , 0.53248656, 0.72244406, 0.7353015 ,\n",
       "         0.96483696, 0.16786468, 0.90267646, 0.7099563 , 1.0777512 ],\n",
       "        [0.68396914, 0.7232406 , 0.5885476 , 0.6886622 , 0.46607208,\n",
       "         0.12456607, 0.40797138, 0.44389725, 0.4162284 , 0.67459416],\n",
       "        [0.44218156, 0.11500546, 0.18217334, 0.40598246, 0.5963875 ,\n",
       "         0.9347578 , 0.60474813, 1.0560443 , 0.56543946, 0.61771095],\n",
       "        [0.73437   , 0.3312138 , 0.34358713, 0.40092692, 0.52457476,\n",
       "         0.73340666, 0.73804915, 0.45133373, 0.8690573 , 0.6646099 ],\n",
       "        [0.66909754, 0.13802493, 0.5658511 , 0.29664183, 0.38101888,\n",
       "         0.8204186 , 0.13908875, 0.20837915, 0.36790717, 0.33452594],\n",
       "        [0.16006704, 0.9423986 , 0.43141836, 0.17299576, 0.31877893,\n",
       "         0.44244462, 0.80437046, 1.0701869 , 1.0586107 , 0.49574572],\n",
       "        [0.87068194, 0.49679062, 0.41390416, 0.7327774 , 0.76822215,\n",
       "         0.11005756, 0.40395746, 0.50420696, 0.44726476, 0.10793885]]],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n",
       "array([[2.9214406, 2.9880924, 2.808669 , 2.9367938, 2.8524787, 2.950623 ,\n",
       "        2.892315 , 2.9182906, 2.941332 , 2.918378 ]], dtype=float32)>"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# crf_forward\n",
    "inputs = rest_of_input\n",
    "stat = first_input\n",
    "transition_params = M.transition_params\n",
    "\"shape: rest_of_input,first_input,transition_params\"\n",
    "[i.numpy().shape for i in [inputs,stat,transition_params]]\n",
    "\n",
    "inputs = tf.transpose(inputs, [1, 0, 2])\n",
    "transition_params = tf.expand_dims(transition_params, 0)\n",
    "\"shape(after): rest_of_input,first_input,transition_params\"\n",
    "[i.numpy().shape for i in [inputs,stat,transition_params]]\n",
    "\n",
    "\n",
    "last_index = tf.maximum(tf.constant(0, dtype=sequence_lengths.dtype), sequence_lengths - 1)\n",
    "\n",
    "\n",
    "stat\n",
    "tf.expand_dims(stat, 2)\n",
    "transition_params\n",
    "tf.expand_dims(stat, 2) + transition_params\n",
    "tf.reduce_logsumexp(transition_scores, [1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3.6(tf2)",
   "language": "python",
   "name": "tf2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "252.543px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
