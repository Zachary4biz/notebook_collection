#!/usr/bin/env python
# coding: utf-8

# In[1]:


from IPython.core.interactiveshell import InteractiveShell
InteractiveShell.ast_node_interactivity = "all"
get_ipython().run_line_magic('matplotlib', 'inline')
from tqdm.auto import tqdm
import concurrent.futures
from multiprocessing import Pool
import copy,os,sys,psutil
from collections import Counter,deque
import numpy as np
from zac_pyutils.ExqUtils import zprint


# In[2]:


import tensorflow as tf
import numpy as np
import json
import re
import itertools
import pickle
import time


# In[3]:


# å…è®¸GPUæ¸è¿›å ç”¨
sess_conf = tf.ConfigProto()
sess_conf.gpu_options.allow_growth = True  # å…è®¸GPUæ¸è¿›å ç”¨
sess_conf.allow_soft_placement = True  # æŠŠä¸é€‚åˆGPUçš„æ”¾åˆ°CPUä¸Šè·‘
with tf.Session(config=sess_conf) as sess:
    print(sess)


# In[4]:


# sess.run(.. options=run_opt)å¯ä»¥åœ¨OOMçš„æ—¶å€™æä¾›å½“å‰å·²ç»å£°æ˜äº†çš„å˜é‡
run_opt = tf.RunOptions()
run_opt.report_tensor_allocations_upon_oom = True


# # è¯•è¯•çœ‹èƒ½ä¸èƒ½yieldæ–¹å¼æ„é€ å‡ºå•è¯ç´¢å¼•
# - è¦è·Ÿåé¢å»æ­£æ–‡ä½¿ç”¨ç›¸åŒçš„ `load_f` åŠ è½½æ–¹å¼ï¼ˆç›¸åŒçš„é¢„å¤„ç†ï¼‰
# ğŸ‘Œå·²å®Œæˆ

# In[5]:


# data_name = "labeled_timeliness_region_taste_emotion_sample.json.bak.head1k"
data_name = "labeled_timeliness_region_taste_emotion_sample.json.bak"
fp = "/home/zhoutong/NLP/data/{}".format(data_name)
result_set_fp = "/home/zhoutong/NLP/data/{}_char2idx".format(data_name)
coded_article_fp = "/home/zhoutong/NLP/data/{}_encoded_article.pkl".format(data_name)

"fp: ",fp
"result_set_fp: ", result_set_fp
"coded_article_fp: ", coded_article_fp


# In[6]:


def load_f(fp_inp):
    with open(fp_inp,"r") as f:
        for line in f:
            title = json.loads(line)['title']
            text = json.loads(line)['text']
            text = re.sub("[\\n]+", "\\n",text)
            yield text


# In[ ]:



def transform(text_inp):
 """
 è¿™é‡Œæ˜¯æŠŠå„ä¸ªæ ‡ç‚¹ç¬¦å·éƒ½å‰ååŠ ä¸Šç©ºæ ¼åˆ†å¼€ï¼Œä¸ç¡®å®šè¿™æ ·æ˜¯å¦å¯ä»¥å¢åŠ æ–‡æœ¬ç”Ÿæˆæ—¶å¯¹æ ‡ç‚¹çš„å‡†ç¡®è¡¨ç¤º
 ç†è®ºä¸Šåœ¨å»ºç«‹ç´¢å¼•çš„æ—¶å€™è¡¨å¾è¿‡çš„å…ƒç´ ï¼ˆä¾‹å¦‚"\n"ç´¢å¼•ä¸º0ï¼‰å°±æœ‰å¯èƒ½æ€§
 ä½†æ˜¯ä¸åˆ†å¼€ï¼Œç›´æ¥æŠŠ "you!"(idx=11) å½“ä½œä¸€ä¸ªæ–°çš„æ•´ä½“è€Œä¸æ˜¯ "you"(idx=9) å’Œ "!"(idx=10) å¯èƒ½ä¹Ÿè¡Œ
 """
 for t in ["\\n",", "]:
     text_inp = re.sub(t, " "+t+" ",text_inp)
 text_inp = re.sub("\. "," . ",text_inp) # "." ä¸å¥½ç›´æ¥æ”¾åœ¨å¾ªç¯ä¸­ä¸€èµ·åšï¼Œè§„çŸ©ä¸å¤ªä¸€æ ·å•ç‹¬åšäº† 
 return text_inp


text_g = load_f(fp)
result_set = set()
while True:
 chunk = list(itertools.islice(text_g,10000))
 if len(chunk) > 0:
     for text in chunk:
         # ä¸ä½¿ç”¨transform
         # text = transform(text)
         result_set.update(text.replace("\n"," \n ").strip().split(" "))
 else:
     result_set = [i for i in result_set if i != ""]
     break


import pickle
result_set_d = dict([(word,idx) for idx,word in enumerate(result_set)])
with open(result_set_fp+".pickle","wb+") as f:
 pickle.dump(result_set_d,f)


# In[ ]:


with open(result_set_fp+".pickle","rb+") as f:
    word2idx_dict = pickle.load(f)


# In[ ]:


list(itertools.islice(word2idx_dict.items(),10))


# # å®éªŒæ€§è´¨ | çœ‹çœ‹å‡ºæ¥çš„ç»“æœå¯¹ä¸å¯¹

# In[ ]:


from collections import deque

text_g = load_f(fp)
wordsIdx = deque()
stopCnt = 0
while True:
    chunk = list(itertools.islice(text_g,10000))
    if len(chunk) > 0:
        for text in chunk:
            words = text.replace("\n"," \n ").strip().split(" ")
            words = [i for i in words if i != ""]
            wordsIdx.append([word2idx_dict[w] for w in words])
            print(">>>", words[:10])
            for i in list(itertools.islice(wordsIdx,10)):
                print(i[:10])  # æ¯æ¬¡éƒ½æ‰“å°wordsIdxçš„top10æ®µè½çš„top10ä¸ªè¯
            stopCnt += 1
            assert stopCnt<=5
    else:
        result_set = [i for i in result_set if i != ""]
        break


# # æ–‡ç« æ›¿æ¢æˆwordç´¢å¼•
# - è¿™é‡Œæ¯ç¯‡æ–‡ç« éƒ½æ˜¯ä¸€ä¸ªå•ç‹¬çš„æ•°ç»„`append`åˆ°`wordsIdx`é‡Œ
# - è¿™ä¸ªäºŒç»´æ•°ç»„å­˜npyæ–‡ä»¶å¤ªå¤§äº†ï¼Œè½¬æˆäºŒç»´listå­˜
#     - npy: 5.1G | deque_pkl: 3.2G | list_pkl: 3.2G
#     - ç›´æ¥ä»¥dequeå­˜å’Œè½¬æˆlistå­˜å ç”¨ç©ºé—´ç›¸åŒ
# 

# In[ ]:


from collections import deque

text_g = load_f(fp)
wordsIdx = deque()
with tqdm() as pbar:
    while True:
        chunk = list(itertools.islice(text_g,10000))
        if len(chunk) > 0:
            for text in chunk:
                words = text.replace("\n"," \n ").strip().split(" ")
                words = [i for i in words if i != ""]
                words_idx = ([word2idx_dict[w] for w in words]+[-1]*1024)[:300]  # æ¯ç¯‡æ–‡ç« æœ€å¤šå–1024ä¸ªè¯
                wordsIdx.append(words_idx)
                pbar.update(1)
        else:
            break
with open(coded_article_fp,"wb+") as fwb:
    pickle.dump(list(wordsIdx),fwb)


# In[ ]:


with open(coded_article_fp,"rb+") as frb:
    coded_article = pickle.load(frb)


# # CharRNN åŸºäºå­—ç¬¦

# ## æ­å»ºæ¨¡å‹

# ### è¾“å…¥å±‚

# In[5]:


def build_inputs(batch_size, num_steps):
    '''
    æ„å»ºè¾“å…¥å±‚
    
    batch_size: æ¯ä¸ªbatchä¸­çš„åºåˆ—ä¸ªæ•°
    num_steps: æ¯ä¸ªåºåˆ—åŒ…å«çš„å­—ç¬¦æ•°
    '''
    inputs = tf.placeholder(tf.int32, shape=(batch_size, num_steps), name='inputs')
    targets = tf.placeholder(tf.int32, shape=(batch_size, num_steps), name='targets')
    
    # åŠ å…¥keep_prob
    keep_prob = tf.placeholder(tf.float32, name='keep_prob')
    
    return inputs, targets, keep_prob


# ### LSTM
# - `BasicLSTMCell` æ›¿æ¢ä¸º `LSTMCell` 
# - LSTMéœ€è¦çŸ¥é“ `batch_size` åªæ˜¯ç”¨æ¥åšå…¨é›¶åˆå§‹åŒ–æ—¶éœ€è¦çŸ¥é“ç»´åº¦

# In[6]:


def build_lstm(lstm_size, num_layers, batch_size, keep_prob):
    ''' 
    æ„å»ºlstmå±‚
        
    keep_prob
    lstm_size: lstméšå±‚ä¸­ç»“ç‚¹æ•°ç›®
    num_layers: lstmçš„éšå±‚æ•°ç›®
    batch_size: batch_size

    '''
    def construct_cell(node_size):
        # æ„å»ºä¸€ä¸ªåŸºæœ¬lstmå•å…ƒ
        lstm = tf.nn.rnn_cell.LSTMCell(node_size)
        # æ·»åŠ dropout
        drop = tf.nn.rnn_cell.DropoutWrapper(lstm, output_keep_prob=keep_prob)
        return drop
    
    # å †å 
    cell = tf.nn.rnn_cell.MultiRNNCell([construct_cell(lstm_size) for _ in range(num_layers)])
    initial_state = cell.zero_state(batch_size, tf.float32)
    
    return cell, initial_state


# ### è¾“å‡ºå±‚
# - `tf.concat(1,lstm_output)` æ›¿æ¢ä¸º `tf.concat(lstm_output,1)`

# In[7]:


def build_output(lstm_output, in_size, out_size):
    ''' 
    æ„é€ è¾“å‡ºå±‚
        
    lstm_output: lstmå±‚çš„è¾“å‡ºç»“æœ
    in_size: lstmè¾“å‡ºå±‚é‡å¡‘åçš„size
    out_size: softmaxå±‚çš„size
    
    '''

    # å°†lstmçš„è¾“å‡ºæŒ‰ç…§åˆ—concateï¼Œä¾‹å¦‚[[1,2,3],[7,8,9]],
    # tf.concatçš„ç»“æœæ˜¯[1,2,3,7,8,9]
    seq_output = tf.concat(lstm_output, 1) # tf.concat(concat_dim, values)
    # reshape
    x = tf.reshape(seq_output, [-1, in_size])
    tf.summary.histogram('seq_output_reshape',x)
    
    # å°†lstmå±‚ä¸softmaxå±‚å…¨è¿æ¥
    with tf.variable_scope('softmax'):
        softmax_w = tf.Variable(tf.truncated_normal([in_size, out_size], stddev=0.1))
        softmax_b = tf.Variable(tf.zeros(out_size))
    tf.summary.histogram("softmax_w",softmax_w)
    tf.summary.histogram("softmax_b",softmax_b)
    
    # è®¡ç®—logits
    logits = tf.matmul(x, softmax_w) + softmax_b
    
    # softmaxå±‚è¿”å›æ¦‚ç‡åˆ†å¸ƒ
    out = tf.nn.softmax(logits, name='predictions')
    tf.summary.histogram('pred',out)
    
    return out, logits


# ### è¯¯å·®
# - `softmax_cross_entropy_with_logits` æ›¿æ¢ä¸º `softmax_cross_entropy_with_logits_v2`

# In[8]:


def build_loss(logits, targets, lstm_size, num_classes):
    '''
    æ ¹æ®logitså’Œtargetsè®¡ç®—æŸå¤±
    
    logits: å…¨è¿æ¥å±‚çš„è¾“å‡ºç»“æœï¼ˆä¸ç»è¿‡softmaxï¼‰
    targets: targets
    lstm_size
    num_classes: vocab_size
        
    '''
    
    # One-hotç¼–ç 
    y_one_hot = tf.one_hot(targets, num_classes)
    y_reshaped = tf.reshape(y_one_hot, logits.get_shape())
    
    # Softmax cross entropy loss
    loss = tf.nn.softmax_cross_entropy_with_logits_v2(logits=logits, labels=y_reshaped)
    loss = tf.reduce_mean(loss)
    
    return loss


# ### ä¼˜åŒ–å™¨

# In[9]:


def build_optimizer(loss, learning_rate, grad_clip):
    ''' 
    æ„é€ Optimizer
   
    loss: æŸå¤±
    learning_rate: å­¦ä¹ ç‡
    
    '''
    
    # ä½¿ç”¨clipping gradients
    tvars = tf.trainable_variables()
    grads, _ = tf.clip_by_global_norm(tf.gradients(loss, tvars), grad_clip)
    for g in grads:
        tf.summary.histogram(g.name, g)
    train_op = tf.train.AdamOptimizer(learning_rate)
    optimizer = train_op.apply_gradients(zip(grads, tvars))
    
    return optimizer


# ### æ¨¡å‹
# - ä½¿ç”¨ `placeholder` æ›¿ä»£å›ºå®šçš„sizeå’Œsteps
# - å†…éƒ¨æ–°å»ºä¸€å¼ è®¡ç®—å›¾è€Œä¸æ˜¯ä½¿ç”¨resetåçš„é»˜è®¤è®¡ç®—å›¾
# - å¢åŠ summary

# In[45]:


class CharRNN:
    def __init__(self, num_classes, batch_size=64, num_steps=50, 
                       lstm_size=128, num_layers=2, learning_rate=0.001, 
                       grad_clip=5, summary_path=None, sampling=False):
    
        batch_size, num_steps = batch_size, num_steps
        
        # æ–°å»ºä¸€å¼ å›¾
        self.graph = tf.Graph()
        
        with self.graph.as_default():
            # è¾“å…¥å±‚
            self.inputs, self.targets, self.keep_prob = build_inputs(batch_size, num_steps)

            # LSTMå±‚
            cell, self.initial_state = build_lstm(lstm_size, num_layers, batch_size, self.keep_prob)

            # å¯¹è¾“å…¥è¿›è¡Œone-hotç¼–ç 
            x_one_hot = tf.one_hot(self.inputs, num_classes)

            # è¿è¡ŒRNN
            outputs, state = tf.nn.dynamic_rnn(cell, x_one_hot, initial_state=self.initial_state)
            self.final_state = state

            # é¢„æµ‹ç»“æœ
            self.prediction, self.logits = build_output(outputs, lstm_size, num_classes)

            # Loss å’Œ optimizer (with gradient clipping)
            self.loss = build_loss(self.logits, self.targets, lstm_size, num_classes)
            self.optimizer = build_optimizer(self.loss, learning_rate, grad_clip)

            # summary
            tf.summary.scalar("loss", self.loss)
            #    lstmçš„variablesåœ¨dynamic_runä¹‹åæ‰ä¼šæœ‰å€¼ä¸ç„¶æ˜¯ç©ºçš„list
            for idx,tensor in enumerate(cell.variables):
                if idx % 2 == 0:
                    _ = tf.summary.histogram(f"lstm_kernel_{idx}",tensor)
                else:
                    _ = tf.summary.histogram(f"lstm_bias_{idx}",tensor)
            self.merge_summary = tf.summary.merge_all()
            self.writer = tf.summary.FileWriter(summary_path, self.graph) if summary_path is not None else None
        


# ## æ–‡æœ¬ç¼–ç 

# ### æ•°æ®æŒ‡å®š

# In[11]:


def load_f(fp_inp):
    with open(fp_inp,"r") as f:
        for line in f:
            yield line

data_name = "anna.txt"
fp = "/home/zhoutong/NLP/data/{}".format(data_name)
result_set_fp = "/home/zhoutong/NLP/data/{}_char2idx".format(data_name)
coded_article_fp = "/home/zhoutong/NLP/data/{}_encoded_article.npy".format(data_name)

"fp: ",fp
"result_set_fp: ", result_set_fp
"coded_article_fp: ", coded_article_fp


# ### char2idx

# In[ ]:


text_g = load_f(fp)
result_set = set()
while True:
    chunk = list(itertools.islice(text_g,10000))
    if len(chunk) > 0:
        for text in chunk:
            # ä¸ä½¿ç”¨transform
            # text = transform(text)
            result_set.update(list(text))
    else:
        result_set = [i for i in result_set if i != ""]
        break


import pickle
result_set_d = dict([(word,idx) for idx,word in enumerate(result_set)])
with open(result_set_fp+".pickle","wb+") as f:
    pickle.dump(result_set_d,f)


# In[12]:


with open(result_set_fp+".pickle","rb+") as frb:
    char2idx = pickle.load(frb)
list(itertools.islice(char2idx.items(),10))
len(char2idx)


# ### idx2char

# In[13]:


idx2char={v:k for k,v in char2idx.items()}
list(itertools.islice(idx2char.items(),10))
len(idx2char)


# ### encoded (doc2idx)

# In[24]:


with open(fp,"r+") as fr:
    text = fr.read()

encoded = np.array([char2idx[c] for c in tqdm(text)])
np.save(coded_article_fp,encoded)  # 14G


# In[14]:


encoded = np.load(coded_article_fp).astype(np.float32)
encoded.shape
encoded[:10]


# ### get_batcheså‡½æ•°

# In[15]:


def get_batches(encoded, batch_size, n_steps, verbose=False):
    chunk_len = batch_size*n_steps 
    n_chunk = int(len(encoded)/chunk_len)
    arr = encoded[:chunk_len*n_chunk]  # æˆªå–æ•´æ•°å€çš„batch_size
    arr = arr.reshape((batch_size,-1))

    for n in range(0, arr.shape[1], n_steps):
        x = arr[:, n:n+n_steps]
        y = np.zeros_like(x)
        y[:, :-1], y[:, -1] = x[:,1:], y[:, 0]  # è¿™é‡Œåº”è¯¥æœ‰é—®é¢˜ï¼Œæœ€åy[:, 0]åº”è¯¥æ”¹æˆä»å–åä¸€ä¸ªæ‰å¯¹ä¸ºä»€ä¹ˆæ˜¯åˆä»0å¼€å§‹å–
        yield x, y


# #### ä»¥ä¸‹æ˜¯å¯¹get_batcheså‡½æ•°çš„ä¸€ä¸ªéªŒè¯
# è¿™é‡Œå…¶å®æ˜¯æŠŠæ•´ä¸ªæ–‡æœ¬è¯­æ–™æŒ‰ã€Œå­—ç¬¦ã€ä½œä¸ºå•ä½åˆ‡åˆ†batchï¼Œå®Œå…¨èˆå¼ƒäº†ã€Œè¯ã€çš„æ¦‚å¿µ
# 
# ä¾‹å¦‚"I come from China"è¿›è¡Œget_batches
# - `batch_size=3,n_steps=4` è¯´æ˜è¿™ä¸ªbatché‡Œ**æœ‰3ä¸ªæ ·æœ¬ï¼ˆå¥å­ï¼‰ï¼Œæ¯ä¸ªæ ·æœ¬æ—¶é—´æ­¥é•¿ï¼ˆå­—ç¬¦æ•°ï¼‰æ˜¯4**
# - è¿™æ—¶ä¼šè®¡ç®—è¿™ä¸€å…±æ˜¯å¤šå°‘ä¸ªå­—ç¬¦ï¼š3x4=12
# - å†è®¡ç®—æ•´ä¸ªå¥å­æ”¯æŒå¤šå°‘ä¸ªbatch`n_chunk = int(len(encoded)/chunk_len)`ï¼ŒæŠŠä½™æ•°å»æ‰
# - æ­¤åæ¯æ¬¡éƒ½ç”¨`[:, n:n+n_steps]`æ¥è¿­ä»£å–ä¸€ä¸ªbatchçš„æ•°æ®
# - è¿™ä¸ªä¾‹å¥ä¸­åˆšå¥½åˆ°'I come from 'æ˜¯12ï¼Œåé¢çš„å°±è¢«å½“ä½™æ•°å»æ‰äº†
# - å¾—åˆ°çš„batchå¦‚ä¸‹ç¤ºä¾‹

# In[119]:


testStr = "I am from Chaoyang Beijing China"
batchSize=3
nSteps=4
print(f">>> æµ‹è¯•æ–‡æœ¬å¤Ÿã€Œ{len(testStr)//(batchSize*nSteps)}ã€ä¸ªchunkï¼Œä½™ä¸‹è¢«æˆªæ–­ä¸¢å¼ƒäº†")
actual_used = testStr[:len(testStr)//(batchSize*nSteps)*(batchSize*nSteps)]  # å®é™…ä½¿ç”¨çš„æ–‡æœ¬éƒ¨åˆ†
print(f"  -æ‰€ä»¥å®é™…ä½¿ç”¨çš„æµ‹è¯•æ–‡æœ¬æ˜¯:'{actual_used}'")
print(f"  -ä¸¢å¼ƒçš„éƒ¨åˆ†æ˜¯          :'{testStr[len(testStr)//(batchSize*nSteps)*(batchSize*nSteps):]}'")
actual_used_reshaped = np.array(list(actual_used)).reshape((batchSize,-1))
print(f">>> get_batcheså‡½æ•°é‡Œå¯¹æˆªæ–­åçš„arrè¿˜åšäº†ä¸ª`reshape((batchSize,-1))`ï¼Œæ•ˆæœæ˜¯:{actual_used_reshaped.shape}\n",actual_used_reshaped)
print("è¿™æ ·åé¢åœ¨å¯¹arrå–ç´¢å¼• [:, n:n+n_steps] æ—¶ï¼Œå…¶å®æ˜¯ï¼šç¬¬0ä¸ªbatchæ˜¯ä»æ¯è¡Œéƒ½å–ç¬¬0æ‰¹çš„ n_steps ä¸ªå…ƒç´ ")
print("è¿™æ ·çœ‹èµ·æ¥ä¸€ä¸ªbatché‡Œçš„å‡ ä¸ªè®­ç»ƒæ ·æœ¬ï¼ˆè®­ç»ƒsequenceï¼‰ä¹‹é—´å¹¶ä¸æ˜¯è¿ç»­çš„ï¼Œä½†æ˜¯å¹¶ä¸å½±å“ï¼Œæ ·æœ¬å†…çš„sequenceæ˜¯è¿ç»­çš„å°±è¡Œï¼ˆå³æ ·æœ¬è¿˜æ˜¯æ­£ç¡®é¡ºåºçš„å­—ç¬¦ï¼‰")

print("\n*****è¿™é‡Œyå–çš„åº”æœ‰é—®é¢˜ï¼Œæ¯ä¸ªè®­ç»ƒæ ·æœ¬çš„æœ€åä¸€ä¸ªyå¥½åƒæ˜¯é”™çš„*****")
for idx,(x,y) in enumerate(get_batches(np.array(list(testStr)),batch_size=3,n_steps=4)):
    print(f"\n>>> åœ¨ç¬¬{idx}ä¸ªbatché‡Œ")
    print(f"x:\n",x)
    print(f"y (xçš„å­—ç¬¦å¾€åå»¶ä¸€ä¸ª):\n",y)


# In[ ]:


text_g = load_f(fp)
def get_batches(text_generator, batch_size, time_step, verbose=False):
    X_batch,Y_batch = [],[]
    X_verbose,Y_verbose = [],[]
    chunk = list(itertools.islice(text_generator, batch_size))
    for text in chunk:
        # æ¯æ¬¡ç”Ÿæˆä¸€ç¯‡æ–‡ç« çš„æ ·æœ¬éƒ½ä»from_idxå¼€å§‹å–time_stepä¸ªå­—ç¬¦
        from_idx=np.random.randint(len(text)-time_step-1) # from_idxç”¨éšæœºæ•°,æœ€åçš„-1æ˜¯ä¸ºäº†æŠŠæœ€åä¸€ä¸ªå­—ç¬¦ç•™ç»™Y
        text_X = text[from_idx:from_idx+time_step]
        text_Y = text[from_idx+1:from_idx+time_step+1]
        if verbose:
            X_verbose.append(text_X)
            Y_verbose.append(text_Y)
        X_batch.append([char2idx[char] for char in text_X])
        Y_batch.append([char2idx[char] for char in text_Y])
    X_batch = np.array(X_batch)
    Y_batch = np.array(Y_batch)
    if verbose:
        return X_batch, Y_batch, np.array(X_verbose), np.array(Y_verbose)
    else:
        return X_batch, Y_batch

get_batches(text_g,20,5,True)


# ## è¶…å‚

# In[37]:


batch_size = 100         # Sequences per batch
num_steps = 100          # Number of sequence steps per batch
lstm_size = 512         # Size of hidden layers in LSTMs
num_layers = 2          # Number of LSTM layers
learning_rate = 0.01    # Learning rate
keep_prob = 0.5         # Dropout keep probability


# ## è®­ç»ƒ

# In[38]:


epochs = 40
# æ¯nè½®è¿›è¡Œä¸€æ¬¡å˜é‡ä¿å­˜
save_every_n = 200
summary_path = './tmp/tensorboard_anna'
base_model_path = "./tmp/lstm_anna/i{}_l{}.ckpt"

model = CharRNN(len(char2idx), batch_size=batch_size, num_steps=num_steps,
                lstm_size=lstm_size, num_layers=num_layers, 
                learning_rate=learning_rate, summary_path=summary_path)

# ç´¢å¼•è½¬æˆå­—ç¬¦
def _tochar(i):
        return idx2char[i]
_tochar_vec = np.vectorize(_tochar)
# å–è¾“å…¥çš„x y predsçš„å­—ç¬¦æ˜ å°„ç»“æœçš„ç¬¬ä¸€ä¸ªæ ·æœ¬
def get_sample_char(x,y,preds,verbose=False):
    # predså…ˆreshapeä¸€ä¸‹
    preds_reshape = preds.reshape(batch_size,num_steps,len(char2idx))
    preds_argmax = np.array([[np.argmax(each_seq) for each_seq in each_batch] for each_batch in preds_reshape])
    x_char,y_char,preds_char = [_tochar_vec(i) for i in [x,y,preds_argmax]]
    if verbose:
        print(f"""
        >>>preds: {preds.shape}
           |_reshape ==> {preds_reshape.shape}
             |_argmax ==> {preds_argmax.shape}
        """)

        print(f">>>x:{x.shape}\n",x,"\n",x_char)
        print(f">>>y:{y.shape}\n",y,"\n",y_char)
        print(f">>>preds_argmax:{preds_argmax.shape}\n",preds_argmax,"\n",preds_char)
    # è¿™æ ·å†™ä¹Ÿæ˜¯ä¸ºäº†é˜²æ­¢\nåœ¨printçš„æ—¶å€™è‡ªåŠ¨è½¬ä¹‰æ¢è¡Œ | æ”¾åˆ°æ•°ç»„ã€å­—å…¸é‡Œå°±ä¸ä¼šprintå‡ºæ¥æ¢è¡Œäº†
    res = {"x":"".join(x_char[0]),"y":"".join(y_char[0]),"preds":"".join(preds_char[0])}
    return res
                
def print_control(cnt,info):
    if cnt % 100 == 0:
        zprint(info)
#         if cnt <= 1000:
#             # 0~1k æ¯100è¾“å‡ºä¸€æ¬¡
#             zprint(info)
#         elif cnt <= 10000:
#             # 1k~10kæ¯1kè¾“å‡ºä¸€æ¬¡
#             if cnt % 1000 == 0:
#                 zprint(info)
#         else:
#             # 1wä»¥åæ¯5kè¾“å‡ºä¸€æ¬¡
#             if cnt % 5000 == 0:
#                 zprint(info)


with model.graph.as_default():
    saver = tf.train.Saver(max_to_keep=100)
    with tf.Session() as sess:
        sess.run(tf.global_variables_initializer())

        counter = 0
        for e in range(epochs):
            # Train network
            new_state = sess.run(model.initial_state)
            loss = 0
            et_batch_cnt = len(encoded) // (batch_size*num_steps)
            all_batch_data = get_batches(encoded, batch_size, num_steps)
            for x, y in all_batch_data:
                counter += 1
                feed = {model.inputs: x,
                        model.targets: y,
                        model.keep_prob: keep_prob,
                        model.initial_state: new_state}
                preds,batch_loss, new_state, _, merged_summary = sess.run([model.prediction,
                                                                     model.loss, 
                                                                     model.final_state, 
                                                                     model.optimizer,
                                                                     model.merge_summary,], 
                                                                     feed_dict=feed)
                
                # ä¿å­˜è¿›å±•
                model.writer.add_summary(merged_summary,counter)
                # è¾“å‡ºprint
                info = f"epoch: {e+1}/{epochs} batch: {counter:0>3d}/{et_batch_cnt} err: {batch_loss:.4f}"
                print_control(counter,info)
                # é¢å¤–è¾“å‡ºä¸€ä¸ªå®Œæ•´çš„å­—ç¬¦ä¸²print
                if counter % et_batch_cnt ==0 or counter == 1:
                    text_summary_list = [tf.summary.text(k, tf.convert_to_tensor(v)) 
                                         for k,v in get_sample_char(x,y,preds).items()]
                    text_summary = tf.summary.merge(text_summary_list)
                    text_summary_ = sess.run(text_summary)
                    model.writer.add_summary(text_summary_,counter)
                # save model graph
                if (counter % save_every_n == 0):
                    _=saver.save(sess, base_model_path.format(counter, lstm_size))

        _=saver.save(sess, base_model_path.format(counter, lstm_size))


# In[39]:


len(tf.train.get_checkpoint_state("./tmp/lstm_anna").all_model_checkpoint_paths)
tf.train.get_checkpoint_state("./tmp/lstm_anna")


# ## ç”Ÿæˆ
# - éœ€è¦æŒ‡å®š`n_samples`ï¼šéœ€è¦ç”Ÿæˆå¤šé•¿çš„å­—ç¬¦ä¸²
# - å°†è¾“å…¥çš„å•è¯è½¬æ¢ä¸ºå•ä¸ªå­—ç¬¦ç»„æˆçš„list
# - ä»ç¬¬ä¸€ä¸ªå­—ç¬¦å¼€å§‹è¾“å…¥CharRNN
# - ä»é¢„æµ‹ç»“æœä¸­é€‰å–å‰top_nä¸ªæœ€å¯èƒ½çš„å­—ç¬¦ï¼ŒæŒ‰é¢„æµ‹ç»“æœæä¾›çš„å„ä¸ªå­—ç¬¦çš„æ¦‚ç‡è¿›è¡Œnp.random.choice
#  - `pick_top_n`é‡Œæ·»åŠ äº†`copy()`æ–¹æ³•ï¼Œé¿å…ç›´æ¥æ›´æ”¹å‚æ•°

# In[40]:


def pick_top_n(preds_, vocab_size, top_n=5, random=False):
    """
    ä»é¢„æµ‹ç»“æœä¸­é€‰å–å‰top_nä¸ªæœ€å¯èƒ½çš„å­—ç¬¦ï¼ŒæŒ‰é¢„æµ‹ç»“æœæä¾›çš„å„ä¸ªå­—ç¬¦çš„æ¦‚ç‡è¿›è¡Œnp.random.choice
    
    preds_: é¢„æµ‹ç»“æœ
    vocab_size
    top_n
    """
    preds = preds_.copy()  # é¿å…æ”¹å˜åŸpreds
    p = np.squeeze(preds)
    # å°†é™¤äº†top_nä¸ªé¢„æµ‹å€¼çš„ä½ç½®éƒ½ç½®ä¸º0
    p[np.argsort(p)[:-top_n]] = 0
    # å½’ä¸€åŒ–æ¦‚ç‡
    p = p / np.sum(p)
    # éšæœºé€‰å–ä¸€ä¸ªå­—ç¬¦ / æˆ–è€…å–æ¦‚ç‡æœ€å¤§çš„å­—ç¬¦
    c = np.random.choice(vocab_size, 1, p=p)[0] if random else np.argmax(preds)
    return c

def sample(checkpoint, n_samples, lstm_size,num_layers, vocab_size, prime="The ", random=False):
    """
    ç”Ÿæˆæ–°æ–‡æœ¬
    
    checkpoint: æŸä¸€è½®è¿­ä»£çš„å‚æ•°æ–‡ä»¶
    n_sample: æ–°é—»æœ¬çš„å­—ç¬¦é•¿åº¦
    lstm_size: éšå±‚ç»“ç‚¹æ•°
    vocab_size
    prime: èµ·å§‹æ–‡æœ¬
    """
    # å°†è¾“å…¥çš„å•è¯è½¬æ¢ä¸ºå•ä¸ªå­—ç¬¦ç»„æˆçš„list
    samples = [c for c in prime]
    print(f">>> samples: {samples}")
    # sampling=Trueæ„å‘³ç€batchçš„size=1 x 1
    model = CharRNN(len(char2idx), batch_size=1, num_steps=len(prime),
                    lstm_size=lstm_size, num_layers=num_layers, 
                    learning_rate=learning_rate)
    with model.graph.as_default():
        saver = tf.train.Saver()

        with tf.Session(config=sess_conf) as sess:
            # åŠ è½½æ¨¡å‹å‚æ•°ï¼Œæ¢å¤è®­ç»ƒ
            saver.restore(sess, checkpoint)
            feed = {model.inputs: np.array([char2idx[c] for c in prime]),
                    model.keep_prob: 1.,}
            preds, new_state = sess.run([model.prediction, model.final_state], 
                                        feed_dict=feed,
                                        options=run_opt)
            top5_prob=preds[0][np.argsort(preds[0])[-5:]]
            top5_idx = np.argsort(preds[0])[-5:]
            print(f">>> å¯¹æ•´ä¸ªprime: {prime} çš„é¢„æµ‹ç»“æœ  [shape]:{preds.shape}")
            print(f"    top5æ˜¯:{top5_prob}<==>{[idx2char[i] for i in top5_idx]}")
            next_char = pick_top_n(preds, vocab_size, random=random)
            print(f"    å¦‚æœæ­¤æ—¶é€‰å–topNç”Ÿæˆå­—ç¬¦(æ˜¯å¦éšæœº:{random})ï¼Œä¼šæ˜¯: [idx]:'{next_char}' [char]:'{idx2char[next_char]}'")
            
            
            # æ·»åŠ å­—ç¬¦åˆ°samplesä¸­
            samples.append(idx2char[c])
            
            inp = np.array([[c]])
            # ä¸æ–­ç”Ÿæˆå­—ç¬¦ï¼Œç›´åˆ°è¾¾åˆ°æŒ‡å®šæ•°ç›®
            for _ in range(n_samples):
                feed = {model.inputs: [[c]],
                        model.keep_prob: 1.}
                preds, new_state = sess.run([model.prediction, model.final_state], 
                                            feed_dict=feed,
                                            options=run_opt)

                c = pick_top_n(preds, vocab_size, random=random)
                samples.append(idx2char[c])

    return ''.join(samples)


# In[44]:


tf.reset_default_graph()
ckpt = tf.train.latest_checkpoint('./tmp/lstm_anna')
sample(ckpt,n_samples=2000,lstm_size=lstm_size,num_layers=num_layers,vocab_size=len(char2idx),prime="Far", random=True)


# # CharRNN å†…éƒ¨ç»†èŠ‚çš„æµ‹è¯•

# In[14]:


with open(result_set_fp+".pickle","rb+") as frb:
    char2idx = pickle.load(frb)

encoded = np.load(coded_article_fp)


# In[15]:


def get_batches_as_iter(encoded, batch_size, time_steps, verbose=False):
    chunk_len = batch_size*time_steps 
    n_chunk = int(len(encoded)/chunk_len)
    arr = encoded[:chunk_len*n_chunk]  # æˆªå–æ•´æ•°å€çš„batch_size
    arr = arr.reshape((batch_size,-1))

    for n in range(0, arr.shape[1], time_steps):
        x = arr[:, n:n+time_steps]
        y = np.zeros_like(x)
        y[:, :-1], y[:, -1] = x[:,1:], y[:, 0] 
        yield x, y


# In[45]:


time_steps = 10
lstm_layers = [256]*2
lstm_size = lstm_layers[0]
num_classes = len(char2idx)
default_BS = 20
default_x,default_y=list(itertools.islice(get_batches_as_iter(encoded, batch_size=default_BS, time_steps=time_steps),1))[0]
print(f">>> default_BS: {default_BS}")
print(f">>> default_x: {default_x.shape}\n",default_x[:3,:10])
print(f">>> default_y: {default_y.shape}\n",default_y[:3,:10])


# In[68]:


tf.reset_default_graph()
# placeholder
inpBS = tf.placeholder(tf.int32, [], name="batch_size")
inpX = tf.placeholder(tf.int32, shape=(None, time_steps), name="inpX")
inpY = tf.placeholder(tf.int32, shape=(None), name="inpY")
X = tf.one_hot(inpX, depth=len(char2idx))
Y = tf.one_hot(inpY, depth=len(char2idx))
# LSTM æ„å»º
lstm_cell_list = []
for nodes_size in lstm_layers:
    lstm = tf.contrib.rnn.BasicLSTMCell(nodes_size)
    lstm_dropout = tf.contrib.rnn.DropoutWrapper(lstm, output_keep_prob=0.8)
    lstm_cell_list.append(lstm_dropout)
mlstm_cell = tf.contrib.rnn.MultiRNNCell(lstm_cell_list)
initial_state = mlstm_cell.zero_state(inpBS, tf.float32)
lstm_output, lstm_final_state = tf.nn.dynamic_rnn(mlstm_cell, X, initial_state = initial_state)

# formt output
seq_output = tf.concat(lstm_output, axis=1) 
softmax_x = tf.reshape(seq_output, [-1, lstm_size])
softmax_w = tf.Variable(tf.truncated_normal([lstm_size, num_classes], stddev=0.1))
softmax_b = tf.Variable(tf.zeros(num_classes))
logits = tf.matmul(softmax_x, softmax_w) + softmax_b
pred = tf.nn.softmax(logits, name='predictions')

# è®¡ç®—loss
y_reshaped = tf.reshape(Y, [-1, num_classes])
loss_ce = tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y_reshaped)
loss = tf.reduce_mean(loss_ce)

# optimize
tvars = tf.trainable_variables()
grad_clip = 5
grads, _ = tf.clip_by_global_norm(tf.gradients(loss, tvars), grad_clip)
train_op = tf.train.AdamOptimizer(0.01)
optimizer = train_op.apply_gradients(zip(grads, tvars))

default_feed = {inpBS:default_BS, inpX:default_x, inpY:default_y}
with tf.Session(config=sess_conf) as sess:
    sess.run(tf.global_variables_initializer())
    inpX_,X_,inpY_,Y_,is_,lo_,lfs_each_layer = sess.run([inpX,X,inpY,Y,initial_state,lstm_output,lstm_final_state], feed_dict=default_feed)
    seq_output_,sf_x,sf_w,sf_b = sess.run([seq_output,softmax_x,softmax_w,softmax_b], feed_dict=default_feed)
    logits_,pred_,y_reshaped_ = sess.run([logits,pred,y_reshaped], feed_dict=default_feed)
    loss_ce_,loss_,_ = sess.run([loss_ce,loss,optimizer],feed_dict=default_feed)
    print(f"""\n>>> æµç¨‹å¦‚ä¸‹
    inpX: {inpX_.shape}
    +onehot=> X: {X_.shape}
    +mlstm=> lstm_output: {lo_.shape}
    +reshape=> softmax_x: {sf_x.shape}
    +softmax(just matmul)=> logits: {logits_.shape}
    
    inpY: {inpY_.shape}
    +onehot=> Y: {Y_.shape}
    +reshape=> y_reshaped: {y_reshaped_.shape}
    
    CE(logits,y_reshaped): {loss_ce_.shape}
    +reduce_mean=> loss: {loss_.shape},scalar:{loss_:.4f}
    """
    )
    print(f">>> X_: {X_.shape}\n")
    print(f">>> Y_: {Y_.shape}\n")
    print(">>> lstm_final_state:")
    for idx,lfs in enumerate(lfs_each_layer):
        print(f"    >>> [layer]:{idx} [c_state:]: {lfs.c.shape}\n")
        print(f"    >>> [layer]:{idx} [h_state:]: {lfs.h.shape}\n")
    print(f">>> lstm_output: {lo_.shape}\n")
    print(f">>> seq_output_: {seq_output_.shape}\n")
    print("seq_output çš„ç¡®æ²¡æœ‰èµ·åˆ°ä½œç”¨,tfä¸­å¯¹ä¸€ä¸ªtensorä½¿ç”¨concatä»€ä¹ˆéƒ½ä¸ä¼šæ”¹å˜ï¼Œä¸€èˆ¬æ˜¯å¯¹ä¸€ä¸ªå†…éƒ¨å…ƒç´ æ˜¯tensorçš„liståšconcat")
    print(f">>> sf_x: {sf_x.shape} sf_w: {sf_w.shape} sf_b: {sf_b.shape}")
    print(f">>> logits_: {logits_.shape} pred_: {pred_.shape}")
    
    
    


# In[ ]:




