{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-16T06:46:35.445798Z",
     "start_time": "2019-08-16T06:46:35.208938Z"
    }
   },
   "outputs": [],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "%matplotlib inline\n",
    "from tqdm.auto import tqdm\n",
    "import concurrent.futures\n",
    "from multiprocessing import Pool\n",
    "import copy,os,sys,psutil\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-16T09:15:59.401892Z",
     "start_time": "2019-08-16T09:15:59.192552Z"
    }
   },
   "outputs": [],
   "source": [
    "import fasttext\n",
    "import json\n",
    "from sklearn.model_selection import StratifiedShuffleSplit,GridSearchCV\n",
    "import numpy as np\n",
    "import itertools\n",
    "import re\n",
    "from tqdm.auto import tqdm\n",
    "from zac_pyutils import ExqUtils  # from pip install\n",
    "from zac_pyutils.ExqUtils import zprint\n",
    "import os\n",
    "from collections import deque\n",
    "import multiprocessing\n",
    "import xgboost as xgb\n",
    "import pickle\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from gensim.models.wrappers import FastText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-16T10:09:01.765292Z",
     "start_time": "2019-08-16T10:09:01.762654Z"
    }
   },
   "outputs": [],
   "source": [
    "    def clean_punctuation(inp_text):\n",
    "        res = re.sub(r\"[~!@#$%^&*()_+-={\\}|\\[\\]:\\\";'<>?,./“”]\", r' ', inp_text)\n",
    "        res = re.sub(r\"\\\\u200[Bb]\", r' ', res)\n",
    "        res = re.sub(r\"\\n+\", r\" \", res)\n",
    "        res = re.sub(r\"\\s+\", \" \", res)\n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-16T10:09:05.014664Z",
     "start_time": "2019-08-16T10:09:04.697007Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a19b8a1b51334dbf8c0dd4c8787d9e57",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=500), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import random\n",
    "item = random.shuffle([(1,2),(1,3),(1,4),(2,2),(2,3),(2,4)])\n",
    "\n",
    "\n",
    "filePath = \"/home/zhoutong/nlp/data/labeled_taste_test.json_down_sampled\"\n",
    "f_iter = ExqUtils.load_file_as_iter(filePath)\n",
    "tokens_list = deque()\n",
    "label_list = deque()\n",
    "for l in tqdm(list(f_iter)[:500]):\n",
    "    text,label = l.split(\"__label__\")\n",
    "    tokens = clean_punctuation(text).split(\" \")\n",
    "    tokens_list.append(tokens)\n",
    "    label_list.append(label)\n",
    "tokens_arr, label_arr = np.array(tokens_list), np.array(label_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-16T10:09:07.740813Z",
     "start_time": "2019-08-16T10:09:07.728566Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OneHotEncoder(categorical_features='all', dtype=<class 'numpy.float64'>,\n",
       "       handle_unknown='error', n_values='auto', sparse=True)"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "array([4])"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 1., 0.],\n",
       "       [1., 0., 0., 0.],\n",
       "       [0., 0., 1., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., 1.],\n",
       "       [0., 1., 0., 0.],\n",
       "       [0., 0., 1., 0.]])"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc = OneHotEncoder()\n",
    "\n",
    "enc.fit(label_arr.reshape([-1,1]))\n",
    "enc.n_values_\n",
    "label_onehot_arr = enc.transform(label_arr.reshape(-1,1)).toarray()\n",
    "label_onehot_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-16T09:27:50.120047Z",
     "start_time": "2019-08-16T09:27:38.239703Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model = fasttext.load_model(\"/home/zhoutong/nlp/data/cc.en.300.bin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-16T09:33:07.777730Z",
     "start_time": "2019-08-16T09:33:07.761953Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "333"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "333"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "array([ 9.25558736e-04, -1.85379684e-02, -2.69682091e-02,  2.99833785e-03,\n",
       "       -3.12423445e-02,  2.42745318e-02, -2.34297365e-02, -6.20268052e-03,\n",
       "       -1.95013676e-02,  2.29928992e-03, -4.30899626e-03,  3.11395433e-02,\n",
       "        6.41629519e-03, -1.15201147e-02, -2.49226857e-02,  1.83201712e-02,\n",
       "        1.48216458e-02, -1.68805458e-02,  3.79943922e-02,  3.60130407e-02,\n",
       "       -1.52527280e-02,  1.02450419e-02, -1.36954859e-02, -3.71798165e-02,\n",
       "        2.76524574e-03, -3.03186551e-02, -1.45839686e-02,  1.79679375e-02,\n",
       "       -1.81397740e-02,  1.62054271e-01, -1.10978819e-03, -8.12156033e-03,\n",
       "        5.07346168e-03, -3.85499559e-02,  2.30662115e-02,  3.24836150e-02,\n",
       "       -1.40327793e-02,  2.32527871e-02, -1.05922502e-02,  7.89642334e-03,\n",
       "       -1.83669329e-02,  1.86136477e-02, -2.12460775e-02, -7.03456253e-03,\n",
       "        1.82290860e-02,  2.91579273e-02, -2.63996962e-02, -1.18836639e-02,\n",
       "        1.14756413e-02,  1.81252602e-02,  1.52050685e-02, -1.68511737e-02,\n",
       "       -2.43493300e-02,  9.81618185e-03, -1.88747421e-02, -2.63679642e-02,\n",
       "       -5.04967645e-02, -3.29004899e-02, -3.61198895e-02,  1.68025475e-02,\n",
       "       -1.28163565e-02,  1.46260252e-03,  7.87343457e-03, -3.01692672e-02,\n",
       "        7.78630609e-03, -1.95113197e-02, -2.61922553e-02,  1.37031013e-02,\n",
       "        3.47060547e-03, -4.06132229e-02,  6.28246441e-02,  1.07694650e-02,\n",
       "        1.79405399e-02, -4.97234166e-02,  7.02735968e-04, -1.59497242e-02,\n",
       "        1.45341465e-02,  2.65735183e-02,  4.53746952e-02, -3.13292556e-02,\n",
       "       -1.61257461e-02,  1.13463625e-02, -1.93997249e-02,  1.93988588e-02,\n",
       "        3.40617932e-02, -4.29525692e-03, -4.49437369e-03, -1.84999295e-02,\n",
       "       -1.36975609e-02,  5.86164277e-03, -2.81149540e-02,  1.88702848e-02,\n",
       "        5.16412184e-02, -1.40364980e-02,  1.20702460e-02,  4.03732471e-02,\n",
       "       -2.20206492e-02,  1.63446143e-02, -1.34476069e-02,  8.85015167e-03,\n",
       "       -1.06386626e-02,  4.30149958e-02,  9.06960398e-04, -5.19452244e-02,\n",
       "       -9.51873045e-03, -1.29487049e-02, -2.59748604e-02,  8.90535861e-03,\n",
       "        3.99723509e-03,  1.80312563e-02,  1.68153998e-02, -1.47630591e-02,\n",
       "        2.99220271e-02,  1.19307525e-02,  6.76910579e-03, -3.80000509e-02,\n",
       "        4.54368442e-03, -3.94984148e-03, -7.90205691e-03, -1.00531001e-02,\n",
       "       -7.41368858e-04,  2.70098671e-02,  2.00043619e-02, -1.26871178e-02,\n",
       "        4.58208332e-03, -1.13914553e-02, -8.54460616e-03, -1.72227097e-03,\n",
       "       -1.33561688e-02,  3.04174647e-02, -6.13180697e-02,  5.97110484e-03,\n",
       "        8.85899458e-03,  7.97473919e-03, -3.60757997e-03, -2.15478968e-02,\n",
       "       -3.80739234e-02, -2.82930806e-02,  4.17929292e-02,  4.57273163e-02,\n",
       "       -2.26315372e-02, -8.62607546e-03,  3.15969298e-03, -1.57944094e-02,\n",
       "        2.64532585e-02, -1.75317598e-03, -3.38884801e-01,  2.47240067e-02,\n",
       "       -5.74997533e-03, -2.95939576e-02, -9.29314271e-02,  1.12837749e-02,\n",
       "        6.53621834e-03,  6.52080961e-03, -1.00071887e-02, -1.04241353e-03,\n",
       "        9.07768011e-02,  3.34048993e-03,  1.84833761e-02,  4.11679596e-02,\n",
       "       -1.94951252e-03,  1.47368917e-02,  4.16431725e-02, -6.03001472e-03,\n",
       "       -1.97299700e-02,  3.40058957e-03,  1.23578636e-02, -3.33409570e-02,\n",
       "       -1.37236081e-02, -4.67676520e-02,  2.20863819e-02, -3.50532345e-02,\n",
       "       -1.15376310e-02, -1.73210901e-05, -8.33728258e-03, -8.14496819e-03,\n",
       "       -4.77034925e-03, -2.85670534e-02,  1.87111683e-02, -6.17785193e-03,\n",
       "       -2.06324365e-02,  6.73244298e-02, -4.81969826e-02,  2.20090225e-02,\n",
       "       -2.36562174e-02,  6.16658898e-03, -9.93601326e-03, -1.12315891e-02,\n",
       "        1.16119934e-02, -2.62963213e-03,  9.08013957e-04, -2.29715519e-02,\n",
       "       -1.15831876e-02,  8.37443303e-03,  3.07025947e-02, -1.19599728e-02,\n",
       "        5.86876355e-04, -1.85653158e-02,  4.30553369e-02,  1.74468886e-02,\n",
       "        2.57196976e-03,  1.55710220e-01,  1.01576578e-02, -1.00307190e-03,\n",
       "       -4.77485359e-03,  3.57431248e-02,  1.73190963e-02, -8.62869900e-03,\n",
       "        2.48747300e-02, -2.40026545e-02, -7.30902422e-03,  9.03684471e-04,\n",
       "       -4.88487333e-02, -1.63948000e-03,  4.37587015e-02, -2.01477744e-02,\n",
       "       -2.17201356e-02, -2.22228630e-03,  3.32863666e-02,  1.51479924e-02,\n",
       "        8.56121350e-03,  7.30064092e-03, -2.29598936e-02,  1.04563367e-02,\n",
       "       -1.14798509e-02,  1.25622880e-02, -2.55136867e-03,  3.67810465e-02,\n",
       "       -6.57281140e-03,  4.02849540e-03,  2.55666133e-02, -1.95416249e-02,\n",
       "       -1.57499220e-02,  2.62840539e-02,  1.44830309e-02,  1.14377653e-02,\n",
       "        1.16791213e-02, -1.95925850e-02, -1.46588832e-02,  1.01658432e-02,\n",
       "       -2.77993884e-02,  2.77049541e-02, -8.50358512e-04, -6.67596459e-02,\n",
       "        4.37037230e-01, -2.72897426e-02,  2.07893513e-02,  7.56628718e-03,\n",
       "       -6.52410323e-03, -1.12224184e-02,  4.28622514e-02,  6.81135105e-03,\n",
       "        2.31726170e-02, -5.43525442e-02,  3.45156267e-02, -1.67800207e-02,\n",
       "       -1.86517704e-02, -1.88331585e-02, -1.14985108e-02,  4.20300029e-02,\n",
       "        4.25241888e-03,  3.74591835e-02, -4.18473966e-02,  1.45562470e-03,\n",
       "       -2.10112259e-02,  9.31144785e-03, -1.58616677e-02, -1.23981079e-02,\n",
       "        1.05474545e-02, -9.50919557e-03, -2.80321483e-02, -2.87650395e-02,\n",
       "       -1.81767046e-02, -1.64147641e-04, -1.29090985e-02, -2.34856289e-02,\n",
       "       -2.63283253e-02, -4.90317419e-02,  3.89457541e-03, -2.64749327e-03,\n",
       "       -7.70553621e-03, -2.03316528e-02, -1.17945261e-01, -3.73277478e-02,\n",
       "       -3.10518909e-02, -3.32873091e-02,  1.13831712e-02, -4.06477638e-02,\n",
       "       -4.04691212e-02, -2.29979549e-02,  4.40777093e-02, -1.78646110e-02,\n",
       "       -1.97115485e-02, -2.00572629e-02, -3.07933874e-02,  3.27916823e-05,\n",
       "        2.14344896e-02,  1.05405547e-01, -5.11658192e-02, -2.00441368e-02],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = tokens_arr[0]\n",
    "len(tokens)\n",
    "len([model[t] for t in tokens])\n",
    "np.mean([model[t] for t in tokens], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-16T09:36:22.624021Z",
     "start_time": "2019-08-16T09:36:22.605056Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2., 2., 5.])"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "array([1, 2, 3, 3, 2, 7])"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "array([[1, 2, 3],\n",
       "       [3, 2, 7]])"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "array([[1, 2, 3],\n",
       "       [3, 2, 7]])"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean([np.array([1,2,3]),np.array([3,2,7])], axis=0)\n",
    "np.concatenate([np.array([1,2,3]),np.array([3,2,7])], axis=0)\n",
    "np.stack([np.array([1,2,3]),np.array([3,2,7])], axis=0)\n",
    "np.array([np.array([1,2,3]),np.array([3,2,7])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-16T10:11:06.501703Z",
     "start_time": "2019-08-16T10:11:06.451659Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 0), (2, 1), (3, 2)]"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokensSet = set(t for tokens in tokens_arr for t in tokens)\n",
    "list(zip([1,2,3],range(3)))\n",
    "tokens2idx = list(zip(tokensSet,range(len(tokensSet))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-16T10:11:08.848198Z",
     "start_time": "2019-08-16T10:11:08.808240Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('', 0),\n",
       " ('th\\u200bTAURUS', 1),\n",
       " ('grind', 2),\n",
       " ('Acknowledges', 3),\n",
       " ('Harington', 4),\n",
       " ('attainable', 5),\n",
       " ('Wipro', 6),\n",
       " ('firework', 7),\n",
       " ('Jovani', 8),\n",
       " ('Laga', 9),\n",
       " ('Selenocysteine', 10),\n",
       " ('diplomacy', 11),\n",
       " ('Welsh', 12),\n",
       " ('Shiwaji', 13),\n",
       " ('Massachusetts', 14),\n",
       " ('Dragon', 15),\n",
       " ('longterm', 16),\n",
       " ('Command', 17),\n",
       " ('din', 18),\n",
       " ('IPL', 19),\n",
       " ('Revaluation', 20),\n",
       " ('OK', 21),\n",
       " ('counting', 22),\n",
       " ('taxpayers', 23),\n",
       " ('perk', 24),\n",
       " ('sheets', 25),\n",
       " ('warsShareholders', 26),\n",
       " ('jangli', 27),\n",
       " ('currencies', 28),\n",
       " ('mastersmuaythai', 29),\n",
       " ('assets', 30),\n",
       " ('email', 31),\n",
       " ('genuinity', 32),\n",
       " ('Gary', 33),\n",
       " ('Charmme', 34),\n",
       " ('CNBC', 35),\n",
       " ('bouquet', 36),\n",
       " ('Shamita', 37),\n",
       " ('Gavin', 38),\n",
       " ('piss', 39),\n",
       " ('obsolete', 40),\n",
       " ('profitable', 41),\n",
       " ('AusOpen', 42),\n",
       " ('Swarth', 43),\n",
       " ('grabs', 44),\n",
       " ('Isley', 45),\n",
       " ('Aher', 46),\n",
       " ('Dopamine', 47),\n",
       " ('Cabo', 48),\n",
       " ('Won’t', 49),\n",
       " ('legendary', 50),\n",
       " ('installments', 51),\n",
       " ('BALL', 52),\n",
       " ('Transfer', 53),\n",
       " ('Bog', 54),\n",
       " ('bulldozed', 55),\n",
       " ('Kia', 56),\n",
       " ('Based', 57),\n",
       " ('Senior', 58),\n",
       " ('indications', 59),\n",
       " ('carved', 60),\n",
       " ('lasts', 61),\n",
       " ('VEHICLE', 62),\n",
       " ('Cinemas', 63),\n",
       " ('Infy', 64),\n",
       " ('Medchal', 65),\n",
       " ('frivolous', 66),\n",
       " ('Israel…', 67),\n",
       " ('switching', 68),\n",
       " ('nuts', 69),\n",
       " ('Switching', 70),\n",
       " ('Science', 71),\n",
       " ('Warrier', 72),\n",
       " ('Tihar', 73),\n",
       " ('Elizabeth', 74),\n",
       " ('AND', 75),\n",
       " ('lifted', 76),\n",
       " ('Capitol', 77),\n",
       " ('Inspired', 78),\n",
       " ('DOCUMENTS', 79),\n",
       " ('der', 80),\n",
       " ('donations', 81),\n",
       " ('Di', 82),\n",
       " ('Prabhakar', 83),\n",
       " ('Harshil', 84),\n",
       " ('TOILET', 85),\n",
       " ('fide', 86),\n",
       " ('lines', 87),\n",
       " ('goat', 88),\n",
       " ('Teams', 89),\n",
       " ('GS', 90),\n",
       " ('acquire', 91),\n",
       " ('Capital’s', 92),\n",
       " ('spray', 93),\n",
       " ('promoter', 94),\n",
       " ('Colby', 95),\n",
       " ('hdr', 96),\n",
       " ('Jammu', 97),\n",
       " ('McFarlane', 98),\n",
       " ('David', 99),\n",
       " ('offsets', 100),\n",
       " ('Davies', 101),\n",
       " ('warns', 102),\n",
       " ('bullshit', 103),\n",
       " ('Pacific', 104),\n",
       " ('threads', 105),\n",
       " ('approximate', 106),\n",
       " ('Aberdeen', 107),\n",
       " ('bolted', 108),\n",
       " ('Mirzapur', 109),\n",
       " ('refugees', 110),\n",
       " ('flame', 111),\n",
       " ('inspired', 112),\n",
       " ('call', 113),\n",
       " ('Murree', 114),\n",
       " ('stereotypes', 115),\n",
       " ('doorstep', 116),\n",
       " ('Brembo', 117),\n",
       " ('surge', 118),\n",
       " ('happening', 119),\n",
       " ('Rakul’s', 120),\n",
       " ('Combat', 121),\n",
       " ('Westside', 122),\n",
       " ('policemen', 123),\n",
       " ('amid', 124),\n",
       " ('Coupling', 125),\n",
       " ('Sham', 126),\n",
       " ('spa', 127),\n",
       " ('Nayeem', 128),\n",
       " ('Governor', 129),\n",
       " ('relax', 130),\n",
       " ('Sarkar', 131),\n",
       " ('Albion', 132),\n",
       " ('profession', 133),\n",
       " ('players', 134),\n",
       " ('placements', 135),\n",
       " ('assumed', 136),\n",
       " ('Curiel', 137),\n",
       " ('vibrant', 138),\n",
       " ('fab', 139),\n",
       " ('Macdonald', 140),\n",
       " ('UPHOLD', 141),\n",
       " ('SIX', 142),\n",
       " ('likely', 143),\n",
       " ('await', 144),\n",
       " ('mice', 145),\n",
       " ('geographies', 146),\n",
       " ('normal', 147),\n",
       " ('world…', 148),\n",
       " ('rubber', 149),\n",
       " ('Kumaraswamy', 150),\n",
       " ('Bajirao', 151),\n",
       " ('Lent', 152),\n",
       " ('bunch', 153),\n",
       " ('documents', 154),\n",
       " ('supplied', 155),\n",
       " ('infinity', 156),\n",
       " ('spy', 157),\n",
       " ('standouts', 158),\n",
       " ('conflicting', 159),\n",
       " ('OPPO', 160),\n",
       " ('PPI', 161),\n",
       " ('honored', 162),\n",
       " ('friend', 163),\n",
       " ('countries', 164),\n",
       " ('Radiant', 165),\n",
       " ('tapered', 166),\n",
       " ('disappearance', 167),\n",
       " ('Harbour', 168),\n",
       " ('Land', 169),\n",
       " ('legislators', 170),\n",
       " ('Injection', 171),\n",
       " ('contraction', 172),\n",
       " ('revoking', 173),\n",
       " ('Lança', 174),\n",
       " ('search', 175),\n",
       " ('onlyJAIPUR', 176),\n",
       " ('Dalal', 177),\n",
       " ('Bridal', 178),\n",
       " ('smiling', 179),\n",
       " ('sampling', 180),\n",
       " ('publishing', 181),\n",
       " ('acceptable', 182),\n",
       " ('precautions', 183),\n",
       " ('Uttam', 184),\n",
       " ('ground', 185),\n",
       " ('Helvétique', 186),\n",
       " ('approx', 187),\n",
       " ('forayed', 188),\n",
       " ('Mohammed', 189),\n",
       " ('William', 190),\n",
       " ('Maharshi', 191),\n",
       " ('Gauri', 192),\n",
       " ('projected', 193),\n",
       " ('engages', 194),\n",
       " ('Koffee', 195),\n",
       " ('Margaret', 196),\n",
       " ('Private', 197),\n",
       " ('tradition', 198),\n",
       " ('defense', 199),\n",
       " ('rewarding', 200),\n",
       " ('Tamil', 201),\n",
       " ('Eli', 202),\n",
       " ('wk', 203),\n",
       " ('Akhilendra', 204),\n",
       " ('deepen', 205),\n",
       " ('hotels', 206),\n",
       " ('knee', 207),\n",
       " ('floods', 208),\n",
       " ('sober', 209),\n",
       " ('attitude', 210),\n",
       " ('Lakshan', 211),\n",
       " ('tempered', 212),\n",
       " ('Publishers', 213),\n",
       " ('TODAY', 214),\n",
       " ('continued', 215),\n",
       " ('independently', 216),\n",
       " ('bowed', 217),\n",
       " ('deformable', 218),\n",
       " ('MEB', 219),\n",
       " ('dharna', 220),\n",
       " ('Karmara', 221),\n",
       " ('Cold', 222),\n",
       " ('instead', 223),\n",
       " ('Federer', 224),\n",
       " ('Classically', 225),\n",
       " ('Plot', 226),\n",
       " ('Pallavi', 227),\n",
       " ('oppression', 228),\n",
       " ('Nashville', 229),\n",
       " ('taluks', 230),\n",
       " ('urge', 231),\n",
       " ('together', 232),\n",
       " ('subscriber', 233),\n",
       " ('eve', 234),\n",
       " ('these', 235),\n",
       " ('NewsroomPost', 236),\n",
       " ('bathtubs', 237),\n",
       " ('paid', 238),\n",
       " ('binge', 239),\n",
       " ('earliest', 240),\n",
       " ('congratulate', 241),\n",
       " ('ones', 242),\n",
       " ('Simranjeet', 243),\n",
       " ('onboarding', 244),\n",
       " ('ailerons', 245),\n",
       " ('pick', 246),\n",
       " ('Boy', 247),\n",
       " ('all', 248),\n",
       " ('What', 249),\n",
       " ('pajamas', 250),\n",
       " ('bizarre', 251),\n",
       " ('Khan’s', 252),\n",
       " ('Happy', 253),\n",
       " ('NakamuraShinsuke', 254),\n",
       " ('app', 255),\n",
       " ('Commerzbank', 256),\n",
       " ('grappling', 257),\n",
       " ('smallest', 258),\n",
       " ('takers', 259),\n",
       " ('hectic', 260),\n",
       " ('included', 261),\n",
       " ('bee', 262),\n",
       " ('contingencies', 263),\n",
       " ('referring', 264),\n",
       " ('al', 265),\n",
       " ('sworn', 266),\n",
       " ('export', 267),\n",
       " ('Friday', 268),\n",
       " ('Richter', 269),\n",
       " ('Unit', 270),\n",
       " ('identifies', 271),\n",
       " ('ordeal', 272),\n",
       " ('page', 273),\n",
       " ('sonalkalraFirst', 274),\n",
       " ('timed', 275),\n",
       " ('grad', 276),\n",
       " ('vanish', 277),\n",
       " ('birds', 278),\n",
       " ('output', 279),\n",
       " ('recall', 280),\n",
       " ('competencies', 281),\n",
       " ('gigabytes', 282),\n",
       " ('Circle', 283),\n",
       " ('ICMR', 284),\n",
       " ('weak', 285),\n",
       " ('Finals', 286),\n",
       " ('whale’s', 287),\n",
       " ('littered', 288),\n",
       " ('arrests', 289),\n",
       " ('pens', 290),\n",
       " ('tend', 291),\n",
       " ('became', 292),\n",
       " ('Abraham', 293),\n",
       " ('Nirahua', 294),\n",
       " ('sand', 295),\n",
       " ('disclose', 296),\n",
       " ('medico', 297),\n",
       " ('McDonald’s', 298),\n",
       " ('Interest', 299),\n",
       " ('PET', 300),\n",
       " ('perspective', 301),\n",
       " ('June', 302),\n",
       " ('Saurashtra', 303),\n",
       " ('Cy', 304),\n",
       " ('Rex', 305),\n",
       " ('ingenue', 306),\n",
       " ('characterised', 307),\n",
       " ('Ranks', 308),\n",
       " ('Believe', 309),\n",
       " ('would', 310),\n",
       " ('Patrick', 311),\n",
       " ('RPF', 312),\n",
       " ('Credit', 313),\n",
       " ('Hon', 314),\n",
       " ('John', 315),\n",
       " ('Jacinda', 316),\n",
       " ('performers', 317),\n",
       " ('MUST', 318),\n",
       " ('Broadway', 319),\n",
       " ('fantasies', 320),\n",
       " ('aksed', 321),\n",
       " ('cottage', 322),\n",
       " ('inked', 323),\n",
       " ('walks', 324),\n",
       " ('wardrobes', 325),\n",
       " ('Businessweek', 326),\n",
       " ('sharpen', 327),\n",
       " ('depiction', 328),\n",
       " ('supporters', 329),\n",
       " ('Meghna', 330),\n",
       " ('Robert', 331),\n",
       " ('Mela', 332),\n",
       " ('Rassie', 333),\n",
       " ('accountable', 334),\n",
       " ('Vernon', 335),\n",
       " ('fuss', 336),\n",
       " ('seller', 337),\n",
       " ('ornate', 338),\n",
       " ('item', 339),\n",
       " ('multiple', 340),\n",
       " ('presents', 341),\n",
       " ('casuals', 342),\n",
       " ('dental', 343),\n",
       " ('talent', 344),\n",
       " ('Tragedy', 345),\n",
       " ('helicopter', 346),\n",
       " ('Boult’s', 347),\n",
       " ('possibilities', 348),\n",
       " ('etching', 349),\n",
       " ('NCL', 350),\n",
       " ('Siddiqui', 351),\n",
       " ('forthcoming', 352),\n",
       " ('Airlines', 353),\n",
       " ('AIM', 354),\n",
       " ('CS', 355),\n",
       " ('Replete', 356),\n",
       " ('Armstrong', 357),\n",
       " ('devise', 358),\n",
       " ('mushrooms', 359),\n",
       " ('pump', 360),\n",
       " ('Model', 361),\n",
       " ('pole', 362),\n",
       " ('catalogue', 363),\n",
       " ('originated', 364),\n",
       " ('caffeinated', 365),\n",
       " ('Apex', 366),\n",
       " ('Fifty', 367),\n",
       " ('coaster', 368),\n",
       " ('disconnecting', 369),\n",
       " ('Gmitter', 370),\n",
       " ('Festivals', 371),\n",
       " ('free', 372),\n",
       " ('tyrannical', 373),\n",
       " ('teeth', 374),\n",
       " ('Honourable', 375),\n",
       " ('hangout', 376),\n",
       " ('relief', 377),\n",
       " ('upbeat', 378),\n",
       " ('bares', 379),\n",
       " ('tricolourfilm', 380),\n",
       " ('AFPArcade', 381),\n",
       " ('Even', 382),\n",
       " ('store', 383),\n",
       " ('Emmy', 384),\n",
       " ('relation', 385),\n",
       " ('distressed', 386),\n",
       " ('BRZ’s', 387),\n",
       " ('off’', 388),\n",
       " ('Fire', 389),\n",
       " ('various', 390),\n",
       " ('implement', 391),\n",
       " ('fees', 392),\n",
       " ('speculation', 393),\n",
       " ('Madikeri', 394),\n",
       " ('Knife', 395),\n",
       " ('inconclusive', 396),\n",
       " ('wise', 397),\n",
       " ('shelling', 398),\n",
       " ('Alec', 399),\n",
       " ('cancer', 400),\n",
       " ('brutally', 401),\n",
       " ('prompted', 402),\n",
       " ('rid', 403),\n",
       " ('Moths', 404),\n",
       " ('…', 405),\n",
       " ('Simran', 406),\n",
       " ('antagonists', 407),\n",
       " ('Hang', 408),\n",
       " ('Waves', 409),\n",
       " ('Sumit', 410),\n",
       " ('dependants', 411),\n",
       " ('Bialik', 412),\n",
       " ('Jabariya', 413),\n",
       " ('vicious', 414),\n",
       " ('Whitney', 415),\n",
       " ('punya', 416),\n",
       " ('Helping', 417),\n",
       " ('Magical', 418),\n",
       " ('challenged', 419),\n",
       " ('Ludhiana', 420),\n",
       " ('apples', 421),\n",
       " ('Striders', 422),\n",
       " ('courage', 423),\n",
       " ('somehow', 424),\n",
       " ('shelters', 425),\n",
       " ('hassle', 426),\n",
       " ('Himani', 427),\n",
       " ('ignores', 428),\n",
       " ('agenda', 429),\n",
       " ('complain', 430),\n",
       " ('scrutiny', 431),\n",
       " ('Venugopal', 432),\n",
       " ('Technical', 433),\n",
       " ('milk', 434),\n",
       " ('any', 435),\n",
       " ('Rick', 436),\n",
       " ('watchOS', 437),\n",
       " ('bound', 438),\n",
       " ('brilliance', 439),\n",
       " ('TWO', 440),\n",
       " ('external', 441),\n",
       " ('tatty', 442),\n",
       " ('Bahana', 443),\n",
       " ('foreseeable', 444),\n",
       " ('watering', 445),\n",
       " ('fable', 446),\n",
       " ('Duanne', 447),\n",
       " ('Ebony', 448),\n",
       " ('Search', 449),\n",
       " ('deliberation', 450),\n",
       " ('children’s', 451),\n",
       " ('refreshingly', 452),\n",
       " ('corporate', 453),\n",
       " ('Dagg', 454),\n",
       " ('La', 455),\n",
       " ('Novosphingobium', 456),\n",
       " ('Herbivores', 457),\n",
       " ('writes', 458),\n",
       " ('Maunakea', 459),\n",
       " ('sci', 460),\n",
       " ('conduct', 461),\n",
       " ('Bringing', 462),\n",
       " ('पैंट', 463),\n",
       " ('authenticated', 464),\n",
       " ('WISCONSIN', 465),\n",
       " ('regardless', 466),\n",
       " ('carefree', 467),\n",
       " ('flock', 468),\n",
       " ('french', 469),\n",
       " ('hooks', 470),\n",
       " ('minor', 471),\n",
       " ('dialogues', 472),\n",
       " ('Ratings', 473),\n",
       " ('needed', 474),\n",
       " ('intersection', 475),\n",
       " ('TVLine', 476),\n",
       " ('oncologists', 477),\n",
       " ('appears', 478),\n",
       " ('Shivakumar', 479),\n",
       " ('Sashti', 480),\n",
       " ('holders', 481),\n",
       " ('slapping', 482),\n",
       " ('—in', 483),\n",
       " ('last', 484),\n",
       " ('Hoss', 485),\n",
       " ('advertisers', 486),\n",
       " ('Freya', 487),\n",
       " ('Aayogand', 488),\n",
       " ('Sister', 489),\n",
       " ('Customers', 490),\n",
       " ('Brisbane', 491),\n",
       " ('BSF', 492),\n",
       " ('whitelisting', 493),\n",
       " ('economic', 494),\n",
       " ('storylines', 495),\n",
       " ('sensitive', 496),\n",
       " ('Aquarius', 497),\n",
       " ('officiate', 498),\n",
       " ('teenage', 499),\n",
       " ('administrations', 500),\n",
       " ('Kolhapur', 501),\n",
       " ('scrambling', 502),\n",
       " ('mainly', 503),\n",
       " ('uBlock', 504),\n",
       " ('billion', 505),\n",
       " ('authorizing', 506),\n",
       " ('Ratan', 507),\n",
       " ('Warner', 508),\n",
       " ('Gamer', 509),\n",
       " ('shortfall', 510),\n",
       " ('flimsy', 511),\n",
       " ('Rome', 512),\n",
       " ('‘look', 513),\n",
       " ('Atletico', 514),\n",
       " ('southeast', 515),\n",
       " ('authoring', 516),\n",
       " ('Designatronics', 517),\n",
       " ('Blenders', 518),\n",
       " ('what', 519),\n",
       " ('PE', 520),\n",
       " ('mp', 521),\n",
       " ('Red', 522),\n",
       " ('reckoning', 523),\n",
       " ('Loves', 524),\n",
       " ('head', 525),\n",
       " ('GOAT', 526),\n",
       " ('Sindhi', 527),\n",
       " ('Ghana', 528),\n",
       " ('Kangra', 529),\n",
       " ('Dhar', 530),\n",
       " ('coaching', 531),\n",
       " ('mascot', 532),\n",
       " ('Sharat', 533),\n",
       " ('Lokmanya', 534),\n",
       " ('tabled', 535),\n",
       " ('Hindustani', 536),\n",
       " ('advises', 537),\n",
       " ('beverage', 538),\n",
       " ('betterment', 539),\n",
       " ('dancer', 540),\n",
       " ('Shine', 541),\n",
       " ('constitution', 542),\n",
       " ('databases', 543),\n",
       " ('Rocketry', 544),\n",
       " ('resin', 545),\n",
       " ('showrooms', 546),\n",
       " ('Twenty', 547),\n",
       " ('worshipping', 548),\n",
       " ('wheeler', 549),\n",
       " ('Asks', 550),\n",
       " ('Phule', 551),\n",
       " ('maps', 552),\n",
       " ('Keita', 553),\n",
       " ('Africa', 554),\n",
       " ('burgundy', 555),\n",
       " ('policeman', 556),\n",
       " ('council', 557),\n",
       " ('diabetes', 558),\n",
       " ('sash', 559),\n",
       " ('sanctioned', 560),\n",
       " ('humour', 561),\n",
       " ('Sumati', 562),\n",
       " ('melt', 563),\n",
       " ('urged', 564),\n",
       " ('acids', 565),\n",
       " ('buses', 566),\n",
       " ('Akademy', 567),\n",
       " ('Applause', 568),\n",
       " ('hearing', 569),\n",
       " ('Towed', 570),\n",
       " ('inventory', 571),\n",
       " ('Bedi', 572),\n",
       " ('COKE', 573),\n",
       " ('DIY', 574),\n",
       " ('it', 575),\n",
       " ('observe', 576),\n",
       " ('Nottingham', 577),\n",
       " ('Thanu', 578),\n",
       " ('ayurveda', 579),\n",
       " ('Report’s', 580),\n",
       " ('Interception', 581),\n",
       " ('inexperienced', 582),\n",
       " ('statuesque', 583),\n",
       " ('arrives', 584),\n",
       " ('laying', 585),\n",
       " ('versus', 586),\n",
       " ('Him', 587),\n",
       " ('bureau', 588),\n",
       " ('symbolize', 589),\n",
       " ('stunned', 590),\n",
       " ('choices', 591),\n",
       " ('Rose', 592),\n",
       " ('Hetmyer', 593),\n",
       " ('BarThe', 594),\n",
       " ('recollection', 595),\n",
       " ('Rakulpreet', 596),\n",
       " ('Blu', 597),\n",
       " ('SCPD', 598),\n",
       " ('rahaa', 599),\n",
       " ('Promoter', 600),\n",
       " ('plays', 601),\n",
       " ('highest', 602),\n",
       " ('pysche', 603),\n",
       " ('Software', 604),\n",
       " ('duped', 605),\n",
       " ('Steel', 606),\n",
       " ('alleys', 607),\n",
       " ('mazakshi', 608),\n",
       " ('destabilise', 609),\n",
       " ('Polytechnic', 610),\n",
       " ('Tank', 611),\n",
       " ('notosuchian', 612),\n",
       " ('voice', 613),\n",
       " ('harks', 614),\n",
       " ('‘a', 615),\n",
       " ('overdue', 616),\n",
       " ('unreserved', 617),\n",
       " ('MBA', 618),\n",
       " ('bidder', 619),\n",
       " ('immaculate', 620),\n",
       " ('slugfest', 621),\n",
       " ('Material', 622),\n",
       " ('axial', 623),\n",
       " ('Plus’s', 624),\n",
       " ('New', 625),\n",
       " ('phonemaker', 626),\n",
       " ('Cosmetics', 627),\n",
       " ('cruiser', 628),\n",
       " ('shrank', 629),\n",
       " ('stabilization', 630),\n",
       " ('Quarterly', 631),\n",
       " ('techniques', 632),\n",
       " ('replacement', 633),\n",
       " ('Edition', 634),\n",
       " ('Almost', 635),\n",
       " ('capitulation', 636),\n",
       " ('Kroger’s', 637),\n",
       " ('mzemek', 638),\n",
       " ('Pacer', 639),\n",
       " ('grower', 640),\n",
       " ('Airport', 641),\n",
       " ('shifts', 642),\n",
       " ('pink', 643),\n",
       " ('CMC', 644),\n",
       " ('implicated', 645),\n",
       " ('Holder', 646),\n",
       " ('Updates', 647),\n",
       " ('ECG', 648),\n",
       " ('Heartbroken', 649),\n",
       " ('CBI’s', 650),\n",
       " ('lit', 651),\n",
       " ('trips', 652),\n",
       " ('allot', 653),\n",
       " ('reflects', 654),\n",
       " ('House', 655),\n",
       " ('eastern', 656),\n",
       " ('Efe', 657),\n",
       " ('donate', 658),\n",
       " ('microSD', 659),\n",
       " ('tracks', 660),\n",
       " ('Congress’s', 661),\n",
       " ('Blanchard', 662),\n",
       " ('Bolt', 663),\n",
       " ('Corporators', 664),\n",
       " ('compatible', 665),\n",
       " ('exact', 666),\n",
       " ('Resurrection', 667),\n",
       " ('Graphic', 668),\n",
       " ('arrive', 669),\n",
       " ('sarpanchs', 670),\n",
       " ('Ben', 671),\n",
       " ('youngster', 672),\n",
       " ('behaviours', 673),\n",
       " ('Bhargava', 674),\n",
       " ('Panindar', 675),\n",
       " ('Donning', 676),\n",
       " ('Ritchie', 677),\n",
       " ('withers', 678),\n",
       " ('CE', 679),\n",
       " ('Tavern', 680),\n",
       " ('Together', 681),\n",
       " ('Sehrawat', 682),\n",
       " ('Jenner', 683),\n",
       " ('और', 684),\n",
       " ('Machines', 685),\n",
       " ('Watan', 686),\n",
       " ('per', 687),\n",
       " ('modesty', 688),\n",
       " ('Grayson', 689),\n",
       " ('misrepresented', 690),\n",
       " ('becom', 691),\n",
       " ('Duty', 692),\n",
       " ('talaq', 693),\n",
       " ('funny', 694),\n",
       " ('Biorahul', 695),\n",
       " ('Pandora’s', 696),\n",
       " ('Akhtar', 697),\n",
       " ('Tonight', 698),\n",
       " ('launchpads', 699),\n",
       " ('Ellyse', 700),\n",
       " ('survived', 701),\n",
       " ('Doors', 702),\n",
       " ('innocent', 703),\n",
       " ('Amber', 704),\n",
       " ('dresser', 705),\n",
       " ('tropes', 706),\n",
       " ('best', 707),\n",
       " ('Mganet', 708),\n",
       " ('stitches', 709),\n",
       " ('through', 710),\n",
       " ('Down', 711),\n",
       " ('Cross', 712),\n",
       " ('happenings', 713),\n",
       " ('Ecole', 714),\n",
       " ('Zee', 715),\n",
       " ('gravitationally', 716),\n",
       " ('roof', 717),\n",
       " ('Salvation', 718),\n",
       " ('Lanka’s', 719),\n",
       " ('Pratap', 720),\n",
       " ('satellite', 721),\n",
       " ('overwrought', 722),\n",
       " ('dynastes', 723),\n",
       " ('bog', 724),\n",
       " ('Dekho', 725),\n",
       " ('MemorialCare', 726),\n",
       " ('described', 727),\n",
       " ('trivializing', 728),\n",
       " ('browser', 729),\n",
       " ('acarman', 730),\n",
       " ('sledge', 731),\n",
       " ('transforming', 732),\n",
       " ('Akila', 733),\n",
       " ('skins', 734),\n",
       " ('Fest', 735),\n",
       " ('Cole', 736),\n",
       " ('Pranav', 737),\n",
       " ('plates', 738),\n",
       " ('ESA', 739),\n",
       " ('Summit', 740),\n",
       " ('advert', 741),\n",
       " ('AUTO', 742),\n",
       " ('Output', 743),\n",
       " ('idle', 744),\n",
       " ('dire', 745),\n",
       " ('AFVs', 746),\n",
       " ('play', 747),\n",
       " ('apertures', 748),\n",
       " ('BDG', 749),\n",
       " ('Showtime', 750),\n",
       " ('petrol', 751),\n",
       " ('suffer', 752),\n",
       " ('room', 753),\n",
       " ('freckles', 754),\n",
       " ('split', 755),\n",
       " ('objectionable', 756),\n",
       " ('Oil', 757),\n",
       " ('Nathaniel', 758),\n",
       " ('hunger', 759),\n",
       " ('Mallikarjun', 760),\n",
       " ('Singhvi', 761),\n",
       " ('Somi', 762),\n",
       " ('resumed', 763),\n",
       " ('linkedin', 764),\n",
       " ('ace', 765),\n",
       " ('enter', 766),\n",
       " ('headroom', 767),\n",
       " ('lunar', 768),\n",
       " ('derailment', 769),\n",
       " ('ranging', 770),\n",
       " ('SAGITTARIUS', 771),\n",
       " ('consolidated', 772),\n",
       " ('selfie', 773),\n",
       " ('grip', 774),\n",
       " ('taxesNew', 775),\n",
       " ('design', 776),\n",
       " ('Snowball', 777),\n",
       " ('Gravidarum', 778),\n",
       " ('Phone', 779),\n",
       " ('Roman', 780),\n",
       " ('late', 781),\n",
       " ('NCP', 782),\n",
       " ('athletes', 783),\n",
       " ('denying', 784),\n",
       " ('activation', 785),\n",
       " ('Nathula', 786),\n",
       " ('Catholic', 787),\n",
       " ('mum', 788),\n",
       " ('aag', 789),\n",
       " ('spell’', 790),\n",
       " ('famous', 791),\n",
       " ('Eugenia', 792),\n",
       " ('Dadasaheb', 793),\n",
       " ('hunt', 794),\n",
       " ('Forty', 795),\n",
       " ('pedometer', 796),\n",
       " ('ranges', 797),\n",
       " ('Indians’', 798),\n",
       " ('secondly', 799),\n",
       " ('Saxo', 800),\n",
       " ('resisted', 801),\n",
       " ('Tablet', 802),\n",
       " ('inclusivity', 803),\n",
       " ('Goldsmith', 804),\n",
       " ('Architecture', 805),\n",
       " ('unrelenting', 806),\n",
       " ('Moody’s', 807),\n",
       " ('Rane', 808),\n",
       " ('Advocate', 809),\n",
       " ('slain', 810),\n",
       " ('didi', 811),\n",
       " ('Basic', 812),\n",
       " ('Identity', 813),\n",
       " ('ODD', 814),\n",
       " ('heartbreaking', 815),\n",
       " ('clicks', 816),\n",
       " ('list', 817),\n",
       " ('marking', 818),\n",
       " ('realistic', 819),\n",
       " ('Skin', 820),\n",
       " ('market', 821),\n",
       " ('fairly', 822),\n",
       " ('banking', 823),\n",
       " ('worms', 824),\n",
       " ('suite', 825),\n",
       " ('regrettable', 826),\n",
       " ('rued', 827),\n",
       " ('Accumulate', 828),\n",
       " ('inducing', 829),\n",
       " ('humidity', 830),\n",
       " ('Exlusion', 831),\n",
       " ('branch', 832),\n",
       " ('agriculturist', 833),\n",
       " ('anarkE', 834),\n",
       " ('vacation', 835),\n",
       " ('Isro', 836),\n",
       " ('proprietary', 837),\n",
       " ('treats', 838),\n",
       " ('entourage', 839),\n",
       " ('digit', 840),\n",
       " ('apparently', 841),\n",
       " ('air', 842),\n",
       " ('squashed', 843),\n",
       " ('While', 844),\n",
       " ('waved', 845),\n",
       " ('Fortune', 846),\n",
       " ('Parts', 847),\n",
       " ('Dori', 848),\n",
       " ('southpaw', 849),\n",
       " ('Espncricinfo', 850),\n",
       " ('NAD', 851),\n",
       " ('Matos', 852),\n",
       " ('kingdom', 853),\n",
       " ('undue', 854),\n",
       " ('Cremica', 855),\n",
       " ('affixed', 856),\n",
       " ('Byrasandra', 857),\n",
       " ('mountain', 858),\n",
       " ('milestone', 859),\n",
       " ('THERE', 860),\n",
       " ('perennial', 861),\n",
       " ('Surgical', 862),\n",
       " ('stocks', 863),\n",
       " ('Illustrations', 864),\n",
       " ('portal', 865),\n",
       " ('farmers', 866),\n",
       " ('unimpressed', 867),\n",
       " ('Years', 868),\n",
       " ('attacked', 869),\n",
       " ('DJs', 870),\n",
       " ('Lab', 871),\n",
       " ('implementation', 872),\n",
       " ('Navratri', 873),\n",
       " ('Apollo', 874),\n",
       " ('teammates', 875),\n",
       " ('Tinschert', 876),\n",
       " ('cervical', 877),\n",
       " ('qualified', 878),\n",
       " ('priming', 879),\n",
       " ('between', 880),\n",
       " ('dread', 881),\n",
       " ('Captures', 882),\n",
       " ('holiday', 883),\n",
       " ('Andheri', 884),\n",
       " ('Moon', 885),\n",
       " ('use', 886),\n",
       " ('ER', 887),\n",
       " ('Ralegan', 888),\n",
       " ('undertook', 889),\n",
       " ('blagged', 890),\n",
       " ('maelstrom', 891),\n",
       " ('shorts', 892),\n",
       " ('notion', 893),\n",
       " ('rockets', 894),\n",
       " ('using', 895),\n",
       " ('Mogensen', 896),\n",
       " ('Maid', 897),\n",
       " ('UA', 898),\n",
       " ('Rodrigues', 899),\n",
       " ('aperture', 900),\n",
       " ('Sushmita', 901),\n",
       " ('Barbadian', 902),\n",
       " ('Maternal', 903),\n",
       " ('weird', 904),\n",
       " ('zebra', 905),\n",
       " ('Operation', 906),\n",
       " ('ft', 907),\n",
       " ('published', 908),\n",
       " ('Umakant', 909),\n",
       " ('gods', 910),\n",
       " ('victim’s', 911),\n",
       " ('batsmen', 912),\n",
       " ('CALIFORNIA', 913),\n",
       " ('math', 914),\n",
       " ('struggled', 915),\n",
       " ('Gokul', 916),\n",
       " ('Nashik', 917),\n",
       " ('Batch', 918),\n",
       " ('moon', 919),\n",
       " ('swing', 920),\n",
       " ('Ivan’s', 921),\n",
       " ('surplus', 922),\n",
       " ('Textile', 923),\n",
       " ('optimal', 924),\n",
       " ('gas', 925),\n",
       " ('Rajnath', 926),\n",
       " ('coatings', 927),\n",
       " ('DivorceForce', 928),\n",
       " ('builds', 929),\n",
       " ('recognised', 930),\n",
       " ('nutrients', 931),\n",
       " ('aari', 932),\n",
       " ('Sophie', 933),\n",
       " ('noticed', 934),\n",
       " ('monkeys', 935),\n",
       " ('milliseconds', 936),\n",
       " ('saath', 937),\n",
       " ('reported', 938),\n",
       " ('Pursuit’s', 939),\n",
       " ('acquisitions', 940),\n",
       " ('humanity', 941),\n",
       " ('exams', 942),\n",
       " ('merited', 943),\n",
       " ('fix', 944),\n",
       " ('beleaguered', 945),\n",
       " ('hone', 946),\n",
       " ('undo', 947),\n",
       " ('suspend', 948),\n",
       " ('natural', 949),\n",
       " ('occasionally', 950),\n",
       " ('foreground', 951),\n",
       " ('backline', 952),\n",
       " ('ADHD', 953),\n",
       " ('delivering', 954),\n",
       " ('Punjab', 955),\n",
       " ('budget', 956),\n",
       " ('if', 957),\n",
       " ('kilometer', 958),\n",
       " ('imitate', 959),\n",
       " ('Sun', 960),\n",
       " ('forgetting', 961),\n",
       " ('Haidee', 962),\n",
       " ('medal', 963),\n",
       " ('Tommaso', 964),\n",
       " ('Meetings', 965),\n",
       " ('inequities', 966),\n",
       " ('befitting', 967),\n",
       " ('species', 968),\n",
       " ('Seppi', 969),\n",
       " ('distributing', 970),\n",
       " ('GIPHY', 971),\n",
       " ('length', 972),\n",
       " ('regularity', 973),\n",
       " ('suggestions', 974),\n",
       " ('jewellers', 975),\n",
       " ('beefburger', 976),\n",
       " ('coaches', 977),\n",
       " ('yes', 978),\n",
       " ('technical', 979),\n",
       " ('reliving', 980),\n",
       " ('majestically', 981),\n",
       " ('acquired', 982),\n",
       " ('Chairmanship', 983),\n",
       " ('couplings', 984),\n",
       " ('identical', 985),\n",
       " ('big', 986),\n",
       " ('roughly', 987),\n",
       " ('again', 988),\n",
       " ('Left', 989),\n",
       " ('Monster', 990),\n",
       " ('ponzi', 991),\n",
       " ('approval', 992),\n",
       " ('Anushka', 993),\n",
       " ('Ghatlodiya', 994),\n",
       " ('Formal', 995),\n",
       " ('kiosks', 996),\n",
       " ('cobbled', 997),\n",
       " ('BC', 998),\n",
       " ('varies', 999),\n",
       " ...]"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens2idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-16T08:57:41.047935Z",
     "start_time": "2019-08-16T08:57:41.040065Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "500"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_onehot_arr.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-16T08:57:48.918288Z",
     "start_time": "2019-08-16T08:57:48.912541Z"
    }
   },
   "outputs": [],
   "source": [
    "sss = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=2019)\n",
    "train_idx, test_idx = list(sss.split(np.zeros(label_onehot_arr.shape[0]), label_onehot_arr))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-16T08:57:51.172446Z",
     "start_time": "2019-08-16T08:57:51.165735Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "array(['0\\n', '3\\n', '3\\n', '0\\n', '3\\n', '1\\n', '2\\n', '1\\n', '1\\n',\n",
       "       '3\\n', '1\\n', '0\\n', '0\\n', '2\\n', '0\\n', '3\\n', '2\\n', '1\\n',\n",
       "       '3\\n', '1\\n', '3\\n', '0\\n', '3\\n', '0\\n', '3\\n', '3\\n', '1\\n',\n",
       "       '1\\n', '3\\n', '2\\n', '2\\n', '3\\n', '2\\n', '3\\n', '1\\n', '3\\n',\n",
       "       '2\\n', '2\\n', '3\\n', '0\\n', '1\\n', '2\\n', '1\\n', '3\\n', '1\\n',\n",
       "       '0\\n', '0\\n', '2\\n', '0\\n', '3\\n', '2\\n', '2\\n', '1\\n', '2\\n',\n",
       "       '2\\n', '0\\n', '2\\n', '2\\n', '0\\n', '3\\n', '3\\n', '1\\n', '2\\n',\n",
       "       '0\\n', '3\\n', '3\\n', '3\\n', '2\\n', '0\\n', '1\\n', '2\\n', '1\\n',\n",
       "       '3\\n', '2\\n', '3\\n', '1\\n', '1\\n', '1\\n', '1\\n', '0\\n', '3\\n',\n",
       "       '1\\n', '1\\n', '1\\n', '0\\n', '3\\n', '0\\n', '0\\n', '0\\n', '2\\n',\n",
       "       '2\\n', '3\\n', '1\\n', '2\\n', '0\\n', '0\\n', '1\\n', '1\\n', '3\\n',\n",
       "       '1\\n'], dtype='<U2')"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(label_arr[test_idx]) == [label_list[i] for i in test_idx]\n",
    "label_arr[test_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-16T06:57:41.682663Z",
     "start_time": "2019-08-16T06:57:41.677641Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Redmi',\n",
       " 'Note',\n",
       " 'Pro',\n",
       " 'to',\n",
       " 'be',\n",
       " 'around',\n",
       " 'Rs',\n",
       " 'are',\n",
       " 'for',\n",
       " 'the',\n",
       " 'of',\n",
       " 'in',\n",
       " 'on',\n",
       " 'February',\n",
       " 'is',\n",
       " 'as',\n",
       " 'early',\n",
       " 'next',\n",
       " 'week',\n",
       " 'Weibing',\n",
       " 'will',\n",
       " 'an',\n",
       " 'which',\n",
       " 'has',\n",
       " 'been',\n",
       " 'The',\n",
       " 'by',\n",
       " 'also',\n",
       " 'that',\n",
       " 'same',\n",
       " 'In',\n",
       " 'a',\n",
       " 'at',\n",
       " 'how',\n",
       " 'would',\n",
       " 'and',\n",
       " 'said',\n",
       " 'those',\n",
       " 'who',\n",
       " 'have',\n",
       " 'their',\n",
       " 'but',\n",
       " 'what',\n",
       " 't',\n",
       " 'first',\n",
       " 'we',\n",
       " 'hearing',\n",
       " 'about',\n",
       " 's',\n",
       " 'details',\n",
       " '',\n",
       " 'KGF',\n",
       " 'with',\n",
       " 'film',\n",
       " 'all',\n",
       " 'was',\n",
       " 'now',\n",
       " 'Cr',\n",
       " 'very',\n",
       " 'Deepak',\n",
       " 'social',\n",
       " 'his',\n",
       " 'from',\n",
       " 'Mumbai',\n",
       " 'Marathon',\n",
       " 'For',\n",
       " 'it',\n",
       " 'he',\n",
       " 'then',\n",
       " 'just',\n",
       " 'one',\n",
       " 'do',\n",
       " 'Facebook',\n",
       " 'LOL',\n",
       " 'its',\n",
       " 'or',\n",
       " 'more',\n",
       " 'can',\n",
       " 'Ahmedabad',\n",
       " 'Bench',\n",
       " 'Essar',\n",
       " 'Steel',\n",
       " 'January',\n",
       " 'last',\n",
       " 'two',\n",
       " 'Mohena',\n",
       " 'her',\n",
       " 'show',\n",
       " 'this',\n",
       " 'She',\n",
       " 'I',\n",
       " 'am',\n",
       " 'end',\n",
       " 'Duffy',\n",
       " 'Fudge',\n",
       " 'season',\n",
       " 'McLoughlin',\n",
       " 'Marissa',\n",
       " 'fish',\n",
       " 'Wise',\n",
       " 'Marketer',\n",
       " 'Loyalty',\n",
       " 'Academy',\n",
       " 'Conference',\n",
       " 'loyalty',\n",
       " 'marketers',\n",
       " 'customer',\n",
       " 'conference']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "allWords = [token for tokens in tokens_list for token in tokens]\n",
    "subWords = [word for word in allWords if word not in {}]\n",
    "\n",
    "wordCount = Counter(subWords)\n",
    "\n",
    "words = [w for w,cnt in wordCount.items() if cnt >= 5]\n",
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filePath = \"/home/zhoutong/nlp/data/labeled_taste_test.json_down_sampled\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-15T08:31:17.437291Z",
     "start_time": "2019-08-15T08:31:17.376833Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aaa217d71c474042b6dbb7fe59c35f25",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\t1\n",
      "1\t0\n",
      "2\t2\n",
      "3\t3\n",
      "4\t2\n",
      "5\t2\n",
      "6\t2\n",
      "7\t1\n",
      "8\t1\n",
      "9\t1\n",
      "10\t2\n",
      "\n"
     ]
    }
   ],
   "source": [
    "base_p = \"/home/zhoutong/nlp/data\" # \"/Users/zac/Downloads/data\" /home/zhoutong/nlp/data /data/work/data\n",
    "p_origin = base_p + \"/labeled_timeliness_region_taste_emotion_sample.json\"\n",
    "f_iter = ExqUtils.load_file_as_iter(p_origin)\n",
    "job = \"taste\"\n",
    "job_idx_file = os.path.join(base_p,\"sampleIdx_{}\".format(job))\n",
    "\n",
    "cnt = 0\n",
    "for idx,line in tqdm(enumerate(f_iter)):\n",
    "    info = json.loads(line)\n",
    "    print(str(idx)+\"\\t\"+str(info[job]))\n",
    "    cnt += 1\n",
    "    if cnt>10:\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-07T07:33:44.198936Z",
     "start_time": "2019-08-07T07:33:44.177542Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "class EmbModel(object):\n",
    "    def __init__(self):\n",
    "        self.model_param = {\n",
    "            'max_depth': 6,\n",
    "            'learning_rate': 0.1,\n",
    "            'n_estimators': 20,\n",
    "            'objective': 'binary:logistic',\n",
    "            'booster': 'gbtree',\n",
    "            'nthread': None,\n",
    "        }\n",
    "        self.params_grid = {\n",
    "            'max_depth': [1, 2, 3, 4, 5, 6],\n",
    "            'n_estimators': [10, 15, 20, 50, 52, 55, 60, 70, 80],\n",
    "        }\n",
    "        self.model_emb = None\n",
    "        self._clf = None\n",
    "\n",
    "    def load_word_embedding(self, model_path):\n",
    "        zprint(\"loading word_embedding model. from: {}\".format(model_path))\n",
    "        self.model_emb = fasttext.load_model(model_path)\n",
    "\n",
    "    # 清理符号\n",
    "    @staticmethod\n",
    "    def _clean_text(inp_text):\n",
    "        res = re.sub(r\"[~!@#$%^&*()_+-={\\}|\\[\\]:\\\";'<>?,./]\", r' ', inp_text)\n",
    "        res = re.sub(r\"\\n+\", r\" \", res)\n",
    "        res = re.sub(r\"\\s+\", \" \", res)\n",
    "        res = res.strip()\n",
    "        return res\n",
    "\n",
    "    def _get_article_vector(self, text):\n",
    "        if self.model_emb is None:\n",
    "            raise Exception(\"self.model_emb is None. use 'MyModel.load_word_embedding()' to load embedding model\")\n",
    "        else:\n",
    "            return np.mean([self.model_emb.get_sentence_vector(sen) for sen in text.split(\".\")], axis=0)\n",
    "\n",
    "\n",
    "    def fit(self, text_list, label_list, weight_list):\n",
    "        input_vec_list = np.array([self._get_article_vector(self._clean_text(text)) for text in text_list])\n",
    "        self._clf = GridSearchCV(xgb.sklearn.XGBClassifier(**self.model_param), self.params_grid, verbose=1, cv=4, scoring='roc_auc')\n",
    "        self._clf.fit(input_vec_list, np.array(label_list), sample_weight=np.array(weight_list))\n",
    "\n",
    "    def train_supervised(self, fasttext_format_sample_path,weight_dict=None,label_prefix=\"__label__\"):\n",
    "        with open(fasttext_format_sample_path, \"r\") as f:\n",
    "            content = [i.strip().split(label_prefix) for i in f.readlines()]\n",
    "        text_list = [text for text,_ in content]\n",
    "        label_list = [label_prefix+label_ for _,label_ in content]\n",
    "        weight_list = [1.0 for _ in label_list]\n",
    "        if weight_dict is not None:\n",
    "            weight_list = [weight_dict[label] for label in label_list]\n",
    "        self.fit(text_list, label_list, weight_list)\n",
    "        return self\n",
    "\n",
    "    def predict(self, text):\n",
    "        input_vec = self._get_article_vector(self._clean_text(text))\n",
    "        if self._clf is None:\n",
    "            raise Exception(\"self._clf is None. use 'MyModel.fit()' or 'MyModel.load()' to init self._clf\")\n",
    "        return self._clf.predict(input_vec)\n",
    "\n",
    "    def save(self, save_path):\n",
    "        pickle.dump(self._clf, open(save_path, \"wb\"))\n",
    "\n",
    "    def load(self, load_path):\n",
    "        self._clf = pickle.load(open(load_path, \"rb\"))\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-12T06:35:39.678787Z",
     "start_time": "2019-08-12T06:35:39.662692Z"
    },
    "code_folding": [
     17,
     28,
     38
    ]
   },
   "outputs": [],
   "source": [
    "base_p = \"/home/zhoutong/nlp/data\" # /Users/zac/Downloads/data  /home/zhoutong/nlp/data  /data/work/data\n",
    "job = \"taste\" # timeliness taste emotion region(1,0)\n",
    "# 正式数据\n",
    "p = base_p+\"/labeled_timeliness_region_taste_emotion_sample.json\"\n",
    "# 准备词向量训练样本\n",
    "prepare_samples_corpus = False\n",
    "p_train_corpus = base_p + \"/corpus4we.text\"\n",
    "# 直接按不均衡样本训练\n",
    "prepare_samples = True # \n",
    "p_train = base_p+\"/labeled_{}_train.json\".format(job)\n",
    "p_test = base_p+\"/labeled_{}_test.json\".format(job)\n",
    "model_path = base_p+\"/{}_model.ftz\".format(job)\n",
    "# 亚采样\n",
    "prepare_samples_downsamples = True\n",
    "p_train_downsample = p_train + \"_down_sampled\"\n",
    "p_test_downsample = p_test + \"_down_sampled\"\n",
    "model_path_downsample = base_p+\"/{}_model_down_sampled.ftz\".format(job)\n",
    "downsample_queue_dict = {\n",
    "    'timeliness':dict((str(i),deque([], 2900)) for i in range(1,9)),\n",
    "    'taste':dict((str(i),deque([], 61508)) for i in range(0,4)),\n",
    "    'emotion':dict((str(i),deque([], 43642)) for i in range(0,3)),\n",
    "    'region':dict((str(i),deque([], 220591)) for i in range(0,2)),\n",
    "}\n",
    "# 过采样\n",
    "prepare_samples_oversamples = True\n",
    "p_train_oversample = p_train + \"_oversample\"\n",
    "p_test_oversample = p_test + \"_oversample\"\n",
    "model_path_oversample = base_p+\"/{}_model_oversample.ftz\".format(job)\n",
    "oversample_queue_dict = {\n",
    "    'timeliness':dict((str(i),deque([], 40*10000)) for i in range(1,9)),\n",
    "    'taste':dict((str(i),deque([], 30*10000)) for i in range(0,4)),\n",
    "    'emotion':dict((str(i),deque([], 40*10000)) for i in range(0,3)),\n",
    "    'region':dict((str(i),deque([], 40*10000)) for i in range(0,2)),\n",
    "}\n",
    "# 使用 wordEmbedding & XGB 做分类\n",
    "use_EmbModel = True\n",
    "we_model_path = base_p + \"/cc.en.300.bin\"\n",
    "model_path_we = base_p+\"/{}_model_we.ftz\".format(job)\n",
    "total_weight_dict = {\n",
    "    'timeliness':{'__label__1':11.0,'__label__2':1.0,'__label__3':7.0,'__label__4':25.0,'__label__5':97.0,'__label__6':153.0,'__label__7':18.0,'__label__8':9.0},\n",
    "    'taste':{'__label__0':3.25,'__label__1':1.0,'__label__2':4.0,'__label__3':6.25},\n",
    "    'emotion':{'__label__0':4.0,'__label__1':11.0,'__label__2':1.0},\n",
    "    'region':{'__label__0':2.0,'__label__1':1.0}\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-12T06:35:51.108098Z",
     "start_time": "2019-08-12T06:35:51.100537Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "class Util():\n",
    "    # 清理符号\n",
    "    @staticmethod\n",
    "    def clean_text(inp_text):\n",
    "        res = re.sub(r\"[~!@#$%^&*()_+-={\\}|\\[\\]:\\\";'<>?,./]\", r' ', inp_text)\n",
    "        res = re.sub(r\"\\n+\", r\" \", res)\n",
    "        res = re.sub(r\"\\s+\", \" \", res)\n",
    "        return res\n",
    "    # fasttext自带的测试API\n",
    "    @staticmethod\n",
    "    def fasttext_test(model, file_p):\n",
    "        n, precision, recall = model.test(file_p)\n",
    "        zprint(\"test 结果如下:\")\n",
    "        zprint('P@1:'+str(precision))  # P@1 取排第一的分类，其准确率\n",
    "        zprint('R@1:'+str(recall))  # R@1 取排第一的分类，其召回率\n",
    "        zprint('Number of examples: {}'.format(n))\n",
    "        zprint(model.predict(\"I come from china\"))\n",
    "    # 自定义验证各类别的 recall percision f1\n",
    "    @staticmethod\n",
    "    def metric_on_file(label_pred_list):\n",
    "        all_label = set(i[0] for i in label_pred_list)\n",
    "        res = []\n",
    "        for curLbl in sorted(all_label):\n",
    "            TP = sum(label == pred == curLbl for label, pred in label_pred_list)\n",
    "            label_as_curLbl = sum(label == curLbl for label, pred in label_pred_list)\n",
    "            pred_as_curLbl = sum(pred == curLbl for label, pred in label_pred_list)\n",
    "            P = TP / pred_as_curLbl if TP > 0 else 0.0\n",
    "            R = TP / label_as_curLbl if TP > 0 else 0.0\n",
    "            F1 = 2.0 * P * R / (P + R) if TP > 0 else 0.0\n",
    "            res.append((curLbl,R,P,F1))\n",
    "        res.append(('__label__M', sum(R for _,R,_,_ in res)/len(res) ,sum(P for _,_,P,_ in res)/len(res), sum(F1 for _,_,_,F1 in res)/len(res)))\n",
    "        for curLbl,R,P,F1 in res:\n",
    "            print(\"[label]: {}, [recall]: {:.4f}, [precision]: {:.4f}, [f1]: {:.4f}\".format(curLbl,R,P,F1))\n",
    "\n",
    "        label_grouped = itertools.groupby(sorted([label for label, pred in label_pred_list]))\n",
    "        pred_grouped = itertools.groupby(sorted([pred for label, pred in label_pred_list]))\n",
    "        label_distribution = dict((k, len(list(g))) for k, g in label_grouped)\n",
    "        pred_distribution = dict((k, len(list(g))) for k, g in pred_grouped)\n",
    "        print(\"[label分布]: \", label_distribution)\n",
    "        print(\"[pred分布]: \", pred_distribution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-05T03:02:25.995050Z",
     "start_time": "2019-08-05T03:01:55.986068Z"
    },
    "code_folding": [
     10
    ],
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "timeliness {'1': 39566, '2': 456327, '3': 64505, '4': 17625, '5': 4698, '6': 2979, '7': 24271, '8': 49380}\n",
      "emotion {'0': 122369, '1': 43642, '2': 493340}\n",
      "taste {'0': 117872, '1': 384200, '2': 95771, '3': 61508}\n",
      "region {'0': 438760, '1': 220591}\n"
     ]
    }
   ],
   "source": [
    "##############################################################################################################\n",
    "# 分析样本分布\n",
    "# elapsed: roughly 29.4s\n",
    "# timeliness {'1': 39566, '2': 456327, '3': 64505, '4': 17625, '5': 4698, '6': 2979, '7': 24271, '8': 49380}\n",
    "# emotion {'0': 122369, '1': 43642, '2': 493340}\n",
    "# taste {'0': 117872, '1': 384200, '2': 95771, '3': 61508}\n",
    "# region {'0': 438760, '1': 220591}\n",
    "###############################################################################################################\n",
    "find_distribution = False\n",
    "all_job = ['timeliness','emotion','taste','region']\n",
    "if find_distribution:\n",
    "    content_iter = ExqUtils.load_file_as_iter(p)\n",
    "    ori_distribution = {'timeliness': {}, 'emotion': {}, 'region': {}, 'taste': {}}\n",
    "    while True:\n",
    "        data = list(itertools.islice(content_iter, 10000 * 10))\n",
    "        if len(data) > 0:\n",
    "            json_res = [json.loads(i.strip()) for i in data]\n",
    "            # sample_list = [c['title'] + \". \" + c['text'] for c in content]\n",
    "            for job in all_job:\n",
    "                job_label_list = np.asarray(sorted([str(c[job]) for c in json_res]))\n",
    "                for k, g in itertools.groupby(job_label_list):\n",
    "                    ori_distribution[job].update({k: len(list(g)) + ori_distribution[job].get(k, 0)})\n",
    "        else:\n",
    "            break\n",
    "    for job in all_job:\n",
    "        print(job,ori_distribution[job])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-07T03:10:11.761231Z",
     "start_time": "2019-08-07T02:07:42.236653Z"
    },
    "code_folding": [
     3,
     18,
     39,
     52,
     60,
     90
    ],
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "####################\n",
    "# 准备 词向量 训练样本\n",
    "####################\n",
    "if prepare_samples_corpus:\n",
    "    print(\"提取文本语料用于训练词向量\")\n",
    "    print(\"清空文件\")\n",
    "    print(os.popen('> ' + p_train_corpus), p_train_corpus)\n",
    "    with open(p,\"r\") as f:\n",
    "        json_res = [json.loads(i.strip()) for i in f.readlines()]\n",
    "    text_list = [c['title'] + \". \" + c['text'] for c in json_res]\n",
    "    text_list = [clean_text(i)+\"\\n\" for i in text_list]\n",
    "    with open(p_train_corpus,\"w\") as f:\n",
    "        f.writelines(text_list)\n",
    "\n",
    "####################\n",
    "# 准备（分类）训练样本\n",
    "# {'1': 39566, '2': 456327, '3': 64505, '4': 17625, '5': 4698, '6': 2979, '7': 24271, '8': 49380}\n",
    "####################\n",
    "if prepare_samples:\n",
    "    print(\"加载各样本\")\n",
    "    content_iter = ExqUtils.load_file_as_iter(p)\n",
    "    distribution = {}\n",
    "    print(\"清空文件\")\n",
    "    print(os.popen('> '+p_train),p_train)\n",
    "    print(os.popen('> '+p_test),p_test)\n",
    "    while True:\n",
    "        data = list(itertools.islice(content_iter, 10000 * 15))\n",
    "        if len(data) > 0:\n",
    "            json_res = [json.loads(i.strip()) for i in data]\n",
    "            sample_list = [c['title'] + \". \" + c['text'] for c in json_res]\n",
    "            job_label_list = np.asarray(sorted([str(c[job]) for c in json_res]))\n",
    "            for k, g in itertools.groupby(job_label_list):\n",
    "                distribution.update({k: len(list(g)) + distribution.get(k, 0)})\n",
    "            sss = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=0)\n",
    "            train_idx, test_idx = list(sss.split(sample_list, job_label_list))[0]\n",
    "            with open(p_train, \"a\") as f:\n",
    "                for idx in tqdm(train_idx):\n",
    "                    to_write = clean_text(sample_list[idx]) + \"__label__\" + job_label_list[idx]\n",
    "                    f.writelines(to_write + \"\\n\")\n",
    "            with open(p_test, \"a\") as f:\n",
    "                for idx in tqdm(test_idx):\n",
    "                    to_write = clean_text(sample_list[idx]) + \"__label__\" + job_label_list[idx]\n",
    "                    f.writelines(to_write + \"\\n\")\n",
    "        else:\n",
    "            break\n",
    "    total = sum(list(distribution.values()))\n",
    "    print(\">>> 整体（训练+测试）样本分布：\"+str([(k, round(v / total, 4)) for k, v in distribution.items()]))\n",
    "\n",
    "###############################################\n",
    "# 准备（分类）训练样本\n",
    "# 对不均衡的数据: 亚采样多数类\n",
    "###############################################\n",
    "if prepare_samples_downsamples:\n",
    "    print(\"downsample, 加载各样本\")\n",
    "    content_iter = ExqUtils.load_file_as_iter(p)\n",
    "    samples_dict = downsample_queue_dict[job]\n",
    "    print(\"清空文件\")\n",
    "    print(os.popen('> '+ p_train_downsample), p_train_downsample)\n",
    "    print(os.popen('> '+ p_test_downsample), p_test_downsample)\n",
    "    # 加载文件遍历进行FIFO\n",
    "    while True:\n",
    "        data = list(itertools.islice(content_iter, 5000 * 1))\n",
    "        if len(data) > 0:\n",
    "            content = [json.loads(i.strip()) for i in data]\n",
    "            for c in content:\n",
    "                text = c['title']+\" \"+c['text']\n",
    "                label = str(c[job])\n",
    "                samples_dict[label].append(text)\n",
    "        else:\n",
    "            break\n",
    "    # 数据拆分: {label: list(text)} -> text_list & label_list\n",
    "    text_list, label_list = deque(), deque()\n",
    "    for k, v in samples_dict.items():\n",
    "        text_list.extend(v)\n",
    "        label_list.extend([k] * len(v))\n",
    "    sss = StratifiedShuffleSplit(n_splits=1, test_size=0.1, random_state=0)\n",
    "    train_idx, test_idx = list(sss.split(text_list, label_list))[0]\n",
    "    with open(p_train_downsample, \"a\") as f:\n",
    "        for idx in tqdm(train_idx):\n",
    "            to_write = clean_text(text_list[idx]) + \"__label__\" + label_list[idx]\n",
    "            f.writelines(to_write + \"\\n\")\n",
    "    with open(p_test_downsample, \"a\") as f:\n",
    "        for idx in tqdm(test_idx):\n",
    "            to_write = clean_text(text_list[idx]) + \"__label__\" + label_list[idx]\n",
    "            f.writelines(to_write + \"\\n\")\n",
    "            \n",
    "###############################################\n",
    "# 准备（分类）训练样本\n",
    "# 对不均衡的数据: 过采样少数类\n",
    "###############################################\n",
    "if prepare_samples_oversamples:\n",
    "    print(\"oversample 加载各样本\")\n",
    "    content_iter = ExqUtils.load_file_as_iter(p)\n",
    "    samples_dict = oversample_queue_dict[job]\n",
    "    print(\"清空文件\")\n",
    "    print(os.popen('> ' + p_train_oversample), p_train_oversample)\n",
    "    print(os.popen('> ' + p_test_oversample), p_test_oversample)\n",
    "    # 加载文件遍历进行FIFO\n",
    "    while True:\n",
    "        data = list(itertools.islice(content_iter, 100000 * 2))\n",
    "        if len(data) > 0:\n",
    "            content = [json.loads(i.strip()) for i in data]\n",
    "            for c in content:\n",
    "                text = c['title'] + \" \" + c['text']\n",
    "                label = str(c[job])\n",
    "                samples_dict[label].append(text)\n",
    "        else:\n",
    "            # 循环结束时，扩充为填满的类别（oversampling）\n",
    "            for label, text_deque in samples_dict.items():\n",
    "                while len(text_deque)<text_deque.maxlen:\n",
    "                    text_deque.extend(text_deque)\n",
    "                print(\"    {} oversampling完毕, 当前deque长度: {}\".format(label,len(text_deque)))\n",
    "            break\n",
    "    text_list, label_list = deque(), deque()\n",
    "    for k, v in samples_dict.items():\n",
    "        text_list.extend(v)\n",
    "        label_list.extend([k] * len(v))\n",
    "    sss = StratifiedShuffleSplit(n_splits=1, test_size=0.1, random_state=0)\n",
    "    train_idx, test_idx = list(sss.split(text_list, label_list))[0]\n",
    "    with open(p_train_oversample, \"a\") as f:\n",
    "        for idx in tqdm(train_idx):\n",
    "            to_write = clean_text(text_list[idx]) + \"__label__\" + label_list[idx]\n",
    "            f.writelines(to_write + \"\\n\")\n",
    "    with open(p_test_oversample, \"a\") as f:\n",
    "        for idx in tqdm(test_idx):\n",
    "            to_write = clean_text(text_list[idx]) + \"__label__\" + label_list[idx]\n",
    "            f.writelines(to_write + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-07T07:33:58.003994Z",
     "start_time": "2019-08-07T07:33:57.999961Z"
    },
    "code_folding": [
     6
    ],
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[train_path]: /home/zhoutong/nlp/data/labeled_taste_train.json\n",
      "[test_path]: /home/zhoutong/nlp/data/labeled_taste_test.json\n",
      "[model_path]: /home/zhoutong/nlp/data/taste_model_we.ftz\n"
     ]
    }
   ],
   "source": [
    "####################\n",
    "# 训练集、模型参数配置\n",
    "####################\n",
    "train_path = p_train # p_train_oversample p_train_downsample p_train\n",
    "test_path = p_test # p_test_oversample p_test_downsample p_test\n",
    "persist_path = model_path_we # model_path_oversample model_path_downsample model_path_we model_path\n",
    "print(\"[train_path]: {}\\n[test_path]: {}\\n[model_path]: {}\".format(train_path,test_path,persist_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-07T07:34:16.801542Z",
     "start_time": "2019-08-07T07:34:01.549288Z"
    }
   },
   "outputs": [],
   "source": [
    "label_prefix = \"__label__\"\n",
    "weight_dict = total_weight_dict[job]\n",
    "\n",
    "with open(train_path, \"r\") as f:\n",
    "    content = [i.strip().split(label_prefix) for i in f.readlines()]\n",
    "text_list = [text for text,_ in content]\n",
    "label_list = [label_prefix+label_ for _,label_ in content]\n",
    "weight_list = [1.0 for _ in label_list]\n",
    "if weight_dict is not None:\n",
    "    weight_list = [weight_dict[label] for label in label_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-07T07:35:10.077700Z",
     "start_time": "2019-08-07T07:35:10.061837Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'RRB ALP Answer Key Objection Tracker Live From Today New Delhi RRB ALP answer key objection submission has begun RRBs had released the ALP answer key for nd CBT yesterday While the link to access RRB ALP answer key and to submit objection was the same the process to submit objection on RRB ALP answer key question paper and responses was scheduled to begin today Candidates will be allowed to submit objection on RRB ALP answer key question paper and candidate responses till tomorrow February midnight RRB ALP Objection Tracker Login Direct LinkRRB ALP nd CBT answer key objection How to submit RRB ALP answer key objection submission will begin today Step one Go to any RRB website Step two Click on the link Click here to Login for nd Stage ALP Technicians Objection tracker Step three Read the instructions given carefully and click on the login button at the end of page Step four Enter your user id and date of birth and click on login Step four Select the question id from the dropdown list to raise objection on a question For incorrect answer key select correct option ID from the FOUR option IDs available on the top right corner of the question and select the correct option ID from the drop down list of option IDs Step five Furnish explanation for your objection in the box provided Step six Pay the fee or total fee for objections raised Candidates have to pay Rs for each objection raised In case a candidate s objection is found to be valid RRBs would refund the fee submitted by the candidate to their respective bank accounts '"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-07T07:36:01.792879Z",
     "start_time": "2019-08-07T07:36:01.780955Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'__label__0'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-07T07:36:55.545503Z",
     "start_time": "2019-08-07T07:36:55.538673Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a\n"
     ]
    }
   ],
   "source": [
    "print(\"a\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-08-07T07:14:13.856Z"
    },
    "code_folding": [
     7,
     47
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|2019-08-07 15:22:18| 开始训练有监督（分类）模型...\n"
     ]
    }
   ],
   "source": [
    "if use_EmbModel :\n",
    "    print(\"use_EmbModel\")\n",
    "    model = EmbModel()\n",
    "    model.load_word_embedding(we_model_path)\n",
    "    print(\"use weight as: \" + str(total_weight_dict[job]))\n",
    "    model.train_supervised(fasttext_format_sample_path=train_path,weight_dict=total_weight_dict[job])\n",
    "    model.save(persist_path)\n",
    "else:\n",
    "    #######################\n",
    "    # 有监督（分类）模型训练\n",
    "    #######################\n",
    "    zprint(\"开始训练有监督（分类）模型...\")\n",
    "    supervised_params = {\n",
    "        # 'input': '',\n",
    "        'lr': 0.01,  # 学习率\n",
    "        'dim': 180,  # 词向量维数\n",
    "        'ws': 5,  # 上下文窗口\n",
    "        'epoch': 15,  # epoch\n",
    "        'minCount': 10,  # 每个词最小出现次数\n",
    "        'minCountLabel': 0,  # 每个label最小出现次数\n",
    "        'minn': 2,  # 字符级别ngram的最小长度\n",
    "        'maxn': 4,  # 字符级别ngram的最大长度\n",
    "        'neg': 5,  # 负采样个数\n",
    "        'wordNgrams': 3,  # 词级别ngram的个数\n",
    "        'loss': 'softmax',  # 损失函数 {ns, hs, softmax, ova}\n",
    "        'bucket': 2000000,  # buckets个数， 所有n-gram词hash到bucket里\n",
    "        'thread': 8,  # 线程\n",
    "        'lrUpdateRate': 100,  # change the rate of updates for the learning rate [100]\n",
    "        't': 0.0001,  # sampling threshold [0.0001]\n",
    "        'label': '__label__',  # label prefix ['__label__']\n",
    "        'verbose': 2,  # verbose [2]\n",
    "        'pretrainedVectors': ''  # pretrained word vectors (.vec file) for supervised learning []\n",
    "    }\n",
    "    clf = fasttext.train_supervised(input=train_path, **supervised_params)\n",
    "    zprint(\"总计产生词条：{}个，标签： {}个\".format(len(clf.words), len(clf.labels)))\n",
    "    zprint(\"各个标签为：{}\".format(\", \".join(clf.labels)))\n",
    "\n",
    "\n",
    "    ##############\n",
    "    # 分类模型测试\n",
    "    ##############\n",
    "    test_on_model(clf,test_path)\n",
    "\n",
    "    #################\n",
    "    # 压缩 & 保存 模型\n",
    "    #################\n",
    "    quantization = True\n",
    "    if quantization:\n",
    "        zprint(\"压缩模型\")\n",
    "        clf.quantize(train_path, retrain=True)\n",
    "    zprint(\"保存模型..\")\n",
    "    clf.save_model(persist_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-08-06T03:34:50.514Z"
    },
    "code_folding": [
     3
    ]
   },
   "outputs": [],
   "source": [
    "##############\n",
    "# 分类模型测试\n",
    "##############\n",
    "test_on_model(clf,test_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-08-06T03:34:53.626Z"
    }
   },
   "outputs": [],
   "source": [
    "#################\n",
    "# 压缩 & 保存 模型\n",
    "#################\n",
    "quantization = True\n",
    "if quantization:\n",
    "    zprint(\"压缩模型\")\n",
    "    clf.quantize(train_path, retrain=True)\n",
    "zprint(\"保存模型..\")\n",
    "clf.save_model(persist_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-08-06T03:34:55.788Z"
    },
    "code_folding": [],
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#################\n",
    "# 分类模型测试 自测\n",
    "#################\n",
    "model = fasttext.load_model(persist_path)\n",
    "sep = '__label__'\n",
    "with open(test_path, \"r\") as f:\n",
    "    content = [i.strip() for i in f.readlines()]\n",
    "\n",
    "label_pred_list = []\n",
    "for i in tqdm(content):\n",
    "    text = clean_text(i.strip().split(sep)[0])\n",
    "    label = sep + i.strip().split(sep)[1]\n",
    "    y_pred = model.predict(text)[0][0]\n",
    "    label_pred_list.append((label,y_pred))\n",
    "\n",
    "all_label = set(i[0] for i in label_pred_list)\n",
    "for curLbl in all_label:\n",
    "    TP = sum(label == pred == curLbl for label,pred in label_pred_list)\n",
    "    label_as_curLbl = sum(label == curLbl for label,pred in label_pred_list)\n",
    "    pred_as_curLbl = sum(pred == curLbl for label,pred in label_pred_list)\n",
    "    P = TP / pred_as_curLbl if TP>0 else 0.0\n",
    "    R = TP / label_as_curLbl if TP>0 else 0.0\n",
    "    F1 = 2.0*P*R/(P+R) if TP>0 else 0.0\n",
    "    print(\"[label]: {}, [recall]: {:.4f}, [precision]: {:.4f}, [f1]: {:.4f}\".format(curLbl,R,P,F1))\n",
    "    \n",
    "label_grouped = itertools.groupby(sorted([label for label,pred in label_pred_list]))\n",
    "pred_grouped = itertools.groupby(sorted([pred for label,pred in label_pred_list]))\n",
    "label_distribution = dict((k,len(list(g))) for k,g in label_grouped)\n",
    "pred_distribution = dict((k,len(list(g))) for k,g in pred_grouped)\n",
    "print(\"[label分布]: \", label_distribution)\n",
    "print(\"[pred分布]: \", pred_distribution)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-12T06:36:55.915671Z",
     "start_time": "2019-08-12T06:36:54.344197Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "premodel = fasttext.load_model(\"/home/zhoutong/nlp/data/taste_model_oversample.ftz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-12T06:39:14.331086Z",
     "start_time": "2019-08-12T06:39:03.859887Z"
    }
   },
   "outputs": [],
   "source": [
    "sep = \"__label__\"\n",
    "with open(\"/home/zhoutong/nlp/data/labeled_taste_train.json_oversample\", \"r\") as f:\n",
    "    content = [i.strip() for i in f.readlines()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-12T06:40:39.540341Z",
     "start_time": "2019-08-12T06:40:39.537321Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/zhoutong/nlp/data/labeled_taste_train.json'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "'/home/zhoutong/nlp/data/taste_model_oversample.ftz'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_train\n",
    "model_path_oversample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in tqdm(content):\n",
    "    text = Util.clean_text(i.strip().split(sep)[0])\n",
    "    label = sep + i.strip().split(sep)[1]\n",
    "    y_pred = premodel.predict(text)[0][0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
