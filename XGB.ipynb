{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-31T07:42:03.812254Z",
     "start_time": "2019-05-31T07:42:03.796497Z"
    }
   },
   "outputs": [],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "%matplotlib inline\n",
    "from tqdm import tqdm_notebook\n",
    "import concurrent.futures\n",
    "from multiprocessing import Pool\n",
    "import copy,os,sys,psutil\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-31T07:42:04.212906Z",
     "start_time": "2019-05-31T07:42:04.207708Z"
    },
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "from zac_pyutils import ExqUtils\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "import numpy as np\n",
    "from sklearn.model_selection import KFold, train_test_split, GridSearchCV\n",
    "from sklearn.datasets import load_iris, load_digits, load_boston\n",
    "from sklearn.metrics import confusion_matrix, mean_squared_error\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-31T07:43:33.854903Z",
     "start_time": "2019-05-31T07:43:33.849843Z"
    }
   },
   "outputs": [],
   "source": [
    "%run loadUtil.py # 基础工具类"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Other"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 测试 | ExqUtils工具类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-28T10:07:23.369178Z",
     "start_time": "2019-05-28T10:07:23.349021Z"
    }
   },
   "outputs": [],
   "source": [
    "fileIter = ExqUtils.load_file_as_iter(\"./data/nlp/sample_data.txt\")\n",
    "def func(iter_list): return [i.split(\"\\t\") for i in iter_list]\n",
    "ExqUtils.map_on_iter(fileIter,func,chunk_size=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 测试 | pd处理数据\n",
    "- 分块读取\n",
    "- 行、列处理\n",
    "- 各块数据append保存\n",
    "    - 注意append保存时，header务必写false，不让每个chunk都会写一个header"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-29T06:29:12.916180Z",
     "start_time": "2019-05-29T06:29:12.805266Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [-211382657822.72, 191367567861.76, 1372984720...\n",
       "1    [-33325875745.53254, 124329209956.61206, 12401...\n",
       "2    [-33325875745.53254, 124329209956.61206, 12401...\n",
       "3    [-35363378566.72146, 174563844459.54337, 11503...\n",
       "4    [-35363378566.72146, 174563844459.54337, 11503...\n",
       "5    [-54163746911.075, 97957695487.78902, 85072685...\n",
       "6    [-50780992354.85994, 143699497487.83133, 13999...\n",
       "7    [-31902283089.65517, 186908710882.5747, 146082...\n",
       "8    [-50814519757.92308, 66373391832.0, 9546597701...\n",
       "9    [-50814519757.92308, 66373391832.0, 9546597701...\n",
       "0    [-211382657822.72, 191367567861.76, 1372984720...\n",
       "1    [-33325875745.53254, 124329209956.61206, 12401...\n",
       "2    [-33325875745.53254, 124329209956.61206, 12401...\n",
       "3    [-35363378566.72146, 174563844459.54337, 11503...\n",
       "4    [-35363378566.72146, 174563844459.54337, 11503...\n",
       "5    [-54163746911.075, 97957695487.78902, 85072685...\n",
       "6    [-50780992354.85994, 143699497487.83133, 13999...\n",
       "7    [-31902283089.65517, 186908710882.5747, 146082...\n",
       "8    [-50814519757.92308, 66373391832.0, 9546597701...\n",
       "9    [-50814519757.92308, 66373391832.0, 9546597701...\n",
       "Name: feature, dtype: object"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_chunkList = pd.read_csv(\"./data/nlp/sample_data.txt\", \n",
    "            delimiter=\"\\t\",\n",
    "            names=['id','label','weight','feature'],\n",
    "            chunksize=5,\n",
    "#             iterator=False,\n",
    "           )\n",
    "\n",
    "def my_test(a,b): return str(a)+\"\\t\"+str(b)\n",
    "\n",
    "for chunk in df_chunkList:\n",
    "    chunk['feature'] = chunk['feature'].map(lambda x: x.split(\",\"))\n",
    "    chunk['id+label'] = chunk.apply(lambda row: my_test(row['id'],row['label']),axis=1)\n",
    "    chunk.to_csv(\"./data/nlp/sample_data_processed.txt\",mode='a',sep='\\t',header=False)\n",
    "    \n",
    "df_load = pd.read_csv(\"./data/nlp/sample_data_processed.txt\",delimiter=\"\\t\",names=['id','label','weight','feature',\"label+feature\"])\n",
    "df_load['feature'].map(lambda r: [float(i) for i in eval(r)])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGB | DMatrix "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 说明 | 各种数据加载为DMatrix形式\n",
    "- libsvm\n",
    "```\n",
    "dtrain = xgb.DMatrix('train.svm.txt')\n",
    "dtest = xgb.DMatrix('test.svm.buffer')\n",
    "```\n",
    "- csv\n",
    "````\n",
    "xgb.DMatrix(\"train.csv?format=csv&label_column=0\")\n",
    "```\n",
    "- numpy\n",
    "```\n",
    "data = np.random.rand(5,10)\n",
    "label = np.random.randint(2,size=5)\n",
    "dtrain = xgb.DMatrix(data, label=label)\n",
    "```\n",
    "- scipy\n",
    "```\n",
    "csr = scipy.sparse.csr_matrix((data, (row, col)))\n",
    "dtrain = xgb.DMatrix(csr)\n",
    "```\n",
    "- pandas df\n",
    "```\n",
    "data = pd.DataFrame(np.arange(12).reshape((4,3)), columns=['a','b','c'])\n",
    "label = pd.DataFrame(np.random.randint(2, size=4))\n",
    "dtrain = xgb.DMatrix(data, label=label)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### pre-process | pd加载csv然后导入DMatrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-29T08:35:11.000519Z",
     "start_time": "2019-05-29T08:35:10.918255Z"
    },
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "label = loadUtil.loadSampleCSV()['label'].values\n",
    "weight = loadUtil.loadSampleCSV()['weight'].values\n",
    "feature = loadUtil.loadSampleCSV()['feature'].map(lambda x: [eval(i) for i in x.split(\",\")]).values\n",
    "feature = np.asarray(feature.tolist())\n",
    "train = xgb.DMatrix(data=feature,weight=weight,label=label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "## train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "### 自定义评价函数和目标函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "def my_eval(preds,dtrain):\n",
    "    label=dtrain.get_label()\n",
    "    preds = 1.0/(1.0+np.exp(-preds)) # 输出sigmoid一下\n",
    "    pred = [int(i >= 0.5) for i in preds]\n",
    "    tp = sum([int(i == 1 and j == 1) for i,j in zip(pred,label)])\n",
    "    precision=float(tp)/sum(pred)\n",
    "    recall=float(tp)/sum(label)\n",
    "    return 'f1-score',2 * ( precision*recall/(precision+recall) )\n",
    "\n",
    "def my_obj(y_pre,dtrain):\n",
    "    label=dtrain.get_label()\n",
    "    penalty=2.0\n",
    "    grad=-label/y_pre+penalty*(1-label)/(1-y_pre) #梯度\n",
    "    hess=label/(y_pre**2)+penalty*(1-label)/(1-y_pre)**2 #2阶导\n",
    "    return grad,hess"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "### 训练"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### eval_metric部分参数 [default according to objective]\n",
    "- `rmse`: root mean square error\n",
    "- `mae`: mean absolute error\n",
    "- `logloss`: negative log-likelihood\n",
    "- `auc`: Area under the curve\n",
    "- `map`: Mean Average Precision\n",
    "- `cox-nloglik`: negative partial log-likelihood for Cox proportional hazards regression\n",
    "\n",
    "#### objective部分参数\n",
    "- 回归类\n",
    "    - `reg:squarederror`: regression with squared loss\n",
    "    - `reg:logistic`: logistic regression\n",
    "    - `reg:gamma`: gamma regression with log-link. Output is a mean of gamma distribution. It might be useful, e.g., for modeling insurance claims severity, or for any outcome that might be gamma-distributed.\n",
    "    - `reg:tweedie`: Tweedie regression with log-link. It might be useful, e.g., for modeling total loss in insurance, or for any outcome that might be Tweedie-distributed.\n",
    "    - `reg:linear` : ???\n",
    "- 二分类\n",
    "    - `binary:logistic`: logistic regression for binary classification, output probability\n",
    "    - `binary:logitraw`: logistic regression for binary classification, output score before logistic transformation\n",
    "    - `binary:hinge`: hinge loss for binary classification. This makes predictions of 0 or 1, rather than producing probabilities.\n",
    "- 分布预测（这里是泊松分布）\n",
    "    – `poisson regression` for count data, output mean of poisson distribution\n",
    "    - `max_delta_step` is set to 0.7 by default in poisson regression (used to safeguard optimization)\n",
    "\n",
    "- `survival:cox`: Cox regression for right censored survival time data (negative values are considered right censored). Note that predictions are returned on the hazard ratio scale (i.e., as HR = exp(marginal_prediction) in the proportional hazard function h(t) = h0(t) * HR).\n",
    "\n",
    "- 多分类\n",
    "    - `multi:softmax`: set XGBoost to do multiclass classification using the softmax objective, you also need to set num_class(number of classes)\n",
    "    - `multi:softprob`: （输出的是概率矩阵）same as softmax, but output a vector of ndata * nclass, which can be further reshaped to ndata * nclass matrix. The result contains predicted probability of each data point belonging to each class.\n",
    "\n",
    "- 排序\n",
    "    - `rank:pairwise`: Use LambdaMART to perform pairwise ranking where the pairwise loss is minimized\n",
    "    - `rank:ndcg`: Use LambdaMART to perform list-wise ranking where Normalized Discounted Cumulative Gain (NDCG) is maximized\n",
    "    - `rank:map`: Use LambdaMART to perform list-wise ranking where Mean Average Precision (MAP) is maximized\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "num_round = 100\n",
    "param = {\n",
    "    'max_depth':5,\n",
    "    'eta':0.9,\n",
    "    'verbosity':3,\n",
    "    'objective':'reg:linear',\n",
    "    'nthread':4,\n",
    "    'eval_metric':['auc']\n",
    "}\n",
    "bst = xgb.train(param, dtrain, num_round, evallist,feval=f1_error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 模型持久化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "bst.save_model(\"./data/nlp/xgb_model.bin\")\n",
    "# dump得到的是info信息，后续不能直接拿来predict\n",
    "bst.dump_model(\"./data/nlp/xgb_model.json\",dump_format=\"json\")\n",
    "bst.dump_model(\"./data/nlp/xgb_model.txt\",dump_format=\"text\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "## infer/predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "bst_load = xgb.Booster({'nthread':4}) # init model\n",
    "bst_load.load_model(\"/home/zhoutong/xgb_001.model.bin\") # load data\n",
    "bst.predict(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "## analyze"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "### 可视化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "xgb.plot_importance(bst_load)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "### 特征重要度\n",
    "importance_type:\n",
    "- ‘weight’ - the number of times a feature is used to split the data across all trees.\n",
    "- ‘gain’ - the average gain across all splits the feature is used in.\n",
    "- ‘cover’ - the average coverage across all splits the feature is used in.\n",
    "- ‘total_gain’ - the total gain across all splits the feature is used in.\n",
    "- ‘total_cover’ - the total coverage across all splits the feature is used in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "feat_imp_gain=bst.get_score(importance_type='gain')\n",
    "feat_imp = bst.get_score(importance_type='weight')\n",
    "feat_imp['f123345']\n",
    "feat_imp_gain['f123345']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGB | Sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-03T09:48:17.932327Z",
     "start_time": "2019-06-03T09:48:17.561078Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-2.11382658e+11,  1.91367568e+11,  1.37298472e+11, ...,\n",
       "        -1.73583368e+11, -1.55742669e+11,  1.57820631e+11],\n",
       "       [-3.33258757e+10,  1.24329210e+11,  1.24019984e+11, ...,\n",
       "         7.49401047e+10,  5.53350398e+11, -1.65066818e+11],\n",
       "       [-3.33258757e+10,  1.24329210e+11,  1.24019984e+11, ...,\n",
       "        -1.23697357e+11,  5.07993088e+10, -2.04650562e+11],\n",
       "       ...,\n",
       "       [-4.54710271e+10,  1.34911006e+11,  1.14968763e+11, ...,\n",
       "        -8.50839082e+10,  1.08124771e+11,  2.27753379e+11],\n",
       "       [-4.54710271e+10,  1.34911006e+11,  1.14968763e+11, ...,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "       [-4.54710271e+10,  1.34911006e+11,  1.14968763e+11, ...,\n",
       "         2.74411999e+11,  8.60604006e+09, -7.41707551e+10]])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label分类情况： Counter({1: 57, 0: 53})\n"
     ]
    }
   ],
   "source": [
    "# df = loadUtil.loadDataCSV() # 正式数据\n",
    "df = loadUtil.loadSampleCSV() # 10条样例数据\n",
    "label = df['label'].values\n",
    "weight = df['weight'].values\n",
    "feature = df['feature'].map(lambda x: [eval(i) for i in x.split(\",\")]).values\n",
    "feature = np.asarray(feature.tolist())\n",
    "feature\n",
    "print(\"label分类情况：\",Counter(label))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train & infer/predict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 方式一 | KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-31T07:45:41.469889Z",
     "start_time": "2019-05-31T07:45:41.203290Z"
    },
    "code_folding": [],
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "confusion_matrix:\n",
      " [[11  6]\n",
      " [ 9 11]]\n",
      "confusion_matrix:\n",
      " [[14  5]\n",
      " [ 9  9]]\n",
      "confusion_matrix:\n",
      " [[11  6]\n",
      " [ 4 15]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zhoutong/python3/lib/python3.6/site-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "/home/zhoutong/python3/lib/python3.6/site-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "/home/zhoutong/python3/lib/python3.6/site-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n"
     ]
    }
   ],
   "source": [
    "model_param = {\n",
    "    'max_depth': 6,\n",
    "    'learning_rate': 0.1,\n",
    "    'n_estimators': 20,\n",
    "    'objective': 'binary:logistic',\n",
    "    'booster': 'gbtree',\n",
    "    'nthread': None,\n",
    "}\n",
    "train_param = {\n",
    "    'eval_set': None,\n",
    "    'eval_metric': None,\n",
    "}\n",
    "\n",
    "# 1/n_splits 比例的样本作为测试集，重复n_splits（即所有样本都作为测试集迭代过）\n",
    "kf = KFold(n_splits=3,shuffle=True,random_state=2019)\n",
    "for (train_index, test_index) in kf.split(feature):\n",
    "#     print(\"\\n\\ntrain&test index:\",train_index,test_index)\n",
    "    train_param['X'] = feature[train_index]\n",
    "    train_param['y'] = label[train_index]\n",
    "    train_param['sample_weight'] = weight[train_index]\n",
    "    xgb_model = xgb.XGBClassifier(**model_param).fit(**train_param)\n",
    "    predictions = xgb_model.predict(feature[test_index])\n",
    "    print(\"confusion_matrix:\\n\",confusion_matrix(label[test_index], predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 方式二 | GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-31T07:50:04.466942Z",
     "start_time": "2019-05-31T07:49:46.953121Z"
    },
    "code_folding": [],
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 folds for each of 54 candidates, totalling 216 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done 216 out of 216 | elapsed:   17.4s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=4, error_score='raise',\n",
       "       estimator=XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "       colsample_bytree=1, gamma=0, learning_rate=0.1, max_delta_step=0,\n",
       "       max_depth=6, min_child_weight=1, missing=None, n_estimators=20,\n",
       "       n_jobs=1, nthread=None, objective='binary:logistic', random_state=0,\n",
       "       reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
       "       silent=True, subsample=1),\n",
       "       fit_params=None, iid=True, n_jobs=1,\n",
       "       param_grid={'max_depth': [1, 2, 3, 4, 5, 6], 'n_estimators': [10, 15, 20, 50, 52, 55, 60, 70, 80]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring='roc_auc', verbose=1)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_param = {\n",
    "    'max_depth': 6,\n",
    "    'learning_rate': 0.1,\n",
    "    'n_estimators': 20,\n",
    "    'objective': 'binary:logistic',\n",
    "    'booster': 'gbtree',\n",
    "    'nthread': None,\n",
    "}\n",
    "train_param = {\n",
    "    'X': feature,\n",
    "    'y': label,\n",
    "    'eval_set': None,\n",
    "    'eval_metric': None,\n",
    "}\n",
    "xgb_model = xgb.XGBRegressor(**model_param)\n",
    "\n",
    "params_grid = {\n",
    "    'max_depth': [1,2,3,4,5,6],\n",
    "    'n_estimators': [10,15,20,50,52,55,60,70,80],\n",
    "}\n",
    "clf = GridSearchCV(xgb_model, params_grid, verbose=1, cv=4, scoring='roc_auc')\n",
    "clf.fit(feature,label,sample_weight=weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### clf的迭代信息"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-31T08:04:05.186309Z",
     "start_time": "2019-05-31T08:04:05.173298Z"
    },
    "code_folding": [
     0
    ],
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> searcher.param_grid:\n",
      " {'max_depth': [1, 2, 3, 4, 5, 6], 'n_estimators': [10, 15, 20, 50, 52, 55, 60, 70, 80]}\n",
      ">>> searcher.best_score_:\n",
      " 0.7730999837951709\n",
      ">>> searcher.best_params_:\n",
      " {'max_depth': 1, 'n_estimators': 60}\n",
      ">>> searcher.cv_results_:\n",
      "\n",
      "mean_train_score: 0.936,mean_test_score: 0.648, {'max_depth': 1, 'n_estimators': 10}\n",
      "mean_train_score: 0.959,mean_test_score: 0.698, {'max_depth': 1, 'n_estimators': 15}\n",
      "mean_train_score: 0.974,mean_test_score: 0.705, {'max_depth': 1, 'n_estimators': 20}\n",
      "mean_train_score: 0.998,mean_test_score: 0.747, {'max_depth': 1, 'n_estimators': 50}\n",
      "mean_train_score: 0.998,mean_test_score: 0.758, {'max_depth': 1, 'n_estimators': 52}\n",
      "mean_train_score: 0.999,mean_test_score: 0.758, {'max_depth': 1, 'n_estimators': 55}\n",
      "mean_train_score: 1.000,mean_test_score: 0.773, {'max_depth': 1, 'n_estimators': 60}\n",
      "mean_train_score: 1.000,mean_test_score: 0.754, {'max_depth': 1, 'n_estimators': 70}\n",
      "mean_train_score: 1.000,mean_test_score: 0.760, {'max_depth': 1, 'n_estimators': 80}\n",
      "mean_train_score: 0.997,mean_test_score: 0.681, {'max_depth': 2, 'n_estimators': 10}\n",
      "mean_train_score: 0.999,mean_test_score: 0.714, {'max_depth': 2, 'n_estimators': 15}\n",
      "mean_train_score: 1.000,mean_test_score: 0.726, {'max_depth': 2, 'n_estimators': 20}\n",
      "mean_train_score: 1.000,mean_test_score: 0.760, {'max_depth': 2, 'n_estimators': 50}\n",
      "mean_train_score: 1.000,mean_test_score: 0.755, {'max_depth': 2, 'n_estimators': 52}\n",
      "mean_train_score: 1.000,mean_test_score: 0.751, {'max_depth': 2, 'n_estimators': 55}\n",
      "mean_train_score: 1.000,mean_test_score: 0.746, {'max_depth': 2, 'n_estimators': 60}\n",
      "mean_train_score: 1.000,mean_test_score: 0.744, {'max_depth': 2, 'n_estimators': 70}\n",
      "mean_train_score: 1.000,mean_test_score: 0.736, {'max_depth': 2, 'n_estimators': 80}\n",
      "mean_train_score: 1.000,mean_test_score: 0.648, {'max_depth': 3, 'n_estimators': 10}\n",
      "mean_train_score: 1.000,mean_test_score: 0.653, {'max_depth': 3, 'n_estimators': 15}\n",
      "mean_train_score: 1.000,mean_test_score: 0.681, {'max_depth': 3, 'n_estimators': 20}\n",
      "mean_train_score: 1.000,mean_test_score: 0.698, {'max_depth': 3, 'n_estimators': 50}\n",
      "mean_train_score: 1.000,mean_test_score: 0.697, {'max_depth': 3, 'n_estimators': 52}\n",
      "mean_train_score: 1.000,mean_test_score: 0.693, {'max_depth': 3, 'n_estimators': 55}\n",
      "mean_train_score: 1.000,mean_test_score: 0.690, {'max_depth': 3, 'n_estimators': 60}\n",
      "mean_train_score: 1.000,mean_test_score: 0.687, {'max_depth': 3, 'n_estimators': 70}\n",
      "mean_train_score: 1.000,mean_test_score: 0.680, {'max_depth': 3, 'n_estimators': 80}\n",
      "mean_train_score: 1.000,mean_test_score: 0.648, {'max_depth': 4, 'n_estimators': 10}\n",
      "mean_train_score: 1.000,mean_test_score: 0.642, {'max_depth': 4, 'n_estimators': 15}\n",
      "mean_train_score: 1.000,mean_test_score: 0.643, {'max_depth': 4, 'n_estimators': 20}\n",
      "mean_train_score: 1.000,mean_test_score: 0.697, {'max_depth': 4, 'n_estimators': 50}\n",
      "mean_train_score: 1.000,mean_test_score: 0.692, {'max_depth': 4, 'n_estimators': 52}\n",
      "mean_train_score: 1.000,mean_test_score: 0.698, {'max_depth': 4, 'n_estimators': 55}\n",
      "mean_train_score: 1.000,mean_test_score: 0.699, {'max_depth': 4, 'n_estimators': 60}\n",
      "mean_train_score: 1.000,mean_test_score: 0.695, {'max_depth': 4, 'n_estimators': 70}\n",
      "mean_train_score: 1.000,mean_test_score: 0.690, {'max_depth': 4, 'n_estimators': 80}\n",
      "mean_train_score: 1.000,mean_test_score: 0.648, {'max_depth': 5, 'n_estimators': 10}\n",
      "mean_train_score: 1.000,mean_test_score: 0.642, {'max_depth': 5, 'n_estimators': 15}\n",
      "mean_train_score: 1.000,mean_test_score: 0.643, {'max_depth': 5, 'n_estimators': 20}\n",
      "mean_train_score: 1.000,mean_test_score: 0.697, {'max_depth': 5, 'n_estimators': 50}\n",
      "mean_train_score: 1.000,mean_test_score: 0.692, {'max_depth': 5, 'n_estimators': 52}\n",
      "mean_train_score: 1.000,mean_test_score: 0.698, {'max_depth': 5, 'n_estimators': 55}\n",
      "mean_train_score: 1.000,mean_test_score: 0.699, {'max_depth': 5, 'n_estimators': 60}\n",
      "mean_train_score: 1.000,mean_test_score: 0.695, {'max_depth': 5, 'n_estimators': 70}\n",
      "mean_train_score: 1.000,mean_test_score: 0.690, {'max_depth': 5, 'n_estimators': 80}\n",
      "mean_train_score: 1.000,mean_test_score: 0.648, {'max_depth': 6, 'n_estimators': 10}\n",
      "mean_train_score: 1.000,mean_test_score: 0.642, {'max_depth': 6, 'n_estimators': 15}\n",
      "mean_train_score: 1.000,mean_test_score: 0.643, {'max_depth': 6, 'n_estimators': 20}\n",
      "mean_train_score: 1.000,mean_test_score: 0.697, {'max_depth': 6, 'n_estimators': 50}\n",
      "mean_train_score: 1.000,mean_test_score: 0.692, {'max_depth': 6, 'n_estimators': 52}\n",
      "mean_train_score: 1.000,mean_test_score: 0.698, {'max_depth': 6, 'n_estimators': 55}\n",
      "mean_train_score: 1.000,mean_test_score: 0.699, {'max_depth': 6, 'n_estimators': 60}\n",
      "mean_train_score: 1.000,mean_test_score: 0.695, {'max_depth': 6, 'n_estimators': 70}\n",
      "mean_train_score: 1.000,mean_test_score: 0.690, {'max_depth': 6, 'n_estimators': 80}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zhoutong/python3/lib/python3.6/site-packages/sklearn/utils/deprecation.py:122: FutureWarning: You are accessing a training score ('mean_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n"
     ]
    }
   ],
   "source": [
    "def log_details(searcher):\n",
    "    print(\">>> searcher.param_grid:\\n\",searcher.param_grid)\n",
    "    print(\">>> searcher.best_score_:\\n\",searcher.best_score_)\n",
    "    print(\">>> searcher.best_params_:\\n\",searcher.best_params_)\n",
    "\n",
    "    print(\">>> searcher.cv_results_:\\n\")\n",
    "    targetItems = [\n",
    "        'mean_train_score','mean_test_score',\n",
    "    #     'mean_fit_time','mean_score_time',\n",
    "    ]\n",
    "    for i in range(0,len(searcher.cv_results_['params'])):\n",
    "        cur_param = searcher.cv_results_['params'][i]\n",
    "        print(\",\".join([item+\": \"+\"{:.3f}\".format(searcher.cv_results_[item][i]) for item in targetItems])+f\", {cur_param}\")\n",
    "        \n",
    "log_details(clf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 模型持久化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-31T03:54:05.571715Z",
     "start_time": "2019-05-31T03:54:05.563803Z"
    }
   },
   "outputs": [],
   "source": [
    "pickle.dump(clf, open(\"/home/zhoutong/data/nlp/best_from_clf.xgb.pkl\", \"wb\"))\n",
    "# 下面是检验一下模型保存后load出来是否还和之前一样\n",
    "# clf2 = pickle.load(open(\"./data/nlp/best_from_clf.xgb.pkl\", \"rb\"))\n",
    "# print(np.allclose(clf.predict(feature), clf2.predict(feature)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 正式Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-31T06:56:26.290723Z",
     "start_time": "2019-05-31T06:56:26.288303Z"
    }
   },
   "outputs": [],
   "source": [
    "from zac_pyutils import ExqUtils\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## prepare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-03T09:59:07.985986Z",
     "start_time": "2019-06-03T09:57:01.383424Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>label</th>\n",
       "      <th>weight</th>\n",
       "      <th>feature</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1547316214767546</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-2.1138265782272E11,1.9136756786176E11,1.37298...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1547359265142185</td>\n",
       "      <td>1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>-3.332587574553254E10,1.2432920995661206E11,1....</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id  label  weight  \\\n",
       "0  1547316214767546      0     1.0   \n",
       "1  1547359265142185      1     1.5   \n",
       "\n",
       "                                             feature  \n",
       "0  -2.1138265782272E11,1.9136756786176E11,1.37298...  \n",
       "1  -3.332587574553254E10,1.2432920995661206E11,1....  "
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(350067, 4)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "array([[-2.11382658e+11,  1.91367568e+11,  1.37298472e+11, ...,\n",
       "        -1.73583368e+11, -1.55742669e+11,  1.57820631e+11],\n",
       "       [-3.33258757e+10,  1.24329210e+11,  1.24019984e+11, ...,\n",
       "         7.49401047e+10,  5.53350398e+11, -1.65066818e+11]])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path_test = \"/home/zhoutong/data/nlp/tagged_corpus_byW2V_id_label_weight_feat_6.9w.txt\"\n",
    "path_train = \"/home/zhoutong/data/nlp/tagged_corpus_byW2V_id_label_weight_feat_28w.txt\"\n",
    "df_test = pd.read_csv(path_test, delimiter=\"\\t\", names=['id','label','weight','feature'])\n",
    "df_train = pd.read_csv(path_train, delimiter=\"\\t\", names=['id','label','weight','feature'])\n",
    "df = pd.concat([df_test,df_train])\n",
    "label = df['label'].values\n",
    "weight = df['weight'].values\n",
    "feature = df['feature'].map(lambda x : [float(i) for i in x.split(\",\")]).values\n",
    "feature = np.asarray(feature.tolist())\n",
    "df.head(2)\n",
    "df.shape\n",
    "feature[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-04T02:22:07.733891Z",
     "start_time": "2019-06-04T02:22:07.724999Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "model_param = {\n",
    "    'max_depth': 6,\n",
    "    'learning_rate': 0.1,\n",
    "    'n_estimators': 20,\n",
    "    'objective': 'binary:logistic',\n",
    "    'booster': 'gbtree',\n",
    "    'nthread': None,\n",
    "}\n",
    "train_param = {\n",
    "    'eval_set': None,\n",
    "    'eval_metric': None,\n",
    "}\n",
    "xgb_model = xgb.XGBRegressor()\n",
    "params_grid = {\n",
    "    'max_depth': [2,4],\n",
    "    'n_estimators': [10,50,70,100],\n",
    "    'learning_rate': [0.001,0.1,0.02]\n",
    "}\n",
    "# clf = GridSearchCV(xgb_model, params_grid, verbose=1, cv=4, scoring=['roc_auc','f1'],refit='roc_auc')\n",
    "clf = GridSearchCV(xgb_model, params_grid, verbose=2, cv=3, scoring='roc_auc')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-06-04T02:16:21.809Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 24 candidates, totalling 72 fits\n",
      "[CV] learning_rate=0.001, max_depth=2, n_estimators=10 ...............\n"
     ]
    }
   ],
   "source": [
    "clf.fit(feature,label,sample_weight=weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-06-04T02:16:29.333Z"
    }
   },
   "outputs": [],
   "source": [
    "pickle.dump(clf, open(\"/home/zhoutong/data/nlp/best_from_clf.xgb.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Initialization Cell",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "288px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
